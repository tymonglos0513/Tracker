{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Results-oriented Senior Data Engineer with 13+ years of experience specializing in building robust data pipelines and delivering high-performance applications. Proficient in **AWS**, **GCP**, **Azure**, and **Snowflake**, with extensive knowledge of **Python**, **SQL**, and **Airflow** for data orchestration and workflow management. Skilled in modern ETL tools like **DBT** and utilizing big data technologies including **Hadoop**, **Spark**, and **Hive** for data processing and analysis.\nDemonstrated expertise in leveraging cloud-native architectures and containers with **Docker** and **Kubernetes**, ensuring scalable data solutions. Strong track record in optimizing data storage solutions using **Redshift**, **BigQuery**, **DataBricks**, and **SAP HANA**.\nAdditionally, experienced in deploying secure data solutions while adhering to compliance standards, combined with a solid foundation in building AI/ML applications using **MLflow**, **Airflow**, and **Kubeflow** to enhance data-driven decision-making. A proven collaborator in cross-functional teams, dedicated to fostering innovation and excellence in data engineering.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "• Developed and optimized ETL processes utilizing **Snowflake**, **AWS**, and **Azure** for efficient data integration, ensuring seamless data flow and accessibility across teams.\n• Employed **Python** and **SQL** to manipulate and analyze large datasets, enhancing data-driven decision-making through insights and reporting.\n• Implemented and maintained workflows in **Airflow** to automate and schedule recurring data processes, increasing operational efficiency by **30%**.\n• Designed data solutions in **GCP** using **BigQuery** for real-time analytics, enabling insights on consumer behaviors and trends in financial and healthcare sectors.\n• Leveraged **Docker** and **Kubernetes** for containerization and orchestration of data applications, promoting scalability and consistent deployment within varied environments.\n• Enhanced data processing capabilities using **DBT** to build robust data models, facilitating accurate transformations and analytics compatible with **Redshift** and **DataBricks**.\n• Conducted advanced data engineering tasks with **Hadoop**, **Hive**, and **Spark** to process data at scale, resulting in the handling of over **500TB** of data daily.\n• Collaborated closely with data teams to integrate data solutions via **REST-API**, ensuring secure access and manipulation of vital data assets across health and finance domains.\n• Optimized storage solutions using **S3** and **Vertica** for improved data retrieval times in reporting applications, achieving a **40%** reduction in query times.\n• Engaged in performance tuning and management of data outputs, supporting analytics platforms with tools such as **SAP HANA** and Teradata.\n• Established robust data governance protocols, ensuring compliance with industry regulations and enhancing data quality metrics by **25%**."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Leveraged **AWS**, **Azure**, and **GCP** for cloud data solutions, orchestrating ETL processes and data transformations using **Apache Airflow** and **DBT** to ensure seamless data pipeline efficiency.\nDeveloped and maintained robust data models in **Snowflake**, **Redshift**, and **Bigquery**, optimizing performance for analytics and reporting tasks leading to a **30%** reduction in query times.\nIntegrated **Docker** and **Kubernetes** for container orchestration, improving deployment consistency and resource management across development and production environments.\nUtilized **SQL** and **Python** for data manipulation and extraction, implementing complex queries that improved data retrieval speed by **50%**.\nDesigned and developed data workflows on data platforms including **SAP HANA**, **Teradata**, and **Vertica** to support diverse analytical requirements and enhance reporting capabilities.\nExecuted data processing jobs using **Hadoop**, **Spark**, and **Hive**, focusing on large-scale data processing and analytics while refining job execution time by **20%**.\nCollaborated with cross-functional teams to understand data requirements and deliver scalable solutions that improved decision-making processes across the organization.\nImplemented best practices in data governance and quality checks to ensure high standards in data integrity and security."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Leveraged **AWS**, **GCP**, and **Azure** cloud services to build scalable data pipelines, ensuring efficient data transformation and loading into **Snowflake**, **Redshift**, and **BigQuery** with a focus on performance and reliability.\nDesigned and optimized ETL processes using **Apache Airflow** and **DBT** (version 0.21.0), ensuring streamlined data integration and transformation workflows that support real-time analytics and reporting for large datasets exceeding **5 TB**.\nDeveloped and maintained SQL scripts for data extraction and analysis, improving query performance by **30%** through optimized indexing and efficient data modeling.\nUtilized **Docker** and **Kubernetes** for containerization and orchestration of data services, ensuring seamless deployment and scaling of applications across diverse environments.\nImplemented data quality checks and monitoring solutions using **Apache Spark** and **Hadoop** frameworks to ensure data integrity and compliance with company standards for **20+** data sources.\nCreated and managed data lakes on **S3**, enabling cost-effective storage and processing of unstructured data, resulting in a **40%** reduction in operational costs for data storage and access.\nCollaborated with cross-functional teams to design and implement REST API services that provided analytics insights, enhancing data accessibility and integration for business stakeholders, reducing time to insight by **50%**.\nIncorporated advanced access control mechanisms using **Azure RBAC** and **OAuth 2.0**, ensuring data security and compliance with GDPR for all data handling processes.\nWorked with business intelligence tools and frameworks, driving data-driven decision-making processes that led to an increase in operational efficiency by **25%**."
    }
  ],
  "skills": " **Backend Frameworks:**\n\tPython: FastAPI, Flask, Django\n\n **Programming Languages:**\n\tPython\n\tJavaScript/TypeScript\n\n **Frontend Frameworks:**\n\tReact\n\tVue\n\tAngular\n\n **API Technologies:**\n\tREST-API\n\n **Databases:**\n\tPostgreSQL (Fintech)\n\tMySQL (Healthcare)\n\tMongoDB (Gaming)\n\tRedis\n\tSnowflake\n\tBigquery\n\tRedshift\n\tDataBricks\n\tVertica\n\tTeradata\n\tSAP HANA\n\n **Serverless and Cloud Functions:**\n\tAWS: ECS, Lambda, RDS, S3\n\tAzure: App Services, Blob Storage, SQL Database\n\tGCP\n\n **DevOps:**\n\tDocker\n\tKubernetes\n\tGitHub Actions\n\tGitLab CI/CD\n\n **Cloud & Infrastructure:**\n\tTerraform\n\tAnsible\n\tHelm\n\tDocker Compose\n\n **Other:**\n\tMLflow\n\tAirflow\n\tKubeflow\n\tAuthentication & Security: Keycloak (OIDC, RBAC), OAuth2, JWT\n\tNginx, Let’s Encrypt, Certbot\n\tHadoop\n\tHive\n\tSpark",
  "apply_company": "Rush Street Interactive"
}
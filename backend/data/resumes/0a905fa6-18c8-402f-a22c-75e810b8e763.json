{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Pentaho Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Pentaho Engineer with 13+ years of experience in data integration and warehousing, skilled in **Pentaho**, **Data Integrator**, **DataStage**, **SQL**, and **PL/SQL**. Proven track record in building efficient **data pipelines** and implementing robust **data warehousing** solutions. Demonstrated expertise in **Airflow** for orchestrating workflows and enhancing data management.\nExpert in utilizing **Python** for advanced processing tasks and knowledgeable in **Databricks** for big data analytics. Strong team collaboration and problem-solving skills fostered through diverse project experience.\nAdditionally, I possess a deep understanding of **microservices**, **event-driven architectures**, and CI/CD methodologies. My background includes building AI/ML-powered applications using **FastAPI**, **Django**, and cloud deployment on **AWS** and **Azure**, ensuring compliance with industry standards like HIPAA and PCI DSS.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Pentaho Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Pentaho** and **Kettle** for data integration and ETL processes, ensuring efficient pipeline management and seamless data flow across various platforms.\nDesigned and optimized data warehouses using **Oracle** and **SQL** technologies, successfully increasing data retrieval efficiency by **40%**.\nDeveloped robust data pipelines leveraging **Airflow** for orchestrating workflows, enhancing reliability and monitoring with real-time logging.\nCollaborated within cross-functional teams to identify and resolve data-related challenges, improving project delivery timelines by **20%** through effective team collaboration.\nImplemented data visualization solutions using **MicroStrategy** to provide actionable insights from complex datasets, significantly aiding business decision-making.\nManaged and optimized large-scale data processes using **Databricks** for enhanced performance on big data analytics queries, achieving an overall reduction of processing time by **30%**.\nConducted thorough testing and validation of data transformations using **DataStage** and **Powercenter**, ensuring accuracy and compliance with business requirements.\nCreated and maintained detailed documentation of data workflows using **metadata injection** techniques to facilitate knowledge sharing and project continuity.\nDesigned and executed data quality checks across all stages of the data pipeline, leading to improved data integrity and reducing errors by **25%**.\nLed initiatives for continuous improvement in data processing methodologies, applying problem-solving strategies to streamline operations and enhance performance."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Leveraged **Pentaho** and **Data Integrator** to design and implement data pipelines, ensuring efficient ETL processes for financial data with a focus on accuracy and performance.\nUtilized **SQL** and **PL/SQL** for advanced querying and data manipulation, enhancing data extraction from **Oracle** databases, resulting in a reduction of data retrieval time by **30%**.\nDeveloped robust data warehousing solutions, employing **Databricks** and **DataStage** to streamline data storage and retrieval, thus improving reporting efficiency by **25%**.\nImplemented data governance strategies, employing **Metadata injection** techniques in **Pentaho** to maintain data integrity and facilitate better decision-making across teams.\nCollaborated in cross-functional teams, using **Airflow** to manage complex data workflows and automate tasks, improving overall operational efficiency by **20%**.\nStreamlined reporting processes utilizing **MicroStrategy** and **Powercenter**, leading to more timely insights and quicker actionability for business users.\nDesigned scalable data pipeline architectures, integrating third-party tools with existing systems effectively and ensuring minimal downtime during transitions.\nFostered an environment of team collaboration and strong problem-solving skills, helping to identify and resolve data issues swiftly and efficiently."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Utilized **Pentaho** and **Data Integrator** to design and implement data pipelines for efficient ETL processes, enhancing data quality and accessibility for a robust data warehousing solution.\nApplied **SQL** and **PL/SQL** to create optimized queries and scripts for extracting, transforming, and loading data from various sources, improving data retrieval times by **30%**.\nDeveloped and maintained data warehouse solutions leveraging **Oracle** and **MicroStrategy**, delivering analytical insights and reporting capabilities that supported strategic business decisions.\nImplemented workflow orchestration using **Airflow** for scheduling and monitoring data pipeline jobs, ensuring timely data availability and reducing potential delays by **25%**.\nCollaborated with cross-functional teams to gather requirements and troubleshoot issues, enhancing team productivity and fostering an environment of problem-solving and innovation.\nEngineered automated data pipelines with **Databricks** and **DataStage**, optimizing data workflow efficiency and accuracy, consequently increasing processing speed by **40%**.\nUtilized **SSIS** and **Powercenter** for complex data integration tasks, streamlining processes and improving overall data management efficiency.\nApplied **Kettle** for lightweight data transformation tasks, ensuring smooth metadata injection across various systems and enhancing data consistency.\nEnsured optimal team collaboration through regular meetings and updates, contributing to project success and maintaining a high standard of project management practices."
    }
  ],
  "skills": " \n**Programming Languages:**\n\tPython, SQL, PL/SQL\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript: React, Vue, Angular\n\n**API Technologies:**\n\tOAuth2, JWT\n\n**Serverless and Cloud Functions:**\n\tAWS: Lambda\n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, Oracle\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS: ECS, RDS, S3, Azure: App Services, Blob Storage, SQL Database\n\n**Other:**\n\tAirflow, MLflow, Kubeflow, Pentaho, Data Integrator, Data Warehousing, Data Pipeline, Databricks, MicroStrategy, DataStage, SSIS, Powercenter, Kettle, Metadata injection, Team collaboration, Problem-solving",
  "apply_company": "Best in Bi Solutions"
}
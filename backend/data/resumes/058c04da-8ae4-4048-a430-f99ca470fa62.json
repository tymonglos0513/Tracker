{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-187801395/",
  "profile_summary": "Data Engineer with 13+ years of experience in building and optimizing data pipelines across healthcare and financial industries. Proficient in **AWS**, **SQL**, **Python**, and skilled in **data engineering** practices, including data modeling, warehousing, and ETL processes. Experienced with **Apache Hadoop**, **Snowflake**, **Spark**, and **Databricks** for big data solutions. Strong expertise in **containerization** with **Docker** and **Kubernetes** for scalable architectures. Familiar with orchestration tools such as **MLflow** and **Airflow** to streamline workflows, as well as third-party ETL tools like **Talend** and **Fivetran**.\n\nProven track record of implementing robust data retention policies while ensuring compliance with HIPAA, FHIR, PCI DSS, and SOC 2 standards. Additionally, adept in leveraging libraries such as **Pandas**, **NumPy**, and **PySpark** for data manipulation and analysis. Excellent communication skills enhance collaboration in cross-functional teams to drive project success.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **AWS** for cloud architecture, deploying data solutions with a strong emphasis on scalability and performance, ensuring compliance with data retention policies by adhering to industry standards.\nEngineered efficient **SQL** data models and ETL processes for Big Data projects incorporating tools such as **Apache Hadoop** and **Databricks**, handling data ingestion for over **10TB** daily.\nDeveloped and maintained containerized data workflows using **Docker** and **Kubernetes**, optimizing resource utilization and improving deployment speed by **30%**.\nCreated complex data pipelines leveraging **ETL** frameworks like **Talend** and **Stitch**, ensuring high-quality data flow for analytics and reporting across various platforms.\nImplemented data warehousing solutions using **Snowflake**, supporting complex querying and analysis for real-time insights while reducing query time by **40%**.\nDesigned data engineering workflows with **PySpark**, **Pandas**, and **NumPy** for processing large datasets, improving processing time by **50%** for batch operations.\nCollaborated with cross-functional teams to communicate data insights effectively, enhancing data-driven decision-making across the organization.\nApplied advanced data modeling techniques to create schemas that facilitate ease of access and usability for end-users.\nDeveloped and integrated automated data quality checks, streamlining the analysis process while supporting compliance with data standards and policies.\nFostered a culture of continuous improvement through effective communication of findings and strategies adjustment based on data performance metrics."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Developed and implemented **ETL** pipelines for ingesting financial data from internal and third-party sources, leveraging **Python** and **Apache Airflow** to support both batch and real-time processing, ensuring data integrity across **5+** databases.\nEngineered robust **data modeling** and **data warehousing** strategies using **SQL**, **Snowflake**, and **Apache Hadoop**, improving data accessibility and reporting capabilities across business units by **30%**.\nDesigned and maintained scalable **Big Data** solutions in a **cloud architecture** utilizing **AWS**, enhancing storage efficiency and retrieval speeds by **25%**.\nEmployed **containerization** technologies like **Docker** and **Kubernetes** for deploying services, streamlining operations and improving deployment times by **40%**.\nImplemented data retention policies and worked on **data engineering** processes to ensure compliance and optimized storage costs, significantly reducing expenses by **15%**.\nCollaborated effectively with cross-functional teams to communicate complex data concepts and analytics findings, enhancing stakeholder engagement and decision-making processes."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Engineered scalable and efficient **data architecture** leveraging **AWS**, ensuring high availability for **cloud services** in data engineering workflows.\nOptimized data pipelines using **ETL** strategies and frameworks such as **Apache Hadoop**, **DBT**, and **Spark**, resulting in a **30%** reduction in data processing times.\nDeveloped robust **data modeling** techniques and constructed data warehousing solutions utilizing **Snowflake** to support analytical requirements and enable quick insights across the business.\nApplied **containerization** with **Docker** and **Kubernetes** for effective orchestration, streamlining deployment processes for various data services with **100%** uptime during transitions.\nPerformed **SQL** queries on **PostgreSQL** and **MongoDB** for data extraction and manipulation, ensuring fast access to critical data and enhancing reporting capabilities across departments.\nImplemented efficient data retention policies adhering to best practices, resulting in a **25%** cost savings on data storage.\nUtilized **PySpark** for big data processing tasks, achieving significant performance improvements on large datasets through optimized compute resources.\nCollaborated effectively across cross-functional teams, enhancing **communication skills** for successful project outcomes and alignment with business goals.\nDesigned and executed complex **data engineering** solutions, fostering an environment of continuous integration and delivery for data workflows using **Git** and **Jenkins**.\nConducted analysis and reporting on large-scale datasets with tools like **Pandas** and **NumPy**, assisting in strategic decision-making based on real-time insights."
    }
  ],
  "skills": "Programming Languages:\n\t**Python**, **JavaScript**, **TypeScript**\n\nBackend Frameworks:\n\t**FastAPI**, **Flask**, **Django**\n\nFrontend Frameworks:\n\t**React**, **Vue**, **Angular**\n\nAPI Technologies:\n\t**Keycloak (OIDC, RBAC)**, **OAuth2**, **JWT**\n\nServerless and Cloud Functions:\n\t**AWS: Lambda, ECS, S3**, **Azure: App Services, Blob Storage, SQL Database**\n\nDatabases:\n\t**PostgreSQL (Fintech)**, **MySQL (Healthcare)**, **MongoDB (Gaming)**, **Redis**, **SQL**, **Snowflake**, **DBT**\n\nBig Data:\n\t**Apache Hadoop**, **Databricks**, **Spark**, **PySpark**, **Airflow**, **Kubeflow**\n\nData Engineering:\n\t**data modeling**, **data warehousing**, **ETL**, **data retention policies**\n\nDevOps:\n\t**Docker**, **Kubernetes**, **GitHub Actions**, **GitLab CI/CD**, **CI/CD**, **Terraform**, **Ansible**, **Helm**, **Docker Compose**\n\nCloud & Infrastructure:\n\t**cloud architecture**, **containerization**, **orchestration**, **Communication Skills**\n\nOther:\n\t**MLflow**, **Pandas**, **NumPy**, **Talend**, **Informatica**, **Datastage**, **Stitch**, **Fivetran**"
}
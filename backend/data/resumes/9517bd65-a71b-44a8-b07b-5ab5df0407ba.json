{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of expertise in **Data Engineering**, **Data Warehousing**, **ETL**, **ELT**, and **Data Modeling**. Proficient in **Python** and experienced in using **dbt**, **Snowflake**, and **Databricks** for efficient data integration and management. Strong background in deploying solutions using **AWS**, implementing **Infrastructure as Code** with **Terraform** and **CloudFormation**. Skilled in ensuring **Data Quality Management** for robust performance. Additionally, I bring hands-on experience in building microservices, event-driven architectures, and CI/CD pipelines, utilizing technologies such as **React**, **Next.js**, **Vue**, **Node.js**, **FastAPI**, and **Django**. Proven ability to create high-performance applications supporting predictive analytics and real-time processing across the healthcare and financial industries, with a focus on compliance-driven development aligned with HIPAA and PCI DSS.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Designed and built **data engineering** solutions utilizing **Python** and **dbt** for effective data transformation and modeling, ensuring best practices in **data quality management** across environments.\n- Implemented **ETL** and **ELT** processes using **Snowflake** and **Databricks** for efficient data warehousing and analytics, processing over **1 million** records daily.\n- Developed infrastructure as code using **Terraform** and **CloudFormation** to automate deployment of scalable data systems and ensure reliability in builds.\n- Led data modeling initiatives to enhance data integrity with **real-time** capabilities across health and finance datasets, supporting BI insights and analytics.\n- Collaborated with cross-functional teams to integrate **AWS** services for seamless data flows and real-time analytics solutions.\n- Managed data quality assurance processes, establishing metrics and benchmarks to ensure the accuracy of critical datasets utilized in analytics.\n- Built and maintained CI/CD pipelines for data workflows to enable **multi-environment** deployments with **high** automation and testing coverage.\n- Presented advanced analytics results and data visualizations directly impacting business strategies and decision-making processes, leveraging tools like **Power BI** and **D3.js**.\n- Conducted training sessions on best practices in **data warehousing**, data modeling, and the use of **data customer platforms** to improve service delivery.\n- Drove initiatives to enhance data ingestion techniques that increased throughput by **30%**, responding to evolving analytical demands."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Demonstrated expertise in **Data Engineering** by developing ETL pipelines for ingesting financial data utilizing **Python**, **Apache Airflow**, and **Azure Data Factory**, effectively supporting both batch and real-time processing for over **1 million records** daily.\nImplemented advanced **Data Warehousing** solutions, utilizing **Snowflake** and **Databricks** to optimize storage and retrieval, reducing query times by **30%**.\nBuilt robust data models using **dbt**, enhancing data quality and integrity across diverse data sources, ensuring compliance with financial regulations.\nDesigned and executed **Infrastructure as Code** strategies with **Terraform** and **CloudFormation**, ensuring consistent and reproducible cloud resource management on **AWS** and **Azure** environments.\nManaged **Data Quality Management** processes, developing automated validation checks that minimize data discrepancies by **95%**, thus ensuring reliable analytics outcomes.\nLeveraged **Python** for scripting and automation tasks that enhanced data workflows, greatly improving efficiency across various stages of data processing.\nCollaborated on customer data platforms to merge data insights and tailor financial products, impacting customer engagement metrics positively.\nAdopted modern architectural practices to drive the shift towards **ELT** paradigms, reducing overhead in data transformation processes."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Engineered robust **data pipelines** using **Python** and **dbt** for efficient **ETL** and **ELT** processes, enhancing data quality and management across complex data workflows for the organization.\nDesigned and maintained **data warehousing** solutions with **Snowflake** to store, retrieve, and analyze critical business information, ensuring optimal performance with a **10%** improvement in query response times.\nApplied **Infrastructure as Code** methodologies using **Terraform** and **CloudFormation** to automate environment provisioning, achieving a **30%** reduction in setup time while maintaining consistent deployment standards.\nDeveloped comprehensive **data modeling** techniques that improved data integrity and reduced redundancy, enabling seamless integration of customer data from various platforms into a unified repository.\nCollaborated with cross-functional teams to implement **AWS** services for scalable storage and computing solutions, optimizing resource utilization by **15%** and reducing costs.\nInstituted regular **data quality management** protocols, significantly increasing the accuracy of reporting metrics resulting in better-informed business decisions.\nIntegrated effective **Customer Data Platforms** to streamline customer insights and analytics, enhancing targeted marketing efforts and improving customer retention by **7%**.\nExecuted sophisticated analytics workflows in **Databricks**, enabling real-time data processing capabilities and supporting dynamic queries that informed business strategies.\nOptimized data warehousing operations through continuous monitoring and performance tuning, increasing operational efficiency and minimizing downtime for data access.\nEstablished streamlined processes for data extraction and transformation, ensuring timely delivery of insights to support analytics initiatives across the organization."
    }
  ],
  "skills": " **Backend Frameworks:**\n\t•\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\t•\tReact, Vue, Angular\n\n **Programming Languages:**\n\t•\tPython, JavaScript/TypeScript\n\n **API Technologies:**\n\t•\t[AWS services: Lambda]\n\n **Cloud & Infrastructure:**\n\t•\tAWS, Azure\n\t•\tCloudFormation\n\n **DevOps:**\n\t•\tDocker, Kubernetes\n\t•\tGitHub Actions, GitLab CI/CD\n\t•\tTerraform, Ansible, Helm, Docker Compose\n\n **Databases:**\n\t•\tPostgreSQL, MySQL, MongoDB, Redis, Snowflake\n\n **Other:**\n\t•\tKeycloak (OIDC, RBAC), OAuth2, JWT\n\t•\tMLflow, Airflow, Kubeflow, Customer Data Platforms, Data Quality Management, Data Modeling, ETL, ELT, dbt, Data Warehousing",
  "apply_company": "Olo"
}
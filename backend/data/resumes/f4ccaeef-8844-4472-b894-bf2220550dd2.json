{
  "name": "Tomasz Lee",
  "role_name": "Senior Data Engineer",
  "email": "tomasz.lee8@outlook.com",
  "phone": "+48 732 145 942",
  "address": "Warsaw, Poland",
  "linkedin": "https://www.linkedin.com/in/tomasz-lee-b9a25a391/",
  "profile_summary": "Results-driven Senior Data Engineer with 9+ years of experience in implementing data solutions and building scalable data pipelines. Proficient in **SQL**, **Databricks**, **Google BigQuery**, **Airflow**, **dbt**, **Docker**, **Kubernetes**, and **CI/CD** practices, with a strong focus on data warehouse modeling, metadata management, data governance, and orchestration. Demonstrated ability to manage data versioning, schema evolution, and maintain high observability standards across projects. Proven record of optimizing performance and improving data processes with expertise in both frontend and backend development, including strong knowledge of **ReactJS**, **NextJS**, **.NET**, and **C#**. Exceptional collaborator with solid communication skills, leading teams to successful project outcomes.",
  "education": [
    {
      "degree": "Master of Computer Science",
      "category": "",
      "from_year": "Apr 2012",
      "to_year": "Oct 2014",
      "location": "",
      "university": "National University of Singapore"
    },
    {
      "degree": "Bachelor of Computer Science",
      "category": "",
      "from_year": "Apr 2008",
      "to_year": "Mar 2012",
      "location": "",
      "university": "National University of Singapore"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "CD Projekt RED",
      "from_date": "May 2022",
      "to_date": "Oct 2025",
      "location": "Warsaw, Poland",
      "responsibilities": "Utilized **SQL** to optimize database queries and indexing, achieving a **20%** reduction in average query execution time.\nImplemented **Docker** and **Kubernetes** for containerization and orchestration, enhancing deployment efficiency and scalability.\nDeveloped and maintained data pipelines using **Apache Airflow** for orchestration, significantly improving data flow management.\nExecuted data versioning strategies using **dbt**, ensuring robust data governance and schema evolution practices.\nLeveraged **Google BigQuery** for data warehousing, optimizing storage and query performance, resulting in faster analytics processing.\nEngaged in **metadata management** to maintain up-to-date information on data assets, enhancing observability and compliance.\nCollaborated with cross-functional teams, demonstrating strong communication skills to ensure alignment on project objectives and deliverables.\nApplied principles of data warehouse modeling to design scalable and efficient data schemas for reporting and analytics.\nAdopted **CI/CD** practices to automate deployment processes, leading to a **40%** reduction in deployment time.\nImplemented secure data storage solutions within **object storage** systems, ensuring data integrity and accessibility."
    },
    {
      "role": "Full Stack Developer",
      "company": "Sea Group",
      "from_date": "Mar 2018",
      "to_date": "Apr 2022",
      "location": "Shopee, Singapore",
      "responsibilities": "Utilized **SQL** to optimize queries and enhance the performance of data retrieval from databases, achieving a 30% improvement in efficiency.\nImplemented **data warehouse modeling** techniques to structure data effectively, ensuring proper management of enterprise data assets.\nDesigned and executed **CI/CD** pipelines with **Docker** and **Kubernetes**, improving deployment speeds by 40% and minimizing downtime.\nDeveloped and orchestrated ETL processes using **Airflow** and **Databricks**, which streamlined data workflows and increased processing efficiency by 25%.\nImplemented **dbt** for data transformation, leading to better data governance and improved accuracy in analytics.\nLeveraged **Google BigQuery** for quick and efficient querying of large datasets, resulting in a 50% reduction in data retrieval times.\nManaged **metadata management** and **schema evolution** to facilitate agile changes in data structures without affecting existing queries, ensuring data integrity.\nMonitored data pipelines for **observability**, enhancing the reliability and robustness of data flow across systems.\nFostered **strong communication** with cross-functional teams to gather requirements and ensure alignment with business objectives, leading to on-time project deliveries.\nMentored junior team members on best practices in **orchestration** and data modeling, boosting overall team competency and skill sets."
    },
    {
      "role": "Software Developer",
      "company": "Grab Holdings Inc",
      "from_date": "Jan 2015",
      "to_date": "Feb 2018",
      "location": "Singapore",
      "responsibilities": "Utilized **SQL** and integrated **Databricks** to develop a robust data warehouse model, improving data retrieval efficiency by **30%**.\nDesigned and implemented orchestration workflows using **Airflow** to automate data processing pipelines, leading to a **25%** reduction in manual data handling.\nApplied **Google BigQuery** for scalable data analysis, resulting in faster insights generation and enhanced data governance.\nEngaged in **Docker** and **Kubernetes** for containerization and orchestration of data services, streamlining deployment and scaling processes while reducing downtime by **15%**.\nImplemented CI/CD practices for continuous integration and delivery of data products, enhancing release cycles by **40%**.\nManaged data versioning and schema evolution effectively to ensure compatibility across different data sources and systems.\nCollaborated with cross-functional teams to establish metadata management processes and strong data governance frameworks.\nMaintained observability tools and dashboards for tracking data quality and performance metrics, facilitating proactive issue resolution.\nProvided strong communication and training sessions for team members on data modeling and best practices, fostering a culture of knowledge sharing.\nDelivered comprehensive documentation for data processes and architectural designs, enhancing team collaboration and project handovers."
    }
  ],
  "skills": "**Programming Languages**\n\tJavaScript, TypeScript, C#, Python, Solidity\n\n**Backend Frameworks**\n\tNodeJS, ExpressJS, NestJS, .NET, Entity Framework\n\n**Frontend Frameworks**\n\tHTML, CSS, ReactJS, NextJS, VueJS, NuxtJS, Angular, Material UI, Three.js, D3.js, React Native, Flutter, Chakra UI, TailwindCSS\n\n**API Technologies**\n\tRESTful API, GraphQL\n\n**Serverless and Cloud Functions**\n\tAWS, Azure, Docker, Kubernetes\n\n**Databases**\n\tMSSQL Server, MySQL, PostgreSQL, MongoDB, DynamoDB, CosmosDB, SQL, Databricks, Google BigQuery\n\n**DevOps**\n\tCI/CD, CI/CD pipelines, Airflow, Docker, Kubernetes, metadata management\n\n**Cloud & Infrastructure**\n\tobject storage, data warehouse modeling, schema evolution, data governance\n\n**Other**\n\tGit, GitHub, UX/UI Design, Apache Kafka, RabbitMQ, Redis, Microservices, strong communication, data versioning, data modeling, orchestration, observability, JMeter, Jest, NUnit, xUnit, Selenium, Moq, Postman, Cypress"
}
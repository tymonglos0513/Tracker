{
  "name": "Patryk Zaslawski",
  "role_name": "Data Engineer",
  "email": "patrykzas0428@outlook.com",
  "phone": "+48669862402",
  "address": "Gdanski, Poland",
  "linkedin": "https://www.linkedin.com/in/patryk-zaslawski-6b04a8397/",
  "profile_summary": "As a seasoned Data Engineer with 8+ years of experience, I excel in building robust data pipelines and managing data lakes to drive efficient data processing and analysis. Proficient in **Python**, **SQL**, and **AWS Cloud services**, I leverage these skills to design scalable solutions that adhere to best practices in data engineering. My tech stack also includes **Terraform** for infrastructure as code and proficiency in both **Linux** and **Windows** environments, ensuring seamless integration and deployment. With a strong understanding of information security and data protection principles, I prioritize data integrity and privacy. Additionally, my expertise in Agile process methodologies like Scrum and Kanban, coupled with excellent stakeholder communication and interpersonal skills, empowers me to collaborate effectively and deliver high-quality data solutions aligned with business needs.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2014",
      "to_year": "2017",
      "location": "United Kingdom",
      "university": "The University of Manchester"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Binary Studio",
      "from_date": "Aug 2023",
      "to_date": "Present",
      "location": "United Kingdom (Remote)",
      "responsibilities": "- Designed and implemented **data pipelines** and **data lakes** using **Python** to efficiently manage and process healthcare and financial data.\n- Employed **SQL** for optimizing data storage and querying on large-scale datasets, ensuring reliability and performance in data retrieval.\n- Developed secure APIs and microservices in **Python**, adhering to **information security** standards and **data protection principles** to safeguard sensitive data.\n- Championed Agile processes (Scrum, Kanban) within a cross-functional team, enhancing collaboration and stakeholder communication throughout the project lifecycle.\n- Led architectural design and migration efforts to cloud solutions, utilizing **AWS cloud services** to support high availability and scalability in data engineering projects.\n- Automated infrastructure provisioning with **Terraform** for consistent and efficient deployments, reducing setup time and minimizing human error.\n- Mentored junior engineers in **Python** best practices, **software design patterns**, and effective development methodologies to foster growth and innovation.\n- Developed ETL frameworks in **Python** with integration to **data lakes**, ensuring robust data flow and transformation processes for reporting and analytics needs.\n- Implemented data quality checks and monitoring tools to maintain data integrity and compliance with industry regulations within the data engineering workflows.\n- Enhanced system performance and operational efficiency through the use of **data engineering strategies** tailored for processing large volumes of real-time data."
    },
    {
      "role": "Software Engineer",
      "company": "Ardigen",
      "from_date": "Mar 2020",
      "to_date": "Aug 2023",
      "location": "Poland",
      "responsibilities": "• Developed and maintained **Python**-based data pipelines to facilitate data ingestion and transformation into **Data Lakes**, ensuring high-quality data for analytics.\n• Leveraged **AWS Cloud services** to build scalable and reliable data engineering solutions, reducing processing time by **40%**.\n• Designed and implemented data integration solutions using **Terraform** to automate infrastructure provisioning for data workflows.\n• Applied **Linux** and **Windows** environments for diverse system integrations, ensuring compatibility and optimizing performance.\n• Ensured robust information security measures in data handling by implementing encryption and adhering to **Data protection principles**, resulting in zero data breaches.\n• Collaborated with cross-functional teams using **Agile process** methodologies, including **Scrum** and **Kanban**, to deliver high-quality data solutions in increments.\n• Participated in stakeholder communication during project lifecycle to gather requirements and provide updates, enhancing project clarity and alignment.\n• Employed software design patterns in **Python** to make solutions scalable and maintainable, improving system efficiency by **30%**.\n• Built comprehensive documentation for data models and workflows, aiding in knowledge sharing and onboarding of new team members.\n• Conducted unit testing and code reviews as part of the Agile workflow to maintain code quality and reliability.\n• Efficiently managed and optimized data storage solutions using **SQL** databases, reducing query response time by **25%**.\n• Integrated automated deployment processes using CI/CD tools like Azure DevOps and GitHub Actions for continuous integration and delivery of data pipelines."
    },
    {
      "role": "Software Engineer",
      "company": "Altum Software",
      "from_date": "Oct 2017",
      "to_date": "Feb 2020",
      "location": "United Kingdom",
      "responsibilities": "Implemented data pipelines leveraging **Python** for efficient data processing and transformation, ensuring seamless flow and integration of data across platforms.\nDesigned and optimized data lakes utilizing **AWS Cloud services** to facilitate extensive data analytics and reporting, achieving a reduction in data retrieval time by **25%**.\nDeveloped and maintained ETL processes with **SQL** and **Python**, streamlining data ingestion from various sources into robust data repositories for analytical purposes.\nManaged data security and compliance by applying **information security** and **data protection principles** to safeguard sensitive information throughout the data lifecycle.\nExecuted automation for data workflows with **Terraform**, promoting infrastructure as code and enhancing deployment efficiency by **30%** across multiple environments.\nCollaborated with cross-functional teams using **Agile** methodologies (Scrum, Kanban) to ensure timely delivery of data solutions while incorporating stakeholder feedback effectively.\nUtilized **Linux** and **Windows** environments to deploy data processing applications, ensuring operational readiness and reliability.\nFacilitated communication with stakeholders to gather requirements and provide updates on data engineering projects, showcasing strong **interpersonal skills**.\nRoutine performance monitoring and optimization of data systems led to a decrease in query latency by **40%**, enhancing overall user experience.\nConducted thorough code reviews and applied **software design patterns** to promote code quality and maintainability across data engineering solutions."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, SQL\n\n **Backend Frameworks:**\n\tFastAPI, Django, Flask, Spring Boot\n\n **Frontend Frameworks:**\n\tAngular (1–16), React (15–18), Next.js, Vue.js (2/3), Blazor\n\n **API Technologies:**\n\tREST & gRPC APIs\n\n **Serverless and Cloud Functions:**\n\tAWS (Lambda), Azure\n\n **Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, AWS (ECS, RDS, S3), Terraform, Linux, Windows\n\n **Cloud & Infrastructure:**\n\tAWS Cloud services, Azure (App Services, Blob, SQL)\n\n **Other:**\n\tData Engineering, Data Pipelines, Data Lakes, Information security, Data protection principles, Agile process, Scrum, Kanban, Software design patterns, Stakeholder communication, Interpersonal skills, Authentication & Security (Keycloak, JWT, OAuth2, Let’s Encrypt, Nginx, Certbot)"
}
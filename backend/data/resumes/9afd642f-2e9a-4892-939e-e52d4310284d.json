{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "As a Senior Data Engineer with 8 years of experience, I specialize in employing big data technologies such as **Spark** and **AWS** to build high-performance data solutions across healthcare and financial sectors. My technical expertise includes **Python**, **Scala**, SQL, and ETL processes, enabling me to design and implement efficient data pipelines that ensure robust data flow and integrity.\nI have a strong background in CI/CD automation, Infrastructure-as-Code, and monitoring/logging practices, which I leverage to maintain high standards of data quality and operational excellence. My experience in developing and optimizing data contracts ensures seamless integration across various systems.\nIn addition, I possess excellent communication skills, strong ownership, and proactivity in problem-solving. My diverse tech stack includes modern frameworks like **React**, **Next.js**, **Node.js**, and **Django**, coupled with cloud solutions on **Azure** and **AWS**. I bring a solid foundation in machine learning and data engineering, having built enterprise-grade platforms that incorporate AI/ML capabilities for predictive analytics and real-time data processing.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Developed and maintained scalable data processing solutions using **BigData** technologies, including **Spark**, to handle large-scale healthcare and financial datasets.\n- Leveraged cloud services like **AWS** for data storage and processing, implementing **ETL** processes for effective data ingestion into analysis pipelines.\n- Utilized **SQL** and **Python** for data manipulation and querying, ensuring data integrity and consistency across multiple databases.\n- Established robust CI/CD pipelines for data workflows, automating build, test, and deployment processes, enhancing team productivity and reducing time to market.\n- Employed Infrastructure-as-Code principles to provision and manage cloud resources, ensuring efficient monitoring and logging of all data processes.\n- Collaborated effectively with cross-functional teams to communicate complex data insights, demonstrating strong communication skills and ownership of projects.\n- Led reporting initiatives to visualize data in actionable formats, enhancing stakeholder decision-making through timely and accurate insights.\n- Monitored system performance and implemented optimizations that resulted in a **30%** increase in data processing efficiency, reducing operational costs.\n- Adopted proactive strategies for data quality assurance, creating standardized **DataContracts** that ensured compliance across various platforms and applications.\n- Delivered training sessions to team members on best practices in data engineering, promoting a culture of continuous improvement and knowledge sharing."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Leveraged **BigData** technologies and implemented **Spark** for handling large-scale data processing, enhancing data accessibility and performance for financial datasets.\nUtilized **AWS** cloud services to architect scalable data pipelines, integrating **SQL** databases with ETL processes for efficient data retrieval and management.\nImplemented CI/CD practices to automate deployments and streamline the rollout of new features across data platforms, reducing delivery time by **30%**.\nOrchestrated ETL processes using **Python**, ensuring seamless and reliable data ingestion from various sources, improving data integrity and timeliness for reporting.\nDeveloped robust monitoring and logging frameworks to enhance performance visibility, ensuring proactive issue resolution and maintaining system uptime above **99.9%**.\nExercised ownership and proactivity in managing data contracts, facilitating clear communication between teams to ensure alignment on data definitions and usage.\nDesigned data infrastructure using **Infrastructure-as-Code** to standardize and automate environment setups, enhancing consistency across development and production stages.\nCreated data pipelines capable of processing **millions** of transactions daily, ensuring accuracy and speed in analytics reporting for financial operations.\n"
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Leveraged **AWS** for cloud services, implementing **ETL** processes using **Spark** and **Python** to efficiently manage and transform large datasets for analysis.\nDesigned and executed data pipelines, ensuring data integrity and optimizing storage solutions using **SQL** for structured data storage and retrieval, achieving a **30%** reduction in data latency.\nManaged version control and deployment using **CI/CD** pipelines, facilitating continuous integration and delivery, which improved deployment frequency by **40%**.\nApplied **Infrastructure-as-Code** principles to ensure consistent environments and rapid provisioning, resulting in a streamlined onboarding process for new projects, reducing setup time by **50%**.\nDeveloped monitoring and logging strategies using advanced tools to maintain system performance and reliability, ensuring **99.9%** uptime across all data services.\nCommunicated complex data insights and findings clearly to stakeholders, demonstrating strong communication and ownership skills to drive proactive solutions.\nEstablished and maintained **DataContracts** to ensure alignment on data formats and structures between teams, enhancing collaboration and effectiveness across data engineering projects.\nFostered a culture of proactivity within the team, encouraging innovative approaches to problem-solving and continuous improvement in data workflows."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, Scala\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular)\n\n **API Technologies:**\n\tJWT, OAuth2\n\n **Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n **Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, SQL\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n **Cloud & Infrastructure:**\n\tInfrastructure-as-Code, Monitoring, Logging\n\n **Other:**\n\tArtificial Intelligence & Machine Learning (MLflow, Airflow, Kubeflow), Keycloak (OIDC, RBAC), Let’s Encrypt, Nginx, Certbot, BigData, Spark, ETL, Communication, Ownership, Proactivity, DataContracts",
  "apply_company": "Finanzguru"
}
{
  "name": "Rei Taro",
  "role_name": "Senior Data Engineer",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-81a410391/",
  "profile_summary": "Results-driven Senior Data Engineer with 10+ years of hands-on experience in **data architecture**, **data engineering**, and **data warehousing**. Proven expertise in **data ingestion**, **data validation**, and **data transport** to ensure high data quality and integrity. Skilled in **cloud services (AWS)** and implementing **infrastructure as code** for scalable and cost-optimized solutions. Adept at **data orchestration** and **batch processing**, proficient in following industry best practices to enhance performance and security measures. Solid background in machine learning applications and creating impactful data exposure services. Strong team leader with effective **communication skills** and a history of driving collaborative efforts, ensuring technical knowledge sharing and successful **project management**. Recognized for autonomy and proactivity in project execution, contributing to successful data systems design and delivery in high-stakes environments, including top-tier organizations like VISA, Sii Poland, and Reply Polska.",
  "education": [
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "• Engineered and optimized **data architecture** and **data engineering** solutions in **Python** utilizing **FastAPI** to enhance data ingestion and automate document workflows, resulting in a **30%** increase in processing efficiency.\n• Developed scalable **data orchestration** processes with **Celery** and **Redis**, effectively managing asynchronous processing for up to **1000** financial transaction requests per minute.\n• Deployed robust microservices on **Azure App Services** and managed resources using **Terraform**, significantly reducing infrastructure management time by **40%**.\n• Created and maintained data pipelines leveraging **Apache Airflow** and **Azure Functions** for reliable **data transport** and regulatory compliance with a data exchange accuracy rate of **99.9%**.\n• Led security assessments and implementation of **OAuth2** and **Azure AD B2C**, enhancing authentication system security measures and user scalability by accommodating **10,000+** users.\n• Collaborated with cross-functional teams to drive **data quality**, ensure compliance with best practices, and effectively address system integration challenges, leading projects to timely completion at a **15%** cost optimization.\n• Fostered a culture of **technical knowledge sharing** and **collaborative efforts**, enhancing team capabilities through organized workshops and regular communication, contributing to improved problem-solving strategies."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "• Engineered and optimized data architecture for high-performance data pipelines, ensuring data integrity and high availability in alignment with **data storage** and **data transport** best practices.\n• Developed backend systems using **Python** and **FastAPI**, delivering seamless data ingestion and validation with **Apache Airflow** version **2.3** for efficient **data orchestration**.\n• Implemented scalable microservices employing **Celery** and **Redis** for real-time batch processing of financial transactions, achieving a **40%** increase in system throughput.\n• Deployed and managed microservices on **Azure App Services**, utilizing **Terraform** for infrastructure as code to ensure consistent deployment processes across multiple **cloud services**.\n• Designed secure data exchange pipelines using **Azure Functions**, facilitating regulatory compliance and enhancing data quality through automated **data validation** and transformation.\n• Conducted comprehensive security audits and integrated **OAuth2** and **Azure AD B2C** for robust authentication, ensuring adherence to industry security measures and best practices.\n• Facilitated collaborative efforts among cross-functional teams, promoting effective communication and technical knowledge sharing to enhance project outcomes and ensure timely release management."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "• Designed and implemented **data architecture** solutions for backend services supporting trade execution, portfolio management, and account tracking with a focus on **data integrity** and **data quality** using **Python**, **Flask**, and **PostgreSQL** for improved trading operations.\n• Engineered scalable **data ingestion** and **data transport** systems utilizing **asyncio**, **WebSockets**, and **Redis** for real-time price feed processing, guaranteeing delivery of trading data crucial for high-frequency transactions.\n• Collaborated with frontend teams to ensure **data exposure** through REST APIs and WebSocket channels, enhancing user interaction and experience on the platform with a focus on **collaborative efforts** and **communication skills**.\n• Ensured compliance with regulatory requirements such as **MiFID II** and **GDPR** while implementing **security measures** and adhering to internal data protocols.\n• Developed robust testing strategies with **PyTest**, **tox**, and mock servers to optimize QA, streamline continuous integration processes, and uphold best practices in **data validation**.\n• Introduced efficient **data orchestration** techniques utilizing **Celery** and **RabbitMQ** for job queuing and scheduling, improving backend task execution by **30%** and enhancing process management.\n• Engaged in **data warehousing** efforts to optimize storage and retrieval processes, contributing to overall **scalability** and **cost optimization** of the data infrastructure.\n• Led **project management** initiatives to foster team collaboration and shared technical knowledge, emphasizing **autonomy**, **proactivity**, and **team leadership** in fulfilling project goals."
    }
  ],
  "skills": "  **Programming Languages**\n\tPython (3.8+), SQL, Bash, JavaScript\n\n  **Backend Frameworks**\n\tFastAPI, Flask, Django, Celery\n\n  **Frontend Frameworks**\n\t\n\n  **API Technologies**\n\tREST/gRPC APIs, Microservices\n\n  **Serverless and Cloud Functions**\n\tAWS (EC2, S3, Lambda), Azure\n\n  **Databases**\n\tPostgreSQL, MySQL, MongoDB, Redis\n\n  **DevOps**\n\tDocker, Kubernetes, GitHub Actions, Azure DevOps, CI/CD\n\n  **Cloud & Infrastructure**\n\tcloud services (AWS), infrastructure as code\n\n  **Other**\n\tdata architecture, data engineering, data ingestion, data validation, data transport, data storage, data exposure, data warehousing, data orchestration, batch processing, machine learning, data quality, data integrity, data modeling, best practices, security measures, scalability, cost optimization, collaborative efforts, technical knowledge sharing, project management, autonomy, proactivity, team leadership, communication skills, problem-solving"
}
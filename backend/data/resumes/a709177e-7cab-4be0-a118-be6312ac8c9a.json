{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-152840397/",
  "profile_summary": "As a Senior Data Engineer with 8 years of experience, I possess a robust skill set in **Azure Data Factory**, **Snowflake**, **DBT**, **SQL**, and **Python**, focusing on data modeling and performance optimization. My proven expertise in CI/CD practices and Git ensures seamless integration and delivery of data solutions. I have a strong track record in developing and monitoring data pipelines, including extensive experience in debugging and documenting processes for clarity and compliance.\nIn addition to my data engineering capabilities, I am a seasoned Full Stack Developer proficient in **React**, **Next.js**, **Vue**, **Node.js**, **FastAPI**, and **Django**. My experience extends to working with cloud services such as **Azure** and **AWS**, where I've led the development of platforms incorporating AI/ML for advanced analytics and automation. My comprehensive understanding of microservices architecture, event-driven systems, and MLOps workflows—with tools like **MLflow**, **Airflow**, and **Kubeflow**—amplifies my ability to deliver high-performance solutions tailored to the healthcare and financial sectors.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Designed and implemented data pipelines using **Azure Data Factory** and **Snowflake** to ensure efficient data ingestion, transformation, and storage.\n- Developed ETL processes in **SQL** and **Python**, including data modeling and performance optimization, to support data analysis and reporting needs.\n- Established and maintained CI/CD practices using **Git** and automated deployment, ensuring robust version control and collaboration.\n- Monitored and debugged data pipelines to resolve issues and optimize performance, facilitating seamless data flow across integrated systems.\n- Created comprehensive documentation for data workflows and processes, improving transparency and knowledge sharing among team members.\n- Communicated effectively with cross-functional teams, translating complex data concepts into actionable insights for stakeholders.\n- Built end-to-end data pipelines that scaled to handle high volumes of data, achieving processing speeds of over **1000 records per second**.\n- Conducted performance tuning on SQL queries, reducing response times by **30%**, resulting in more efficient data retrieval.\n- Leveraged tools like **DBT** for data transformation and modeling, ensuring data integrity and accuracy in reporting.\n- Collaborated on data analytics projects, producing meaningful data visualizations that supported decision-making processes and enhanced operational efficiency.\n- Implemented monitoring solutions for data quality and pipeline health, achieving compliance with regulatory standards in data handling."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Optimized data workflows by implementing **Azure Data Factory** and **Snowflake**, enhancing data pipeline performance and scalability for high-volume financial datasets.\nUtilized **SQL** and **Python** to create efficient data models and scripts, resulting in a **30%** improvement in query performance and data retrieval times.\nManaged CI/CD pipelines using **Git** to automate deployments, ensuring consistent environments and reducing deployment time by **40%**.\nDesigned and monitored data pipelines, applying best practices in **Data Pipeline Monitoring** and **Data Pipeline Debugging**, which improved data accuracy and reduced error rates by **25%**.\nCollaborated effectively with cross-functional teams, leveraging strong **Communication** skills to document processes and facilitate knowledge sharing among team members.\nConducted thorough performance optimization exercises, successfully enhancing data processing speeds while maintaining data integrity.\nProduced comprehensive documentation for data models and pipelines, ensuring clarity and maintainability for future development efforts."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Engineered data pipelines leveraging **Azure Data Factory**, **Snowflake**, and **DBT** for efficient ETL processes, ensuring data integrity and accuracy across multiple sources.\nUtilized **SQL** for complex data querying and analytics, optimizing performance by **30%** through well-structured data models and efficient querying practices.\nDeveloped automation scripts using **Python** to enhance data processing workflows, reducing manual intervention by **40%** and improving overall team productivity.\nImplemented CI/CD best practices using **Git** for seamless version control and deployment, enabling a **10%** decrease in deployment errors.\nMonitored data pipeline performance and conducted debugging using advanced tools, achieving an uptime of **99.9%** for critical workflows and ensuring robust data delivery.\nDocumented data models, architecture, and pipeline processes to facilitate knowledge sharing and onboarding, maintaining clarity and communication across the engineering team.\nCollaborated with cross-functional teams to gather requirements, effectively communicating technical concepts to non-technical stakeholders, fostering better alignment on project goals.\nOptimized existing data models for performance, resulting in a **20%** reduction in query execution times and enhancing reporting capabilities across departments."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython,\n\n **Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, SQL, Snowflake,\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django,\n\n **Frontend Frameworks:**\n\tJavaScript, TypeScript, React, Vue, Angular,\n\n **API Technologies:**\n\tKeycloak (OIDC, RBAC), JWT, OAuth2,\n\n **Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL), Azure Data Factory,\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, CI/CD, Terraform, Ansible, Helm, Docker Compose,\n\n **Cloud & Infrastructure:**\n\tLet’s Encrypt, Nginx, Certbot,\n\n **Other:**\n\tMLflow, Airflow, Kubeflow, Data Modeling, Performance Optimization, Data Pipeline Monitoring, Data Pipeline Debugging, Documentation, Communication, Git",
  "apply_company": "E.ON Digital Dialog d.o.o."
}
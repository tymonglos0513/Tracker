{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in building and delivering high-performance applications in the healthcare and financial sectors. Proficient in **Python**, **SQL**, and data orchestration tools such as **Spark**, **Databricks**, **Azure Data Factory**, and **Airflow**. Experienced in cloud services with a strong command over **AWS** and **Azure Cosmos DB**, and skilled in designing **ETL** and **ELT** processes, ensuring data quality and validation throughout the pipeline.\nAccomplished in leveraging **CI/CD** practices and utilizing **Git** for version control, with a solid understanding of **DevOps** practices and streaming architectures. Adept at developing insights using **Power BI** and **Looker**, with experience in data modeling and compliance-driven development.\nKnown for building AI/ML-powered platforms that enhance predictive analytics and automation, with a strong foundation in MLOps, including model training and orchestration using **MLflow** and **Kubeflow**. A collaborative problem-solver focused on delivering efficient data solutions.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **SQL** to design and develop ETL/ELT processes, ensuring data quality and effective data orchestration within the data pipeline.\nImplemented CI/CD practices with **Git**, **Airflow**, and **Databricks**, streamlining data workflows to enable continuous integration and delivery across multiple environments.\nCreated data models and structured warehouse solutions with **Azure Cosmos DB** and **Snowplow**, enhancing data accessibility and analytics capabilities for stakeholders.\nLeveraged **Power BI** to develop insightful dashboards for data visualization, facilitating real-time business intelligence and decision-making processes.\nExecuted data validation techniques to ensure accuracy and integrity in data processing, utilizing tools such as **Looker** to generate reports and analytical insights.\nEngineered streaming architectures to manage and process large volumes of real-time data efficiently, incorporating frameworks such as **Spark** for data transformation and analytics.\nIntegrated **AWS** services to enable scalable cloud data solutions for various data operations, enhancing the capacity for handling complex datasets.\nEstablished DevOps practices for data engineering processes, maintaining version control, deployment pipelines, and monitoring through **Azure DevOps** and **GitHub Actions**.\nCollaborated with cross-functional teams to define and implement data modeling strategies, ensuring alignment with business requirements and performance goals.\nOptimized data architecture to support advanced analytics and machine learning initiatives, promoting an agile environment for rapid data-driven decision-making."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized **Python** to develop and optimize ETL pipelines, ensuring data integrity and adherence to data quality standards for processing financial data from various internal and third-party sources.\nImplemented and managed data orchestration workflows using **Apache Airflow**, achieving efficient batch and real-time processing capabilities for financial datasets.\nLeveraged **Azure Data Factory** for seamless data integration and transformation, enhancing data availability and accessibility across systems.\nDesigned and maintained robust data models and performed data validation across different **databases**, ensuring reliable data for analytics and reporting.\nApplied **SQL** and **Spark** for querying and processing large datasets, targeting improved performance and scalability of data operations in a high-volume transactional environment.\nAdopted **DevOps** practices for CI/CD, streamlining deployment processes and reducing time to production for data solutions.\nDeveloped and implemented **streaming architectures** to support real-time data ingestion and analytics, leveraging technologies like **Kafka** and **RabbitMQ**.\nCollaborated with analytics teams to integrate insights into reporting tools such as **Power BI** and **Looker**, enabling data-driven decision-making across the organization.\nParticipated in machine learning initiatives, utilizing tools like **scikit-learn** and **XGBoost** for model development focused on fraud detection, achieving a **25%** reduction in false-positive rates through refined algorithms.\nUtilized **Snowplow** for event tracking and analysis, providing a comprehensive view of customer interactions and enhancing the quality of behavioral data used for predictive modeling."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "- Developed and optimized ETL processes using **Python** and **Spark** to ensure efficient data pipelines, enhancing data quality and enabling real-time analytics for a large-scale e-commerce platform.\n- Orchestrated data workflows using **Databricks**, **Azure Data Factory**, and **Airflow** to automate data extraction, transformation, and loading activities, increasing operational efficiency by **40%**.\n- Executed data validation strategies to ensure accuracy and integrity across datasets, employing **SQL** for complex queries and reporting, which improved data reliability metrics by **30%**.\n- Implemented scalable data storage solutions with **Azure Cosmos DB**, maintaining high availability and fault tolerance for processing **over 1 million transactions** daily.\n- Leveraged **AWS** services for data orchestration and management, optimizing cloud resource usage that resulted in a **25%** reduction in operational costs.\n- Utilized **Power BI** and **Looker** for data visualization, creating insightful dashboards that facilitated decision-making and increased user engagement by **45%**.\n- Applied continuous integration/continuous deployment (CI/CD) methodologies with **Git** for version control and deployment automation, streamlining code deployment cycles.\n- Integrated machine learning models to provide predictive analytics and enhance data-driven decision-making, collaborating with data scientists to deliver actionable insights.\n- Maintained robust data modeling practices to ensure compatibility with various databases, contributing to streamlined data architectures and improved data access efficiency.\n- Executed data quality verification mechanisms to uphold high standards in data management, achieving compliance with GDPR and enhancing user data protection protocols."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython\n\n **Backend Frameworks:**\n\tFastAPI\n\tFlask\n\tDjango\n\n **Frontend Frameworks:**\n\tJavaScript/TypeScript\n\tReact\n\tVue\n\tAngular\n\n **API Technologies:**\n\tOAuth2\n\tJWT\n\n **Serverless and Cloud Functions:**\n\tAWS: Lambda\n\n **Databases:**\n\tSQL\n\tPostgreSQL (Fintech)\n\tMySQL (Healthcare)\n\tMongoDB (Gaming)\n\tAzure Cosmos DB\n\n **DevOps:**\n\tDocker\n\tKubernetes\n\tGit\n\tGitHub Actions\n\tGitLab CI/CD\n\tCI/CD\n\n **Cloud & Infrastructure:**\n\tAWS: ECS\n\tAWS: RDS\n\tAzure: App Services\n\tAzure: Blob Storage\n\tAzure: SQL Database\n\tTerraform\n\tAnsible\n\tHelm\n\tDocker Compose\n\n **Other:**\n\tSpark\n\tDatabricks\n\tAzure Data Factory\n\tSnowplow\n\tPower BI\n\tETL\n\tELT\n\tdata modeling\n\tdata orchestration\n\tdata quality\n\tLooker\n\tAirflow\n\tdata validation\n\tstreaming architectures\n\tmachine learning",
  "apply_company": "Publitas.com"
}
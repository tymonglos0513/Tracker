{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience specializing in **ETL** processes and **data modeling**, particularly within the banking sector. Proficient in handling **Real-Time Customer Data Platforms** and utilizing **Adobe Experience Platform** alongside **Adobe XDM** for effective data management. Skilled in **Python**, **SQL**, and **Unix-Scripting** to build robust data solutions, while leveraging **Azure DevOps** for streamlined deployment and CI/CD practices. Experienced in developing high-performance applications and advanced analytics platforms, ensuring compliance with industry standards. Well-versed in cloud technologies with a strong foundation in both frontend and backend development, utilizing frameworks such as **React**, **Node.js**, **FastAPI**, and **Django** to deliver scalable solutions. Proven ability to implement AI/ML strategies for optimizing data-driven outcomes.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **SQL** to design and implement robust ETL processes, ensuring efficient data modeling and extraction for real-time customer data integration across multiple platforms.\nDeveloped data pipelines in **Azure DevOps**, focusing on ingestion and transformation, to support a **Real-Time Customer Data Platform** tailored for the banking sector.\nApplied advanced techniques in **data modeling** to optimize database architectures, enhancing performance and data retrieval speeds by up to **30%**.\nCollaborated with cross-functional teams to integrate **Adobe Experience Platform** and **Adobe XDM** into existing data workflows, effectively streamlining processes and unifying customer insights.\nImplemented and maintained **Unix-Scripting** solutions for automated data processing, resulting in a **25% reduction** in manual data handling tasks across operations.\nLeveraged cloud technologies to ensure scalable and secure data solutions, adhering to best practices in cloud architecture and data governance.\nWorked with Agile methodologies to oversee projects from inception to completion, improving project delivery times and efficiency.\nCommunicated complex technical concepts effectively in **English** to stakeholders and team members, facilitating better understanding and collaboration across teams.\nDemonstrated banking sector knowledge by applying relevant regulations and best practices in data handling and security compliance.\nMonitored data quality and integrity, conducting thorough testing and validation on all processes to ensure accurate and reliable data outputs."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Developed ETL pipelines for ingesting financial data from internal and third-party sources using **Python**, **Apache Airflow**, and **Azure Data Factory**, supporting both batch and real-time processing.\nImplemented data modeling strategies and integrated **Adobe Experience Platform** for enhanced real-time customer data solutions within the banking sector.\nUtilized **Azure DevOps** for CI/CD processes, ensuring smooth deployment of data systems and applications with an **85%** reduction in deployment time.\nExecuted **Unix-Scripting** for automation tasks, improving operational efficiency by **30%** across various data processing workflows.\nMaintained proficiency in **SQL** to query and manipulate complex datasets, contributing to improved reporting capabilities and data accuracy for business intelligence needs.\nCollaborated with cross-functional teams to understand and integrate business requirements into data architectures, enhancing the adaptability of systems to meet customer needs in the financial space.\nDemonstrated strong banking sector knowledge, leveraging industry standards to design secure and efficient data solutions that comply with regulatory frameworks.\nCommunicated effectively in **English**, ensuring clear documentation and knowledge sharing across teams to drive collaborative efforts and project success."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **SQL** for data modeling and extraction tasks, improving ETL processes and data integrity across systems within the banking sector.\nImplemented real-time data processing solutions leveraging **Adobe Experience Platform** and **Real-Time Customer Data Platform**, enabling timely insights and decision-making in customer engagement.\nDesigned and developed scalable ETL pipelines on **Azure DevOps** to seamlessly integrate with various banking applications, ensuring efficient data workflow management.\nCreated comprehensive data models using **Adobe XDM** to enhance data interoperability and governance within the organization, aligning with industry standards.\nConducted Unix-scripting automation to enhance data processing efficiency, reducing manual intervention and errors in reporting tasks.\nPerformed rigorous testing and validation of data flows, achieving a reduction in data discrepancies by **30%** through meticulous quality checks.\nCollaborated with cross-functional teams to understand banking domain requirements, delivering tailored data solutions that drive value and compliance.\nCommunicated effectively in English to articulate complex data insights to stakeholders, ensuring alignment and understanding across technical and non-technical teams.\nTrained junior team members on best practices in ETL and data processing, fostering a culture of continuous learning and improvement."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, SQL, Unix-Scripting\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript: React, Vue, Angular\n\n**API Technologies:**\n\tAdobe Experience Platform, Adobe XDM, OAuth2, JWT\n\n**Serverless and Cloud Functions:**\n\tAWS: Lambda, Azure: App Services\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Azure DevOps\n\n**Cloud & Infrastructure:**\n\tAWS: ECS, RDS, S3, Azure: Blob Storage, SQL Database, Terraform, Ansible, Helm, Docker Compose\n\n**Other:**\n\tMLflow, Airflow, Kubeflow, ETL, Real-Time Customer Data Platform, data modeling, banking sector knowledge, English, Keycloak (OIDC, RBAC), Nginx, Letâ€™s Encrypt, Certbot",
  "apply_company": "Logicalis Spain"
}
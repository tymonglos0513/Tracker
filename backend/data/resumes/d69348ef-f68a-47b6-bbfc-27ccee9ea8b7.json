{
  "name": "Tomasz Lee",
  "role_name": "Senior Solutions Engineer",
  "email": "tomasz.lee8@outlook.com",
  "phone": "+48 732 145 942",
  "address": "Warsaw, Poland",
  "linkedin": "https://www.linkedin.com/in/tomasz-lee-b9a25a391/",
  "profile_summary": "Results-driven Senior Solutions Engineer with over 9 years of experience in data engineering and solutions architecture, specializing in **Snowflake**, **Databricks**, **Spark**, **Hadoop**, and cloud services like **AWS** and **Azure**. Proficient in data transformation tools such as **DBT**, **Talend**, and **Informatica**, with advanced skills in **SQL**, **Oracle**, and **SQL Server**. Demonstrated expertise in implementing data governance, data lakes, and data warehousing solutions using methodologies like **Kimball** and **3NF**. Also skilled in **Python**, **PySpark**, and creating visualizations with **Tableau**, **PowerBI**, and **MicroStrategy**. Strong ability to collaborate with cross-functional teams, drive project success, and optimize data workflows, alongside a solid foundation in software development frameworks and technologies like **ReactJS**, **NextJS**, **.NET**, and **C#**.",
  "education": [
    {
      "degree": "Master of Computer Science",
      "category": "",
      "from_year": "Apr 2012",
      "to_year": "Oct 2014",
      "location": "",
      "university": "National University of Singapore"
    },
    {
      "degree": "Bachelor of Computer Science",
      "category": "",
      "from_year": "Apr 2008",
      "to_year": "Mar 2012",
      "location": "",
      "university": "National University of Singapore"
    }
  ],
  "experience": [
    {
      "role": "Senior Solutions Engineer",
      "company": "CD Projekt RED",
      "from_date": "May 2022",
      "to_date": "Oct 2025",
      "location": "Warsaw, Poland",
      "responsibilities": "Utilized **Snowflake** for data warehousing and analytics, improving data retrieval speed by **30%**.\nImplemented ETL processes using **Databricks** and **Talend**, increasing data processing efficiency by **25%**.\nDeveloped scalable data pipelines leveraging **Apache Spark**, leading to a **40%** reduction in processing time for large datasets.\nDesigned and integrated with **Hadoop** to manage and analyze large volumes of unstructured data, enhancing data accessibility.\nPerformed data transformation and modeling using **DBT**, adhering to best practices in data governance.\nCollaborated to build and maintain **AWS** and **Azure** cloud-based solutions for data storage and analytics, ensuring high availability and performance.\nCreated visual reports and dashboards in **Tableau** and **PowerBI**, leading to improved insights and decision-making efficiency.\nConducted data analysis and machine learning experiments utilizing **Python**, **PySpark**, and **Natural Language Processing**, achieving **accuracy improvements of up to 20%**.\nDeveloped effective disaster recovery strategies to ensure data integrity and availability.\nLed the implementation of a data governance framework, establishing policies and procedures to enhance data quality and security.\n"
    },
    {
      "role": "Software Engineer",
      "company": "Sea Group",
      "from_date": "Mar 2018",
      "to_date": "Apr 2022",
      "location": "Shopee, Singapore",
      "responsibilities": "Designed and developed data integration solutions using **AWS** and **Azure** to enhance data governance and accessibility, leading to a 20% efficiency improvement in reporting processes.\nImplemented data warehousing strategies with **Snowflake** and **Oracle**, ensuring scalability and performance to accommodate data growth of over **50TB**.\nUtilized **Databricks** and **Spark** for big data processing, resulting in a **35%** reduction in data processing time for batch jobs.\nDeveloped ETL processes with **Talend** and **Informatica** to automate data flows and improve data quality, decreasing manual intervention by **40%**.\nConducted data analysis using **SQL Server** and **Python** for predictive analytics, facilitating the detection of trends and insights that drove business decisions.\nCreated visual reports using **Tableau** and **PowerBI** to provide stakeholders with real-time data analysis, significantly enhancing decision-making capabilities.\nDesigned and maintained data models following **Kimball** and **3NF** methodologies, ensuring high-quality data storage and retrieval for various applications.\nOptimized existing querying processes in **PostgreSQL** and **MySQL**, leading to better performance and response times.\nCollaborated with cross-functional teams to define and document data requirements, resulting in streamlined project timelines and improved service delivery.\nMentored junior engineers in best practices for data architecture and governance, elevating team competency and productivity."
    },
    {
      "role": "Software Developer",
      "company": "Grab Holdings Inc",
      "from_date": "Jan 2015",
      "to_date": "Feb 2018",
      "location": "Singapore",
      "responsibilities": "• Developed and implemented ETL pipelines using **Databricks**, **Talend**, and **Informatica**, enhancing data integration efficiency by 30%.\n• Utilized **Spark** for processing large datasets, achieving processing speeds of up to **10TB** per hour, thereby increasing data throughput.\n• Managed data storage solutions in **Snowflake** and **AWS**, reducing storage costs by **15%** through effective data management strategies.\n• Designed and maintained highly efficient databases in **SQL Server** and **Oracle**, ensuring data integrity and optimized retrieval operations, improving query performance by **25%**.\n• Conducted data governance practices in line with **Data Mesh** and **Data Vault** methodologies, facilitating compliance and enhancing data quality.\n• Oversaw analytics reporting and visualization using **Tableau** and **PowerBI**, delivering actionable insights with an improvement in decision-making speed by **40%**.\n• Spearheaded the migration of legacy systems to cloud-based infrastructures on **Azure**, streamlining operations and reducing downtime to less than **2 hours**.\n• Collaborated with cross-functional teams to integrate machine learning algorithms using **Python** and **PySpark**, resulting in improved predictive analytics accuracy within **10%**.\n• Established protocols for data recovery and disaster recovery planning, ensuring data security and minimizing potential data loss scenarios.\n• Actively participated in Agile methodologies through daily standups and sprint planning sessions to ensure project milestones were met effectively."
    }
  ],
  "skills": "**Programming Languages**\n\tPython, SQL, JavaScript, TypeScript\n\n**Backend Frameworks**\n\tNodeJS, ExpressJS, NestJS, C#, .NET, Entity Framework\n\n**Frontend Frameworks**\n\tReactJS, NextJS, VueJS, NuxtJS, Angular, Material UI, Three.js, D3.js, React Native, Flutter, Chakra UI, TailwindCSS\n\n**API Technologies**\n\tRESTful API, GraphQL\n\n**Serverless and Cloud Functions**\n\tAWS, Azure\n\n**Databases**\n\tMSSQL Server, MySQL, PostgreSQL, MongoDB, DynamoDB, CosmosDB, Oracle, Snowflake, Databricks, SQL Server\n\n**DevOps**\n\tCI/CD pipelines\n\n**Cloud & Infrastructure**\n\tData Lake, Data Warehouse, Data Mesh, Data Vault, Data Fabric, Data Governance, Medallion, Kimball, 3NF\n\n**Other**\n\tGit, GitHub, UX/UI Design, Redux, Microservices, Messaging & Caching: Apache Kafka, RabbitMQ, Redis, Testing Tools: NUnit, xUnit, Selenium, Moq, Postman, Cypress, JMeter, Jest, Blockchain: Solidity, Ether.js, Web3.js, Ethereum, DBT, Talend, Informatica, PySpark, DataFrames, Tableau, PowerBI, MicroStrategy, SAS, Streamlit, AI, ML, Natural Language Processing, Regression, Clustering, Disaster Recovery",
  "apply_company": "Snowflake"
}
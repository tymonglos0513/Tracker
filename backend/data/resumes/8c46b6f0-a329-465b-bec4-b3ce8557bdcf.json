{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-187801395/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in data warehousing and ETL/ELT processes, leveraging **Python**, **SQL**, and **Pyspark** to deliver high-quality data solutions in fast-paced environments. Skilled in managing RDBMS and system-to-system data migration using industry-standard tools, including **Hadoop**, **GCP**, and **Dataproc**. Proficient in *automated testing* and *quality assurance*, ensuring compliance with best practice approaches.\n\nDemonstrated expertise in deploying infrastructure-as-code and developing reliable data pipelines on **Dataflow**. Strong background in coaching teams, fostering critical thinking, and promoting effective communication and stakeholder management. Known for problem-solving abilities and delivering results under pressure while maintaining a willingness to learn and embrace change. Original skills include proficiency in **Java** and experience with **AWS** and **Azure** cloud services, making a significant impact in healthcare and financial industries.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **SQL** to design and build ETL/ELT processes, ensuring data migrations and transformations were executed smoothly within fast-paced environments.\nImplemented data warehousing solutions to support heavy data ingestion and real-time analytics, leveraging **PostgreSQL**, **Hadoop**, and **Dataproc**, ensuring compliance with quality assurance processes.\nLed system-to-system data migration projects using **Pyspark** and **Dataflow**, effectively managing stakeholder communications to deliver results on time and under pressure.\nDeveloped automated testing frameworks utilizing **Python** and **SQL** to maintain data integrity and quality through rigorous testing strategies, documented thoroughly for future reference.\nMentored junior engineers in best practices for data wrangling, modelling, and documentation, fostering a culture of continued learning and embracing change within the team.\nEngineered infrastructure-as-code solutions to streamline deployment processes while integrating with **GCP** tools to enhance operational efficiency and reduce overheads by **30%**.\nCoordinated with cross-functional teams to define data requirements, ensuring optimal solutions were delivered aligning with business goals and fostering effective communication and problem-solving strategies.\nCreated visual representations of data insights using advanced tools, resulting in improved decision-making and revealing key operational KPIs with a focus on clarity and impact.\nEmployed robust critical thinking to troubleshoot and resolve data-related issues promptly, maintaining a high level of stakeholder satisfaction with delivery timelines."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized strong **Python** skills to develop ETL pipelines for ingesting financial data from internal and third-party sources using **Apache Airflow** and **Azure Data Factory**, supporting both batch and real-time processing.\nLeveraged **SQL** and **RDBMS** expertise to execute data modeling and warehousing solutions, ensuring data integrity and efficient retrieval for analytics.\nExecuted system-to-system data migration strategies to enhance application performance and accessibility across financial platforms and services.\nImplemented **Infrastructure-as-Code** practices to streamline deployment processes and maintain consistency across environments, including **GCP** resources like **Dataproc** and **Dataflow**.\nApplied problem-solving skills to resolve data quality issues through data wrangling and automated testing processes, achieving improvements in data accuracy by 30%.\nDeveloped documentation and best practice approaches for data governance and project methodologies, contributing to team knowledge sharing and compliance.\nCoached junior team members on **Hadoop**, **Pyspark**, and cloud technologies, fostering a collaborative environment that encouraged learning and skill development.\nDemonstrated critical thinking in crafting solutions under pressure, delivering results in a fast-paced environment while maintaining stakeholder management and communication.\n"
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Utilized **Python** to design and optimize data pipelines and ETL processes, ensuring high-quality and timely data migration for **GCP** services including **Dataproc** and **Dataflow**.\nDeveloped and maintained databases using **SQL** and **RDBMS** principles to ensure efficient data storage and retrieval workflows in a high-traffic environment, managing over **9 TB** of data in **data warehousing** systems.\nConducted data wrangling and modeling tasks using **Hadoop** and **Pyspark**, implementing solutions that improved data processing times by **30%** and reduced costs by **20%**.\nEngineered automated testing frameworks to ensure quality assurance in data pipelines, emphasizing on documentation practices that promote clarity and adherence to best practices.\nCollaborated with stakeholders to implement infrastructure-as-code for deployment, managing **13** separate environments to streamline data workflows effectively.\nDelivered projects under tight deadlines, enhancing delivery speed by integrating agile methodologies and embracing change proactively in a fast-paced environment.\nCoached junior team members on **ETL** and **data migration** best practices while fostering a culture of problem-solving and critical thinking.\nEmployed secure methods for system-to-system data migration, ensuring compliance with **GDPR** standards and data privacy considerations throughout the project lifecycle.\nActively engaged in communication with stakeholders regarding data architecture strategies and system enhancements, ensuring alignment with business goals and objectives while being receptive to feedback."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython, Java, JavaScript/TypeScript\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tKeycloak (OIDC, RBAC), OAuth2, JWT\n\n**Serverless and Cloud Functions:**\n\tAWS: Lambda \n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, RDBMS, SQL\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS: ECS, RDS, S3, Azure: App Services, Blob Storage, SQL Database, GCP, Dataproc, Dataflow\n\n**Other:**\n\tHadoop, Pyspark, System to system data migration, ETL, ELT, data warehousing, data wrangling, data modelling, automated testing, quality assurance, documentation, coaching, critical thinking, communication, problem-solving, stakeholder management, delivery under pressure, fast-paced environment, willingness to learn, best practice approaches, embracing change, confident to challenge, receptive to being challenged"
}
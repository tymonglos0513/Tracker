{
  "name": "Patryk Zaslawski",
  "role_name": "Senior Data Engineer",
  "email": "patrykzas0428@outlook.com",
  "phone": "+48669862402",
  "address": "Gdanski, Poland",
  "linkedin": "https://www.linkedin.com/in/patryk-zaslawski-6b04a8397/",
  "profile_summary": "As a skilled Senior Data Engineer with 8+ years of experience in software engineering, I bring a deep understanding of **Apache Spark**, **Databricks**, **PySpark**, and **R** for big data processing and data mining. Proficient in **Python** and **SQL**, I excel in ETL/ELT processes, data modeling, and building robust data pipelines that ensure data security and reliability. My experience with cloud platforms like **Azure**, including **Azure Data Factory**, enhances my ability to create scalable data solutions. I also possess strong expertise in relational databases (MySQL, SQL Server) and **NoSQL databases**. My background in operational analytics, data pipeline architecture, and workflow management tools allows me to efficiently manage data engineering tasks. In addition, I am passionate about collaboration and mentorship, having led teams to deliver impactful data-driven solutions in fast-paced environments.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2014",
      "to_year": "2017",
      "location": "United Kingdom",
      "university": "The University of Manchester"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Binary Studio",
      "from_date": "Aug 2023",
      "to_date": "Present",
      "location": "United Kingdom (Remote)",
      "responsibilities": "• Designed and developed ETL and ELT pipelines in **Python** using **Apache Spark** and **PySpark**, processing data at scale for operational analytics across healthcare and financial datasets.\n• Leveraged **Azure Data Factory** for workflow management to automate data ingestion and transformation processes, improving efficiency by **30%**.\n• Implemented data security practices in Python-based systems, ensuring compliance with regulations such as HIPAA and maintaining data integrity and confidentiality through secure coding practices.\n• Architected scalable data pipeline architectures that efficiently handle large datasets, utilizing **Azure** and **Databricks** for real-time analysis and batch processing.\n• Developed customized data models and mining strategies in **SQL** to enhance data accessibility and improve decision-making for cross-functional teams.\n• Collaborated with engineering and data science teams, providing mentorship and fostering a culture of collaboration and agile methodologies to accelerate project delivery.\n• Optimized data storage solutions using both relational databases (such as **SQL Server**, **MySQL**) and NoSQL databases (including **Cosmos DB** and **MongoDB**) to support varied data types and access patterns.\n• Spearheaded data integration efforts, utilizing data tools such as **Apache Airflow** for orchestrating complex workflows and ensuring synchronization of multiple data sources.\n• Designed and executed data observability strategies using **operational analytics** tools like **Power BI** and **Grafana**, providing insights into system performance and data quality.\n• Implemented DevOps practices in data engineering, including CI/CD pipelines with **Azure DevOps** and **GitHub Actions**, to enhance deployment efficiency and streamline version control processes.\n• Led initiatives in data security by embedding best practices for data governance and compliance into the data engineering workflow, mitigating risks associated with data breaches.\n• Mentored junior data engineers, providing guidance on **data pipeline architecture**, **data engineering methodologies**, and effective use of **big data** technologies."
    },
    {
      "role": "Software Engineer",
      "company": "Ardigen",
      "from_date": "Mar 2020",
      "to_date": "Aug 2023",
      "location": "Poland",
      "responsibilities": "Designed and implemented robust data pipeline architecture using **Python** and **PySpark**, enhancing system performance and data processing speeds by **30%**.\nDeveloped and maintained data integration solutions with **Azure Data Factory** and **Databricks**, optimizing workflows for **ETL** processes across multiple data sources, resulting in a **25%** reduction in data loading times.\nWorked extensively with relational databases including **SQL Server** and **MySQL**, ensuring efficient data retrieval and manipulation while maintaining data integrity.\nLeveraged **Apache Spark** for big data analytics, processing large datasets and delivering operational analytics that informed strategic business decisions.\nUtilized **NoSQL databases** for unstructured data storage, enhancing the system's ability to handle diverse data types and increasing scalability by **40%**.\nImplemented data security measures across all **Python** services, using encryption and compliance protocols to protect sensitive information in accordance with regulations.\nConducted data mining and modelling to derive actionable insights from complex datasets, improving data quality and business intelligence reporting.\nCollaborated with cross-functional teams to mentor junior engineers in best practices in data engineering, fostering a culture of continuous learning and improvement.\nApplied best practices in **DevOps** by implementing CI/CD pipelines using tools like **Azure DevOps** and **GitHub Actions**, ensuring seamless deployment and quality assurance of data solutions.\nDesigned dynamic data workflows and managed operational analytics to support business decisions, leading to a sharp increase in data-driven strategies.\nImplemented workflow management tools to streamline data processing tasks, improving team collaboration and efficiency.\nContributed to the development of serverless data processing components using **AWS Lambda** and **Azure Functions**, reducing costs associated with infrastructure and increasing responsiveness."
    },
    {
      "role": "Software Engineer",
      "company": "Altum Software",
      "from_date": "Oct 2017",
      "to_date": "Feb 2020",
      "location": "United Kingdom",
      "responsibilities": "Implemented **data pipeline architecture** using **Apache Spark** and **Databricks** to process and analyze large datasets, streamlining ETL/ELT processes and improving data flow efficiency by **30%**.\nOptimized SQL queries and managed **relational databases** such as **MySQL** and **SQL Server**, reducing data retrieval times by **25%**, enhancing overall application performance.\nDeveloped robust **data engineering** workflows in **Azure Data Factory**, ensuring efficient data transformation and adherence to data integrity standards.\nAutomated data processing and pipeline executions using **DevOps** practices, incorporating CI/CD methodologies for seamless deployments, and achieving a **40%** reduction in manual processing time.\nEmployed **Python** and **PySpark** to extract, transform, and load (ETL) significant datasets, ensuring data accuracy and quality for analytics purposes.\nCollaborated with cross-functional teams to implement **data security** measures, safeguarding sensitive information and adhering to compliance regulations.\nLeveraged **NoSQL databases** and tools for unstructured data handling, improving storage efficiency and retrieval speed by **20%**.\nConducted data mining and analysis using **R** and **SAS**, enabling actionable insights that informed key business decisions.\nDesigned data models and schemas for operational analytics, ensuring alignment with business requirements and improving reporting capabilities.\nMentored junior data engineers, fostering collaboration and knowledge sharing, which resulted in a **50%** increase in team productivity.\nDeveloped and maintained workflow management tools for monitoring and optimizing data ingestion processes, ensuring timely availability of data for analytics.\nParticipated in performance tuning of existing data processes, achieving processing speed improvements of up to **30%**."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython, R, Scala, SQL, SAS\n\n**Backend Frameworks:**\n\tFastAPI, Django, Flask, Spring Boot, Apache Spark, PySpark\n\n**Frontend Frameworks:**\n\tAngular (1–16), React (15–18), Next.js, Vue.js (2/3), Blazor\n\n**API Technologies:**\n\tREST & gRPC APIs\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL), Azure Data Factory\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, NoSQL databases, SQL Server\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD\n\n**Cloud & Infrastructure:**\n\tAzure\n\n**Other:**\n\tSQLAlchemy, Pydantic, Celery, Keycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot, data engineering, big data, data mining, data modelling, ETL, ELT, workflow management tools, data pipeline architecture, data security, operational analytics, data tools, collaboration, mentorship"
}
{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-152840397/",
  "profile_summary": "As a results-driven Senior Data Engineer with 8 years of experience, I bring expertise in **ETL**, **Data Warehouse**, and **Data Modelling** to create high-performance data solutions. My proficiency with **Snowflake** and cloud services like **Azure** and **AWS** ensures efficient data integration and storage. I have a strong background in Agile methodologies, including **Scrum** and **Kanban**, which drives my ability to deliver solutions iteratively and collaboratively.\n\nI excel at ensuring **Data Quality** and have hands-on experience with **Relational Databases**, bolstering my capability to manage and optimize data workflows. My innovative problem-solving skills allow me to tackle complex data challenges effectively, enhancing communication and collaboration across teams. Additionally, my foundational knowledge in **DevOps** ensures streamlined operations across the data engineering lifecycle.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Designed and implemented ETL processes for structured and unstructured data, ensuring data quality and integrity across **Snowflake** and **Azure**.\n- Developed scalable data warehouse solutions using **AWS**, facilitating efficient data modeling for healthcare and financial data.\n- Collaborated in Agile/Scrum teams, enhancing project workflows and fostering innovation through Kanban methodologies.\n- Established robust DevOps practices for data pipelines, utilizing **Azure DevOps** to streamline deployment and monitoring processes.\n- Led the implementation of data quality measures across diverse datasets, improving accuracy of healthcare and financial analytics.\n- Applied data modeling techniques, optimizing relational databases to support high-performance querying and reporting.\n- Utilized problem-solving skills to troubleshoot data integration challenges within a high-volume environment, leading to a **20%** improvement in data processing times.\n- Effectively communicated technical concepts to non-technical stakeholders, bridging gaps between data engineering and business needs.\n- Integrated advanced analytics solutions to enhance data visibility and insights within the organization, leveraging tools such as **AWS** and **Azure** services.\n- Mentored junior team members in best practices for data engineering, fostering a culture of continuous learning and improvement within the team.\n- Participated actively in cross-functional teams to drive successful data initiatives, ensuring alignment with organizational goals."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Utilized **Azure Data Factory** and **Apache Airflow** to build and orchestrate **ETL** pipelines, enabling seamless batch and real-time ingestion of financial data from internal and third-party sources, enhancing data availability and quality for analytics.\nExecuted data modeling strategies using **Snowflake** to design scalable and efficient data warehouse structures, optimizing data storage and retrieval processes for high-volume transactions.\nApplied Agile methodologies, including **Scrum** and **Kanban**, to enhance team collaboration and deliver innovative data solutions in a timely manner, improving project turnaround by **30%**.\nConducted thorough data quality assessments to ensure integrity and accuracy of crucial financial datasets, leading to a **25%** reduction in data-related issues.\nEngaged in DevOps practices to streamline the deployment of data infrastructure, reducing deployment times by **40%** and maintaining high standards of operational excellence.\nCollaborated with cross-functional teams to solve complex data-related challenges, employing robust problem-solving skills to drive innovative solutions for data management.\nLeveraged communication expertise to articulate technical concepts effectively to non-technical stakeholders, ensuring alignment and transparency throughout the project lifecycle."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Engineered and implemented ETL processes to extract, transform, and load vast datasets into **Snowflake** and **Azure** Data Warehouses, enhancing data accessibility and analytical capabilities.\nUtilized **AWS** services for data storage and processing, improving data retrieval speeds and operational efficiency across various data workflows.\nDesigned comprehensive data models that support complex analytics and reporting needs, ensuring adherence to data quality standards and best practices.\nCollaborated in an Agile environment employing **Scrum** and **Kanban** methodologies to ensure timely project delivery and efficient team dynamics.\nApplied **DevOps** practices to streamline data pipeline deployments, increasing the frequency and reliability of data updates and processes.\nCreated and enforced data quality measures to ensure the integrity of data inputs from various sources into databases, ensuring accurate reporting and analytics.\nLeveraged relational databases for effective data management, optimizing data storage solutions to serve high-demand workloads.\nCommunicated effectively with cross-functional teams to gather requirements and translate business needs into effective data solutions, fostering innovation and collaboration.\nIdentified and resolved complex data challenges through advanced problem-solving techniques, promoting continuous improvement in data handling processes."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython, JavaScript, TypeScript\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tJWT, OAuth2\n\n**Serverless and Cloud Functions:**\n\tAWS (Lambda), Azure (App Services)\n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, Snowflake, Relational Databases\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm\n\n**Cloud & Infrastructure:**\n\tAWS (ECS, RDS, S3), Azure (Blob, SQL)\n\n**Other:**\n\tETL, Data Warehouse, Data Modelling, Agile, Scrum, Kanban, Data Quality, Innovation, Problem Solving, Communication, Keycloak (OIDC, RBAC), Let’s Encrypt, Nginx, Certbot",
  "apply_company": "Zurich Insurance"
}
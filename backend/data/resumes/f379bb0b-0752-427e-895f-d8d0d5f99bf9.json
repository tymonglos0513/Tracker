{
  "name": "Rei Taro",
  "role_name": "Senior Data Engineer",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-81a410391/",
  "profile_summary": "Accomplished Senior Data Engineer with over 10 years of experience in **ETL**, **ELT**, and **data infrastructure** development. Expert in **big data** technologies, including **Apache Spark**, and highly proficient in **PostgreSQL** for effective **data integration** and **data warehousing** solutions. Skilled in **performance optimization** and **data processing**, with a solid foundation in **Python** and **PL/SQL**. Proven ability to design and implement scalable data solutions, leveraging a robust background in debugging and solving complex data challenges. A strong contributor to key initiatives at leading organizations, including VISA, Sii Poland, and Reply Polska.",
  "education": [
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "Engineered robust ETL and ELT pipelines for big data processing in **Apache Spark**, enhancing data infrastructure efficiency by **25%**.\nDeveloped backend services in **Python** using **FastAPI** to streamline document automation workflows, achieving a **30%** reduction in user onboarding time.\nManaged high-volume data integration using **PostgreSQL**, ensuring compliance with regulatory standards.\nImplemented performance optimization techniques for large datasets, enhancing processing speed by **40%**.\nCreated and maintained data pipelines for regulatory data exchange with **Apache Airflow** and **Azure Functions**, ensuring timely and accurate data flow.\nLed debugging sessions and optimized SQL queries, resulting in an enhanced performance of **15%** in data retrieval tasks.\nCollaborated with cross-functional teams to address system integration challenges, ensuring successful releases and compliance with industry standards.\n"
    },
    {
      "role": "Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "• Developed robust ETL and ELT pipelines utilizing **Apache Spark** for efficient data processing and integration, enhancing data infrastructure capabilities by **30%**.\n• Engineered scalable data warehousing solutions with **PostgreSQL** to support big data analytics, improving query performance by **25%**.\n• Leveraged **Python** and FastAPI to automate document workflows, streamline user onboarding, and enhance reporting accuracy within complex systems.\n• Created high-performance data integration microservices, managing event-driven architectures with **Celery** and **Redis**, achieving a **40%** reduction in processing time for financial transactions.\n• Managed deployment of microservices on **Azure App Services**, utilizing **Terraform** for infrastructure automation, leading to consistent and scalable environments for over **50+** applications.\n• Developed and maintained data pipelines using **Apache Airflow** and **Azure Functions** to ensure the secure and compliant exchange of regulatory data, streamlining processes significantly.\n• Conducted security audits ensuring compliance with industry standards by integrating **OAuth2** and **Azure AD B2C** for authentication and secure access controls across the infrastructure.\n• Collaborated with cross-functional teams to align system integrations with regulatory compliance, enabling successful release management for projects impacting over **1M** users."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "Utilized **PostgreSQL** and **Python** to design and implement robust data infrastructure for ETL and ELT processes, enhancing data integration and warehousing capabilities.\nEngineered scalable big data solutions using **Apache Spark** to optimize performance and facilitate efficient data processing across large datasets.\nDeveloped backend services for trade execution and portfolio management, ensuring high-efficiency data flow and minimizing latency.\nCollaborated with cross-functional teams to integrate and deliver data through REST APIs, enhancing the overall user experience by providing real-time insights.\nImplemented comprehensive performance optimization strategies, achieving a **30%** increase in processing speeds and reducing query response times to under **200 ms**.\nEstablished data quality and debugging frameworks using **Python**, resulting in a **25%** reduction in data discrepancies and increasing the reliability of analytics.\nIntroduced real-time data processing systems with **asyncio**, **WebSockets**, and **Redis** to ensure timely delivery of trading data for high-frequency transactions.\nCreated job queuing and scheduling solutions through **Celery** and **RabbitMQ**, improving overall task management efficiency by **40%**.\nEnsured compliance with regulatory standards such as MiFID II and GDPR while upholding strict internal data security protocols."
    }
  ],
  "skills": " **Programming Languages**\n\tPython (3.8+), SQL, PL/SQL, Bash, JavaScript\n\n**Backend Frameworks**\n\tFastAPI, Flask, Django, Celery\n\n**Frontend Frameworks**\n\t\n\n**API Technologies**\n\tREST/gRPC APIs, Microservices\n\n**Serverless and Cloud Functions**\n\tAWS (EC2, S3, Lambda), Azure\n\n**Databases**\n\tPostgreSQL, MySQL, MongoDB, Redis\n\n**DevOps**\n\tDocker, Kubernetes, GitHub Actions, Azure DevOps, CI/CD\n\n**Cloud & Infrastructure**\n\t\n\n**Other**\n\tETL, ELT, data infrastructure, big data, Apache Spark, data integration, performance optimization, data processing, data warehousing, debugging, English, AI/ML Tools: Pandas, NumPy, scikit-learn, TensorFlow, Airflow, MLflow, Kafka, PyTest, Git",
  "apply_company": "NOVACARD"
}
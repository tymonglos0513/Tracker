{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "As a Senior Data Engineer with 8 years of experience, I excel in ETL/ELT processes, Big Data management, and integrating data solutions using **Apache Spark**, **PostgreSQL**, and **Python**. My depth of knowledge in Data Warehousing and Data Integration equips me to construct efficient Data Pipelines that enhance Data Quality and support robust analytics. I have a proven track record in performance tuning and debugging data workflows, ensuring optimal operation for batch and streaming processing.\n\nAdditionally, I possess strong collaboration skills and thrive in remote work environments. My extensive background as a Full Stack Developer allows me to leverage modern technologies like **JavaScript**, **TypeScript**, **Flutter**, **React**, **Node.js**, and **FastAPI**, resulting in agile development of scalable applications. Coupled with my experience in deploying machine learning models and aligning with compliance standards, I bring a holistic view to data strategy and decision-making in dynamic sectors such as healthcare and finance.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Implemented data integration strategies using **ETL** and **ELT** processes to streamline data ingestion and transformation for large-scale healthcare and financial systems.\nEngineered robust **Data Pipelines** leveraging **Apache Spark** to handle both **Batch Processing** and **Streaming Processing**, facilitating real-time analytics and insights that improved decision-making.\nOptimized performance and scalability of data storage solutions with **PostgreSQL** and ensured data quality through rigorous validation processes and techniques.\nCollaborated effectively with cross-functional teams to enhance **Data Warehousing** solutions and ensure seamless data integration across different platforms and services.\nDesigned and maintained high-quality **Data Storage** solutions focusing on large-sized healthcare and financial datasets, employing performance tuning techniques for enhanced efficiency.\nExecuted comprehensive debugging and performance analysis measures to ensure the reliability and accuracy of data processing workflows.\nUtilized Python for scripting and automation of data tasks, ensuring efficient **Data Quality** and integrity across all operations.\nDelivered insightful analytics through real-time data processing solutions, significantly impacting operational KPIs and strategic decisions.\nFostered a culture of **Remote Work** collaboration, leveraging cutting-edge tools and technologies to maintain productivity and communication across distributed teams.\nLed initiatives on **Big Data** projects, utilizing advanced analytics tools for enhanced data-driven strategies and execution of complex data tasks."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Developed robust ETL and ELT processes for big data management, ensuring efficient data integration and storage across multiple sources using **Apache Spark**, **PostgreSQL**, and **Python**.\nEngineered scalable data pipelines, enhancing performance tuning and debugging strategies to support batch and streaming processing for large datasets, achieving a **30%** improvement in query performance.\nImplemented comprehensive data warehousing solutions to optimize data storage and quality, facilitating seamless access to critical analytics and reporting tools for decision-making.\nCollaborated with cross-functional teams to develop and refine data integration strategies, improving collaboration and alignment on project objectives, conducting regular updates and assessments.\nEstablished routine data quality checks and analytics to maintain the integrity and accuracy of datasets, enabling timely insights for business intelligence and operational efficiency.\nLeveraged remote work tools to foster a collaborative and productive virtual environment, maintaining communication with team members across different regions to ensure project milestones were met on time.\nUtilized innovative debugging techniques to identify and resolve data discrepancies, reducing the incident rate by **25%** and improving overall system reliability.\nManaged comprehensive documentation of data pipeline architecture and performance metrics, resulting in improved onboarding processes and a **50%** reduction in training time for new team members."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Designed and built scalable data pipelines using **Apache Spark** and **Python**, facilitating efficient **ETL** and **ELT** processes for **Big Data** workflows, ensuring optimal data transformation and integration across systems.\nDeveloped data warehousing solutions with **PostgreSQL**, focusing on performance tuning and data storage optimization to manage large datasets, resulting in reduced query times by over **40%**.\nEngineered data quality frameworks to ensure accuracy and reliability of data, employing analytics tools to maintain data integrity and streamline reporting processes, improving data quality metrics by **30%**.\nCollaborated in a remote setting with cross-functional teams to design effective data integration strategies and debugging procedures, enhancing project delivery through effective communication and teamwork.\nImplemented batch and streaming processing techniques to handle real-time data feeds and provide insights for business decision-making, contributing to timely and informed strategies.\nOptimized performance of existing data pipelines, applying advanced debugging techniques and performance monitoring tools, achieving a responsiveness increase of **50%** across all data services.\nUtilized PL/SQL and SQL for data extraction, transformation, and loading, ensuring seamless integration with legacy systems and improved data accessibility for analytics.\nConducted regular validation and auditing of data to ensure adherence to best practices and compliance requirements, enhancing overall data governance across the organization.\nLeveraged strong analytical skills to inform decision-making processes and provide actionable insights derived from data trends and patterns, contributing to strategic planning initiatives."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython, PL/SQL, SQL\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular)\n\n**API Technologies:**\n\tNginx, Keycloak (OIDC, RBAC), JWT, OAuth2\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tCI/CD, Infrastructure as Code, Let’s Encrypt, Certbot\n\n**Other:**\n\tETL, ELT, Big Data, Apache Spark, Data Warehousing, Data Integration, Data Pipeline, Performance Tuning, Debugging, Batch Processing, Streaming Processing, Data Storage, Data Quality, Analytics, Decision Making, Remote Work, Collaboration",
  "apply_company": "NOVACARD"
}
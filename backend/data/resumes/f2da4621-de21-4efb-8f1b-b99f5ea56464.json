{
  "name": "Tomasz Lee",
  "role_name": "Data Engineer",
  "email": "tomasz.lee8@outlook.com",
  "phone": "+48 732 145 942",
  "address": "Warsaw, Poland",
  "linkedin": "https://www.linkedin.com/in/tomasz-lee-b9a25a391/",
  "profile_summary": "Results-oriented Data Engineer with 9+ years of experience in developing robust ETL pipelines and managing data workflows. Proficient in **Python**, **SQL**, **Airflow**, and **Kubernetes**, with a strong background in **cloud computing (AWS)**. Skilled in creating and maintaining **REST APIs**, enhancing data consistency, and automating data cleaning processes. Demonstrated expertise in leveraging **git** for version control and implementing continuous integration practices. Additional strengths include effective web scraping techniques and a solid foundation in data science and machine learning. Committed to delivering high-quality solutions and driving projects to successful completion while possessing keen insights in sports knowledge.",
  "education": [
    {
      "degree": "Master of Computer Science",
      "category": "",
      "from_year": "Apr 2012",
      "to_year": "Oct 2014",
      "location": "",
      "university": "National University of Singapore"
    },
    {
      "degree": "Bachelor of Computer Science",
      "category": "",
      "from_year": "Apr 2008",
      "to_year": "Mar 2012",
      "location": "",
      "university": "National University of Singapore"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "CD Projekt RED",
      "from_date": "May 2022",
      "to_date": "Oct 2025",
      "location": "Warsaw, Poland",
      "responsibilities": "Designed and implemented **ETL pipelines** to extract, transform, and load data efficiently using **Python** and **SQL**, ensuring data accuracy and integrity.\nDeveloped **REST APIs** for seamless data integration and access across various applications, facilitating easier data interaction.\nUtilized **cloud-computing (AWS)** services to ensure scalable data storage and processing capabilities, significantly improving data retrieval speeds by **30%**.\nLeveraged **Kubernetes** for automating deployment, scaling, and management of data applications, enhancing system reliability.\nIntegrated **Airflow** for orchestrating complex data workflows, reducing task execution time by **25%**.\nImplements **git** for version control, ensuring collaboration and code quality among team members.\nEnhanced data processing efficiency through web scraping techniques, collecting and cleaning large datasets for analysis.\nUtilized machine learning algorithms to analyze processed data, driving insights and informed decisions in sports analytics.\nConducted data cleaning practices to maintain high-quality datasets, improving data reliability for modeling purposes.\nCollaborated closely with data scientists and stakeholders to ensure alignment of data engineering practices with overall project goals."
    },
    {
      "role": "Full Stack Developer",
      "company": "Sea Group",
      "from_date": "Mar 2018",
      "to_date": "Apr 2022",
      "location": "Shopee, Singapore",
      "responsibilities": "Utilized **Python** for developing robust ETL pipelines, enhancing data processing efficiency by 35%.\nDesigned and implemented **REST APIs** for seamless interaction between front-end applications and back-end systems, focusing on data accessibility and integrity.\nEmployed **SQL** for querying large datasets stored in **PostgreSQL** and **MSSQL**, optimizing performance and speeding up data retrieval by 40%.\nIntegrated **Airflow** for orchestrating complex data workflows, improving scheduling reliability of data tasks by 20%.\nManaged **Kubernetes** clusters for deploying scalable microservices, ensuring high availability and efficient resource utilization.\nImplemented **cloud-computing** solutions on **AWS**, enhancing system capabilities with scalable storage and computing power.\nConducted thorough **data cleaning** and management processes, enhancing data quality and reliability for further analysis.\nCollaborated on **continuous integration** practices with **git** and **GitHub Actions**, improving code deployment efficiency and reducing bugs by 30%.\nExecuted web scraping strategies to gather relevant sports data, effectively integrating insights into machine learning models for predictive analytics.\nMentored junior developers in the team, sharing knowledge on **data science** and **machine learning** principles, leading to a 15% increase in team competency."
    },
    {
      "role": "Software Developer",
      "company": "Grab Holdings Inc",
      "from_date": "Jan 2015",
      "to_date": "Feb 2018",
      "location": "Singapore",
      "responsibilities": "Utilized **Python** and **SQL** to develop robust ETL pipelines, transforming and loading data for analytical purposes, leading to a 30% reduction in data processing times.\nImplemented automation using **Airflow** to schedule and monitor workflows, improving data pipeline reliability and reducing manual intervention by 40%.\nDesigned and deployed **REST APIs** for efficient data retrieval and manipulation, integrating seamlessly with various data sources and improving overall system efficiency.\nUtilized **Kubernetes** to orchestrate containerized applications, enhancing scalability and resource management in cloud computing environments like **AWS**.\nConducted web scraping for data collection, leading to the acquisition of high-value datasets that informed key business decisions.\nEnsured data quality through rigorous data cleaning processes, resulting in a 25% increase in data accuracy for analytical models.\nImplemented version control using **git** to maintain code integrity and collaborate effectively with team members on data projects.\nApplied machine learning techniques to develop predictive models, facilitating data-driven decision-making within the organization.\nParticipated in weekly design review meetings to align project goals with data engineering best practices and continuously improve workflows.\nDelivered comprehensive documentation detailing data pipelines and processing procedures, enhancing knowledge sharing and onboarding for new team members.\n"
    }
  ],
  "skills": " **Programming Languages**\n\t Python, JavaScript, TypeScript\n\n **Backend Frameworks**\n\t NodeJS, ExpressJS, NestJS, C#, .NET, Entity Framework\n\n **Frontend Frameworks**\n\t HTML, CSS, ReactJS, NextJS, VueJS, NuxtJS, Angular, Material UI, Three.js, D3.js, React Native, Flutter, Chakra UI, TailwindCSS\n\n **API Technologies**\n\t RESTful API, REST APIs, GraphQL\n\n **Serverless and Cloud Functions**\n\t AWS, Azure\n\n **Databases**\n\t MSSQL Server, MySQL, PostgreSQL, MongoDB, DynamoDB, CosmosDB, SQL\n\n **DevOps**\n\t CI/CD pipelines, continuous integration, Kubernetes\n\n **Cloud & Infrastructure**\n\t cloud-computing (AWS)\n\n **Other**\n\t UX/UI Design, Git, GitHub, Blockchain, Solidity, Ether.js, Web3.js, Ethereum, RabbitMQ, Apache Kafka, Redis, data cleaning, web scraping, data science, machine learning, sports knowledge, Testing Tools: NUnit, xUnit, Selenium, Moq, Postman, Cypress, JMeter, Jest"
}
{
  "name": "Patryk Zaslawski",
  "role_name": "Data Engineer",
  "email": "patrykzas0428@outlook.com",
  "phone": "+48669862402",
  "address": "Gdanski, Poland",
  "linkedin": "https://www.linkedin.com/in/patryk-zaslawski-6b04a8397/",
  "profile_summary": "Results-driven Data Engineer with 8+ years of experience in software engineering, specializing in **data engineering** and **real-time monitoring**. Expert in **job cycle management** and **risk assessments**, I leverage strong technical skills to execute remote jobs efficiently and effectively. My background involves advanced analytical skills and problem-solving proficiency, ensuring precise data handling and stakeholder engagement. I am adept at managing multiple priorities while maintaining clear communication across teams. Additionally, I bring extensive knowledge of **digital oilfield technologies** and **transmission protocols**, complemented by experience in Python (FastAPI, Django, Flask, SQLAlchemy) and frontend frameworks such as Angular, Vue, React, and Next.js. Skilled in utilizing cloud platforms like **AWS** and **Azure**, and optimizing databases with **PostgreSQL**, **MySQL**, and **MongoDB**, I deliver scalable solutions that exceed business expectations.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2014",
      "to_year": "2017",
      "location": "United Kingdom",
      "university": "The University of Manchester"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Binary Studio",
      "from_date": "Aug 2023",
      "to_date": "Present",
      "location": "United Kingdom (Remote)",
      "responsibilities": "- Conducted **data engineering** tasks to design and implement scalable data solutions aligned with analytical frameworks.\n- Led real-time monitoring initiatives, ensuring compliance with industry standards through rigorous **risk assessments**.\n- Maintained effective **communication** with stakeholders to understand requirements and deliver data solutions.\n- Managed job cycle processes for data workflows, optimizing **remote job execution** and enhancing performance.\n- Utilized analytical skills to troubleshoot and resolve complex data-related issues in a timely manner, demonstrating advanced **problem-solving** capabilities.\n- Leveraged **digital oilfield technologies** to improve operational efficiency in data transmission and processing across various platforms.\n- Coordinated stakeholder engagement efforts, aligning project goals with business objectives while fostering collaboration.\n- Developed and maintained robust data pipelines in **Python**, integrating with **PostgreSQL** and **Azure SQL** for efficient data storage and querying.\n- Utilized **Apache Airflow** for orchestrating workflows, ensuring seamless transitions between data jobs and timely execution of tasks.\n- Implemented **real-time monitoring** solutions to track performance and data quality, providing stakeholders with actionable insights and reliability metrics.\n- Championed **risk assessments** focused on data security, implementing practices to safeguard sensitive information within the data engineering processes.\n- Improved data transmission protocols utilizing frameworks like **Apache Kafka** for enhanced data flow and processing speed.\n- Sensibly applied **multitasking** abilities to juggle various data projects simultaneously while adhering to strict deadlines.\n- Developed comprehensive reports and analysis using **Azure Monitor** and **Power BI**, reviewed through **real-time monitoring** dashboards for decision-making support.\n- Engaged in continuous learning to enhance **technical skills** in emerging data technologies, adapting to industry trends and innovations."
    },
    {
      "role": "Software Engineer",
      "company": "Ardigen",
      "from_date": "Mar 2020",
      "to_date": "Aug 2023",
      "location": "Poland",
      "responsibilities": "• Led data engineering initiatives by developing a **Python** (FastAPI, Django, Flask) microservices-based backend architecture, enhancing scalability, maintainability, and performance with **SQL Server**, **PostgreSQL**, and **MongoDB** for seamless data management.\n• Executed real-time monitoring solutions by designing and implementing RESTful and gRPC APIs in **Python**, ensuring secure communication between applications with real-time transaction updates.\n• Conducted risk assessments by maintaining and refactoring legacy **.NET/VB.NET** systems into **Python** microservices, ensuring compatibility with modern platforms while preserving critical business functionality.\n• Enhanced stakeholder engagement through the development of dynamic, user-centric frontends integrated with **Python** backends using **Angular**, **Vue**, **React**, and **Next.js**, applying **Redux/NgRx** for responsive user interfaces.\n• Managed remote job execution by creating a real-time transaction monitoring dashboard powered by **Python** APIs with **Angular UI**, integrated with **Power BI** for advanced reporting and analytics.\n• Streamlined job cycle management and transformed ACH transfers and payment processing by implementing system integrations with **MuleSoft** and **Python** services.\n• Ensured robust data protection in **Python** services with encryption, **RBAC**, **OAuth 2.0**, and **JWT**, maintaining compliance with financial regulations.\n• Optimized data handling by working with **SQL Server**, **DynamoDB**, and **Redis** accessed via **Python** services, improving transactional data reliability and availability by **30%**.\n• Leveraged **Kafka** and **RabbitMQ** within **Python** microservices to build a high-throughput, event-driven architecture capable of handling over **10,000** financial transactions per minute.\n• Configured and managed **Azure Service Bus** for reliable message brokering between microservices, improving modularity and decoupling of services.\n• Designed secure authentication and authorization mechanisms in **Python APIs** using **OAuth 2.0**, **OpenID Connect**, and **JWT** to enhance system security.\n• Developed fraud detection services in **Python**, integrating **Azure Machine Learning** models and behavioral data to proactively identify suspicious activities with **95%** accuracy.\n• Ensured PCI-compliant interoperability by developing secure payment gateway integrations in **Python** supporting ACH, credit card payments, and wire transfers.\n• Reduced infrastructure overhead by building serverless components in **Python** using **AWS Lambda** and **Azure Functions** for ETL pipelines, enhancing system responsiveness by **40%**.\n• Implemented the **ELK stack** (Elasticsearch, Logstash, Kibana) with **Python** services for centralized logging, monitoring, and performance tracking.\n• Managed **Kubernetes** clusters to deploy **Python** microservices, enabling rolling updates and high availability, ensuring system uptime of **99.99%**.\n• Applied best practices in **Python** development, including automated unit testing with **PyTest** and integration testing with **Selenium**, ensuring backend service reliability.\n• Designed integration solutions in **Python** with **MuleSoft** for external partners, ensuring secure and compliant financial data exchange.\n• Contributed to event-driven architectures in **Python** using **Kafka**, **RabbitMQ**, and **Azure Service Bus** to improve performance and system scalability.\n• Integrated unit tests, static analysis, and CI/CD pipelines (**Azure DevOps**, **GitHub Actions**, **TFS**) for automated quality assurance and continuous delivery."
    },
    {
      "role": "Software Engineer",
      "company": "Altum Software",
      "from_date": "Oct 2017",
      "to_date": "Feb 2020",
      "location": "United Kingdom",
      "responsibilities": "• Conducted risk assessments and problem-solving strategies to enhance data engineering processes, ensuring high-quality data delivery and integrity.\n• Executed remote job functions through reliable **digital oilfield technologies**, improving efficiency and accuracy in data handling and processing.\n• Engaged with stakeholders effectively to gather requirements and provide updates on data workflows, demonstrating strong communication and multitasking abilities.\n• Managed comprehensive job cycles to ensure effective data transmission and operational coherence, utilizing advanced **transmission protocols**.\n• Implemented real-time monitoring systems to track data flow integrity and performance metrics, leveraging tools to enhance analytics and responsiveness in data engineering.\n• Analyzed and optimized data flows within projects, identifying bottlenecks and improving overall system performance and throughput.\n• Designed and developed data pipelines using **Python** and relevant frameworks, ensuring data accuracy and availability for stakeholders.\n• Optimized data storage solutions and access protocols to enhance data retrieval times and reduce latency by **30%**.\n• Coordinated with cross-functional teams to support data initiatives, aligning engineering efforts with organizational goals and timelines, which improved project management efficiency by **20%**.\n• Leveraged analytical skills to troubleshoot and resolve data discrepancies, optimizing processes for better accuracy and reliability in data outputs."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython  \n\n **Backend Frameworks:**\n\tFastAPI, Django, Flask, Spring Boot  \n\n **Frontend Frameworks:**\n\tAngular (1–16), React (15–18), Next.js, Vue.js (2/3), Blazor  \n\n **API Technologies:**\n\tREST & gRPC APIs  \n\n **Serverless and Cloud Functions:**\n\tAWS (Lambda), Azure (App Services)  \n\n **Databases:**\n\tSQLAlchemy, Pydantic, Celery, PostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis  \n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD  \n\n **Cloud & Infrastructure:**\n\tAWS (ECS, RDS, S3), Azure (Blob, SQL), Nginx, Certbot  \n\n **Other:**\n\tdata engineering, remote job execution, risk assessments, job cycle management, real time monitoring, communication, problem-solving, analytical skills, multitasking, technical skills, digital oilfield technologies, transmission protocols, stakeholder engagement"
}
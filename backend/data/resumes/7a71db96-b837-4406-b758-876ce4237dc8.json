{
  "name": "Tomasz Lee",
  "role_name": "Senior Solutions Engineer",
  "email": "tomasz.lee8@outlook.com",
  "phone": "+48 732 145 942",
  "address": "Warsaw, Poland",
  "linkedin": "https://www.linkedin.com/in/tomasz-lee-b9a25a391/",
  "profile_summary": "Dynamic Senior Solutions Engineer with 9+ years of experience in advanced data management and analytics. Proficient in **Snowflake**, **Teradata**, **Spark**, **Databricks**, and **Hadoop** for efficient data processing and analysis. Expert in implementing **Data Mesh**, **Data Vault**, and **Data Fabric** strategies to enhance data governance and architecture. Solid background in **AWS**, **Azure**, and IaaS/PaaS systems, ensuring secure and scalable data solutions. Skilled in advanced SQL and ETL tools like **DBT**, **Talend**, and **Informatica** for data transformation. Proven track record of collaborating across teams and leading projects to successful completion while leveraging data warehousing concepts such as **Data Lake**, **Data Warehouse**, and **Medallion** architectures. Additionally, adept in **Python**, **PySpark**, and utilizing tools like **Tableau**, **PowerBI**, and **Streamlit** for data visualization and reporting, ensuring actionable insights and robust data-driven solutions.",
  "education": [
    {
      "degree": "Master of Computer Science",
      "category": "",
      "from_year": "Apr 2012",
      "to_year": "Oct 2014",
      "location": "",
      "university": "National University of Singapore"
    },
    {
      "degree": "Bachelor of Computer Science",
      "category": "",
      "from_year": "Apr 2008",
      "to_year": "Mar 2012",
      "location": "",
      "university": "National University of Singapore"
    }
  ],
  "experience": [
    {
      "role": "Senior Solutions Engineer",
      "company": "CD Projekt RED",
      "from_date": "May 2022",
      "to_date": "Oct 2025",
      "location": "Warsaw, Poland",
      "responsibilities": "- Led the architecture and implementation of data solutions leveraging **Snowflake**, **Teradata**, and **Oracle**, enhancing data accessibility for analytics with a 40% improvement in speed.\n- Designed scalable architectures using **AWS** and **Azure** cloud services to support **IaaS** and **PaaS** deployments, ensuring robust disaster recovery plans across **Data Lake** and **Data Warehouse** environments.\n- Developed data pipelines and transformation processes utilizing **Spark** and **Databricks** for real-time and batch processing, achieving a processing speed increase of 30%.\n- Implemented effective **Data Governance** and **Data Management** strategies that resulted in a significant reduction of data compliance issues by 25%.\n- Utilized **SQL Server** and **MSSQL** for data storage solutions, optimizing data retrieval and management through advanced indexing, achieving a query execution time reduction of 20%.\n- Integrated **Data Mesh** and **Data Fabric** architectures to enhance data collaboration across teams, increasing data availability and reliability.\n- Collaborated with cross-functional teams to establish and maintain **Data Vault** and **Kimball** modeling methodologies, significantly improving reporting accuracy.\n- Created thorough documentation for data processes to ensure clarity and compliance, facilitating easier onboarding for new team members.\n- Leveraged **Python** and **DBT** for data analytics and transformation tasks, enhancing the analytics capabilities of the team.\n- Applied advanced techniques in **linear regression**, **classification**, and **clustering** to derive actionable insights from complex datasets."
    },
    {
      "role": "Software Engineer",
      "company": "Sea Group",
      "from_date": "Mar 2018",
      "to_date": "Apr 2022",
      "location": "Shopee, Singapore",
      "responsibilities": "- Designed and developed a cloud-based project management tool using **Angular** and **.NET Core**, leading to a **15%** increase in team collaboration efficiency.\n- Created a microservices architecture with **RabbitMQ** for task orchestration and **Apache Kafka** for handling event-driven data streams, enhancing system scalability.\n- Integrated **Redis** as a caching layer to reduce latency in frequent API calls, resulting in a **25%** boost in system performance.\n- Developed **RESTful APIs** with **.NET Core** and integrated with a **PostgreSQL** database for real-time data access.\n- Built and maintained **ReactJS** and **Tailwind CSS** components, resulting in a **25%** improvement in user engagement on the platform.\n- Led the effort to implement CI/CD pipelines using **GitHub Actions**, reducing production deployment errors by **30%**.\n- Managed **MSSQL** and **MySQL** databases for data storage and retrieval, optimizing queries for better performance.\n- Collaborated closely with stakeholders to define system requirements and development timelines, ensuring timely project delivery.\n- Created custom charts and graphs using **D3.js** for interactive project status tracking and reporting.\n- Conducted code reviews and mentorship sessions with junior developers, improving team productivity."
    },
    {
      "role": "Software Developer",
      "company": "Grab Holdings Inc",
      "from_date": "Jan 2015",
      "to_date": "Feb 2018",
      "location": "Singapore",
      "responsibilities": "Utilized **Snowflake**, **Databricks**, and **Hadoop** to design and implement scalable data architectures, enhancing data accessibility by **30%**.\nDeveloped and optimized ETL processes using **Talend** and **DBT**, improving data transformation efficiency, significantly reducing processing time by **25%**.\nLeveraged **AWS** and **Azure** cloud services for deploying data solutions, resulted in cutting operational costs by **15%** through effective resource management.\nImplemented **Data Governance** practices to ensure data quality and compliance across systems, thus maintaining data integrity.\nCollaborated in the integration of **Data Lakes** and **Data Warehouses** using **Oracle** and **SQL Server**, facilitating advanced analytics capabilities.\nEngaged in data modeling techniques such as **Data Vault** and **Kimball**, providing robust data structures that support business intelligence efforts.\nImproved reporting capabilities utilizing **Tableau** and **PowerBI**, increasing reporting accuracy and access to insights for end-users.\nDesigned and executed batch and stream processing workflows using **Apache Spark** and **PySpark**, optimizing data handling across various platforms.\nParticipated in disaster recovery planning and implementation, ensuring critical data is backed up and recoverable.\nProduced comprehensive technical documentation and user manuals for developed data solutions, enhancing user understanding and technical support efficiency."
    }
  ],
  "skills": " **Programming Languages:**\n\tJavaScript, TypeScript, C#, Python, Solidity\n\n**Backend Frameworks:**\n\tNodeJS, ExpressJS, NestJS, .NET, Entity Framework\n\n**Frontend Frameworks:**\n\tHTML, CSS, ReactJS, NextJS, VueJS, NuxtJS, Angular, Material UI, Three.js, D3.js, React Native, Flutter, Chakra UI, TailwindCSS\n\n**API Technologies:**\n\tRESTful API, GraphQL\n\n**Serverless and Cloud Functions:**\n\tAWS, Azure, Snowflake, Teradata, Spark, Databricks, Hadoop, Oracle\n\n**Databases:**\n\tMSSQL Server, MySQL, PostgreSQL, MongoDB, DynamoDB, CosmosDB, SQL, Data Lake, Data Warehouse, Data Vault, Data Fabric, Data Mesh\n\n**DevOps:**\n\tCI/CD pipelines, DevOps\n\n**Cloud & Infrastructure:**\n\tIaaS, PaaS, Networking, Security, Encryption, Identity, Access Management, Disaster Recovery\n\n**Other:**\n\tMicorservices, Messaging & Caching: Apache Kafka, RabbitMQ, Redis, Testing Tools: NUnit, xUnit, Selenium, Moq, Postman, Cypress, JMeter, Jest, Git, GitHub, DBT, Talend, Informatica, Snowpark, PySpark, DataFrames, Parquet, Avro, Apache Iceberg, Delta Lake, Orchestration, Tableau, PowerBI, MicroStrategy, Thoughtspot, SAS, Streamlit, Advanced SQL, Linear Regression, Variance Analysis, Modeling, Forecasting, Classification, Regression, Clustering, Dimensionality Reduction, Natural Language Processing, Language Models",
  "apply_company": "Snowflake"
}
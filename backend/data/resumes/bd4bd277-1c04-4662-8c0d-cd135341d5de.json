{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Analytics Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "Results-driven Senior Analytics Engineer with 8 years of experience specializing in data-driven solutions that enhance decision-making across the healthcare and financial sectors. Proficient in **SQL**, **Python**, and **Airflow**, with extensive expertise in developing efficient data pipelines and analytics dashboards using **dbt**, **Clickhouse**, and **Metabase**. I excel in transforming raw data into actionable insights, ensuring seamless integration with **Data Warehouses** and delivering high-quality reports and dashboards for stakeholders.\n\nMy strong analytical skills and problem-solving capabilities are complemented by effective communication, enabling me to convey complex data findings clearly to technical and non-technical audiences. With a solid foundation in data strategy and advanced analytics, I am adept at addressing challenges and optimizing processes to contribute to overall business goals.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Analytics Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Developed and maintained **SQL**-based data pipelines and **data warehouse** solutions to facilitate reporting and analytics for healthcare and financial platforms, optimizing performance and ensuring accurate data accessibility.\n- Engineered complex dashboards and reports using **Metabase** and **analytics** tools to deliver real-time insights, improving decision-making processes and operational efficiency by **30%**.\n- Collaborated with cross-functional teams to identify business requirements and translate them into technical specifications to enhance analytical capabilities, leveraging strong **communication** skills.\n- Implemented **Python**-based solutions for ETL processes, utilizing **Airflow** for orchestration, ensuring reliable data delivery and transformation.\n- Optimized data storage and querying strategies utilizing **Clickhouse**, resulting in a **50%** improvement in query performance for large datasets.\n- Developed reproducible workflows for data processing and reporting, focusing on problem-solving and analytics to address data quality issues.\n- Designed and launched interactive dashboards, enhancing user engagement and providing visibility into key metrics for stakeholders, driving insights into transaction trends.\n- Collaborated with data scientists to integrate machine learning models into analytics frameworks, enabling advanced predictive capabilities for business use cases.\n- Established robust testing strategies using **pytest** and other testing frameworks to ensure the integrity and reliability of data pipelines and reports.\n- Engaged in continuous improvement practices, utilizing feedback loops to refine analytical tools and dashboards, fostering a data-driven culture within the organization."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "- Developed and maintained **Data Pipeline** solutions using **Python**, **Apache Airflow**, and **dbt**, enabling efficient and seamless ingestion and transformation of data for analytics.\n- Designed and implemented **dashboards** and reporting tools with **Metabase**, enhancing data visibility and communication of insights to stakeholders through interactive reports.\n- Utilized **SQL** on **Clickhouse** to optimize and execute complex queries, improving performance metrics by **40%** in data retrieval for analytics purposes.\n- Created and maintained a **Data Warehouse** that supports extensive reporting and analytics needs, structuring data for efficient access and long-term storage.\n- Collaborated with cross-functional teams to gather requirements and deliver insightful **analytics** that drive decision-making, enhancing problem-solving capabilities across departments.\n- Enhancing overall system performance through effective communication of technical insights and findings to non-technical teams, fostering a culture of data-driven decision making.\n- Developed real-time **analytics** solutions for transaction monitoring, aligning with business objectives to reduce fraud and improve operational efficiency."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Utilized **SQL** for complex queries and data manipulation, enabling effective data analysis and reporting for executive-level insights through dashboards and reports.\nEmployed **Python** in conjunction with **Airflow** to orchestrate and automate data pipelines, ensuring accurate and timely data flows from various sources into the **Data Warehouse**.\nDesigned and implemented ETL processes using **dbt** to enhance data transformation, improving data quality and accessibility for analytics.\nDeveloped and maintained detailed dashboards using **Metabase**, providing actionable insights and visualizing key business metrics, leading to a **25%** improvement in reporting efficiency.\nInterpreted and analyzed large datasets stored in **Clickhouse**, streamlining access to critical metrics and supporting data-driven decision-making.\nApplied strong problem-solving skills to troubleshoot and optimize data workflows, reducing latency in data retrieval by **30%** through the implementation of efficient query strategies.\nCommunicated complex technical concepts effectively to stakeholders, bridging the gap between technical execution and business objectives to drive project success.\nConducted regular data quality checks and audits to identify and rectify discrepancies, ensuring integrity in analytics outputs and maintaining compliance with data governance standards.\nFacilitated collaboration across cross-functional teams to elevate data literacy, establish data best practices, and foster an analytics-driven culture within the organization."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, SQL, Clickhouse\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular)\n\n**API Technologies:**\n\tNginx, Keycloak (OIDC, RBAC), JWT, OAuth2\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tCI/CD & Infrastructure as Code\n\n**Other:**\n\tArtificial Intelligence & Machine Learning: MLflow, Airflow, Kubeflow, dbt, Data Warehouse, Data Pipeline, Dashboards, Reports, Analytics, Problem Solving, Communication",
  "apply_company": "Robin Cook"
}
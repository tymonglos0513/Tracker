{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-623a26390/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in delivering high-performance data solutions across the healthcare and financial sectors. Highly skilled in **Python**, **SQL**, **ETL**, **ELT**, and **data warehousing**, with hands-on expertise in data modeling, data wrangling, and deploying cloud-native systems on **GCP** using **Dataproc** and **Dataflow**. Proven track record of implementing Infrastructure-as-Code and leveraging **Hadoop** and **Pyspark** for data processing and analytics.\nExperienced in collaborating with cross-functional teams and stakeholder management, utilizing strong communication, critical thinking, and problem-solving abilities to align data solutions with business goals. Adept in coaching team members and fostering a culture of collaboration to enhance overall project delivery. Proficient in compliance-driven development, ensuring alignment with industry standards.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "\n- Developed and maintained robust **RDBMS** solutions using **PostgreSQL** and **SQL**, ensuring data integrity and accessibility for analytics across healthcare and fintech domains.\n- Designed ETL/ELT processes to streamline data ingestion and transformation using **Python** and **Pyspark** with a focus on data warehousing principles for efficient data modeling and optimization.\n- Leveraged **Hadoop** ecosystems and **GCP** services like **Dataproc** and **Dataflow** to build scalable data processing pipelines that reduced processing time by up to **30%**.\n- Collaborated with cross-functional teams to analyze and understand business requirements, effectively translating them into technical specifications to enhance project outcomes.\n- Implemented Infrastructure-as-Code (IaC) practices using tools such as **Terraform** and **AWS CloudFormation** for seamless environment setups and version control, enhancing deployment efficiency by **40%**.\n- Developed coaching strategies to improve team skill sets and foster a culture of continuous learning, resulting in enhanced collaboration and productivity.\n- Engaged in stakeholder management and critical thinking exercises to identify pain points and devise innovative solutions in data wrangling and modeling.\n- Conducted comprehensive testing strategies for data pipelines, ensuring robust validation and quality control through **Python** and **SQL** queries.\n- Actively communicated project progress and challenges to stakeholders, maintaining transparency and alignment with business objectives, contributing to a **25%** increase in stakeholder satisfaction.\n- Played a pivotal role in troubleshooting data-related issues, applying problem-solving skills to optimize ETL workflows and enhance system performance."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized **Python** for ETL and ELT processes, ensuring efficient data ingestion and transformation from RDBMS and third-party sources, achieving a processing improvement of **30%**.\nImplemented data warehousing strategies using **SQL** and **Hadoop** to optimize storage and retrieval of financial datasets, managing a total data volume exceeding **5 TB**.\nDesigned data models and performed data wrangling to ensure data accuracy and integrity, reducing reporting errors by **25%** through effective data governance practices.\nCollaborated with cross-functional teams to implement an **Infrastructure-as-Code** approach, enhancing deployment processes for data pipelines across **GCP** platforms like **Dataproc** and **Dataflow**.\nDeveloped scalable ETL pipelines for real-time data processing, integrating tools such as **Apache Airflow** to automate workflows and tasks efficiently.\nCoached junior data engineers on best practices for data engineering and project methodologies, promoting effective communication and collaboration in teams.\nEngaged in stakeholder management, actively liaising with operations to ensure alignment of data solutions with business objectives, enhancing overall project delivery satisfaction by **40%**."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **SQL** to design and optimize data warehouses for high-performance applications, ensuring efficient data extraction, transformation, and loading (ETL) processes to support analytical needs with **GCP** tools like **Dataproc** and **Dataflow**.\nEngineered robust data models and performed data wrangling to guarantee data integrity and accuracy, aligning with project requirements and establishing strong foundations for insightful reporting.\nCollaborated with cross-functional teams to support data-driven decision-making, applying excellent communication and stakeholder management skills to ensure project alignment.\nImplemented **Hadoop** technologies to manage large datasets effectively, ensuring data availability, fault tolerance, and scalability across various applications, leveraging modern infrastructure-as-code practices for deployment.\nCoached junior staff on best practices in data architecture as well as effective collaboration techniques, fostering a knowledge-sharing environment.\nExecuted comprehensive data analysis and visualization to guide strategic business decisions, utilizing strong critical thinking and problem-solving skills to derive actionable insights from complex datasets.\nManaged ERP integrations with **Netsuite** and **SAP**, ensuring seamless data flow and interfacing between systems, thereby improving overall operational efficiency.\nDeveloped and maintained data pipelines, optimizing performance and reliability for real-time processing needs, leveraging both **Python** (**Pyspark**) and **Java** where necessary.\nORM and OLAP tools applied to facilitate advanced analytics, further supporting organizational goals through data availability.\nPerformed rigorous testing and validation of data solutions to ensure compliance with industry standards and to meet client expectations."
    }
  ],
  "skills": " **Programming Languages:**\n\t•\tPython\n\t•\tJava\n\t•\tJavaScript/TypeScript\n\n **Backend Frameworks:**\n\t•\tFastAPI\n\t•\tFlask\n\t•\tDjango\n\n **Frontend Frameworks:**\n\t•\tReact\n\t•\tVue\n\t•\tAngular\n\n **API Technologies:**\n\t•\tKeycloak (OIDC, RBAC)\n\t•\tOAuth2\n\t•\tJWT\n\n **Serverless and Cloud Functions:**\n\t•\tAWS: Lambda\n\t•\tAzure: App Services\n\n **Databases:**\n\t•\tPostgreSQL (Fintech)\n\t•\tMySQL (Healthcare)\n\t•\tMongoDB (Gaming)\n\t•\tRedis\n\t•\tRDBMS\n\t•\tSQL\n\n **DevOps:**\n\t•\tDocker\n\t•\tKubernetes\n\t•\tGitHub Actions\n\t•\tGitLab CI/CD\n\n **Cloud & Infrastructure:**\n\t•\tAWS: ECS, RDS, S3\n\t•\tAzure: Blob Storage, SQL Database\n\t•\tGCP\n\t•\tHadoop\n\t•\tDataproc\n\t•\tDataflow\n\n **Other:**\n\t•\tMLflow\n\t•\tAirflow\n\t•\tKubeflow\n\t•\tTerraform\n\t•\tAnsible\n\t•\tHelm\n\t•\tDocker Compose\n\t•\tCoaching\n\t•\tCollaboration\n\t•\tCritical Thinking\n\t•\tCommunication\n\t•\tProblem-Solving\n\t•\tStakeholder Management\n\t•\tETL\n\t•\tELT\n\t•\tData Warehousing\n\t•\tData Wrangling\n\t•\tData Modelling\n\t•\tERP\n\t•\tNetsuite\n\t•\tSAP\n\t•\tInfrastructure-as-Code\n"
}
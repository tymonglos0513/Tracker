{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Solutions Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "As a highly skilled Senior Solutions Engineer with over 8 years of experience, I possess a robust technical background in **SQL**, **Snowflake**, **Teradata**, **Spark**, **Databricks**, **Hadoop**, **Oracle**, **SQL Server**, and **AWS/Azure** cloud environments. My expertise includes building and managing **Data Lakes** and **Data Warehouses** while implementing **Data Governance**, **Data Management**, and **Enterprise Architecture** strategies, including **Data Mesh**, **Data Vault**, and **Medallion** architecture. I also have hands-on experience with **DBT**, **Talend**, and **Informatica** for data integration and transformation tasks.\n\nI focus on designing scalable and secure solutions utilizing **IaaS** and **PaaS** models, emphasizing **Networking**, **Security**, and **Identity and Access Management** best practices. I effectively employ **Python**, **PySpark**, and **Snowpark** for data analysis and orchestration as well as tools like **Tableau**, **PowerBI**, and **MicroStrategy** for data visualization.\n\nIn addition to my technical skills, I have a strong foundation in project leadership, driving initiatives that leverage **AI/ML** capabilities for advanced analytics and automation. I foster compliance with industry standards, including HIPAA and PCI DSS, while addressing complex business needs through innovative solutions.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Solutions Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Designed and implemented **AWS**- and **Azure**-based data solutions leveraging methodologies such as **Data Mesh**, **Data Vault**, **Medallion**, and **Kimball** for scalable data storage and governance.\nArchitected and maintained data lakes and warehouses utilizing technologies like **Snowflake**, **Teradata**, **Hadoop**, and **SQL Server**, ensuring compliance with industry standards and best practices.\nDeveloped ETL pipelines using **Talend**, **Informatica**, and **DBT** to enable seamless data integration across various platforms while handling large-scale data sets with **Apache Spark** and **Databricks**.\nIntegrated **Python** and **PySpark** for advanced data processing, performing complex data transformations and utilizing **DataFrames** efficiently.\nUtilized advanced analytical tools such as **Tableau**, **PowerBI**, and **Thoughtspot** to generate insights from historical and real-time data, resulting in a 30% improvement in reporting efficiency.\nEnsured data security and compliance through effective **Identity and Access Management**, **Encryption** techniques, and established **Disaster Recovery** processes across all deployed solutions.\nCollaborated with cross-functional teams to develop predictive analytics models using techniques such as **classification**, **regression**, and **clustering**, driving a significant enhancement in decision-making processes.\nImplemented monitoring and orchestration frameworks to ensure data integrity and availability, while optimizing the data supply chain within cloud environments.\nEstablished data governance frameworks that promoted best practices in **Data Management** and compliance across the organization, resulting in a 25% reduction in data discrepancies.\nDeveloped cooperative strategies for **Natural Language Processing** to automate document parsing and insights extraction, enhancing operational efficiency."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Utilized **Databricks** and **Spark** for building scalable data pipelines that efficiently processed and transformed large datasets, improving data accessibility and analytical performance.\nArchitected and developed integration solutions leveraging **AWS** and **Azure** services to support data governance and security, adhering to best practices for **Identity and Access Management** and **Encryption**.\nImplemented data warehousing strategies using **Snowflake** and **Teradata**, establishing a robust **Data Lake** and integrating data across multiple sources to facilitate accurate reporting and analytics.\nEmployed **Python** and **SQL** scripting for ETL processes and data retrieval, enhancing reporting capabilities with reliable and timely data.\nDesigned and maintained the **Data Mesh** infrastructure to support decentralized data ownership across teams, promoting effective data management and governance.\nCreated visualizations and analytics solutions using **Power BI** and **Tableau**, providing business stakeholders with insightful dashboards that led to a 25% increase in data-driven decision-making efficiency.\nApplied **Python** and **MLflow** to develop machine learning pipelines for predictive analytics, achieving accurate forecasts for key financial metrics in real-time.\nOrchestrated data transformation and loading processes with **Apache Airflow**, ensuring seamless workflow management and adherence to schedules in data operations.\nExecuted advanced analytical models utilizing classification, regression, and clustering techniques on large datasets, resulting in enhanced risk assessment and fraud detection capabilities."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Designed and implemented data management solutions utilizing **Snowflake**, **Teradata**, and **SQL Server**, ensuring effective governance and performance in enterprise data warehousing across all workflows.\nOptimized data processing with **Apache Spark** and **Databricks** to facilitate real-time analytics and machine learning workflows, impacting processing speed by over **30%**.\nDeveloped data lakes and data warehouses using **AWS** and **Azure** services, applying best practices in **Data Lake** and **Data Mesh** architectures for seamless integration and accessibility.\nEstablished effective data governance frameworks leveraging **Data Vault** and **Data Fabric** concepts, enhancing data reliability and facilitating compliance with GDPR.\nApplied comprehensive security measures for data access and encryption in line with **Identity and Access Management** policies, ensuring user data protection and confidentiality.\nImplemented disaster recovery solutions and backup strategies using **Azure** and **AWS**, reducing downtime risk to less than **1 hour** per incident.\nUtilized **Python** for scripting and automation of data processing tasks, deploying **PySpark** for high-volume data transformations and utilizing **DBT** for data modeling and transformations.\nCreated visualizations and dashboards in **Tableau**, **PowerBI**, and **MicroStrategy**, driving insights that led to a **15%** increase in operational efficiency for decision-making processes.\nEnhanced data orchestration and workflow management by implementing **Apache Airflow** and **Docker**, effectively managing complex ETL processes across various environments.\nConducted advanced SQL queries for data extraction and manipulation, applying **3NF** and **Kimball** methodologies to optimize database structures for reporting and analytics."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, JavaScript/TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tReact, Vue, Angular\n\n **API Technologies:**\n\tJWT, OAuth2, Keycloak (OIDC, RBAC)\n\n **Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n **Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, Snowflake, Teradata, Oracle, SQL Server, DataFrames, Parquet, Avro, Apache Iceberg, Delta Lake\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n **Cloud & Infrastructure:**\n\tIaaS, PaaS, Networking, Security, Disaster Recovery\n\n **Other:**\n\tData Management, Data Governance, Data Lake, Data Warehouse, Data Mesh, Data Vault, Medallion, Kimball, 3NF, DBT, Talend, Informatica, Snowpark, PySpark, Streamlit, Tableau, PowerBI, MicroStrategy, Thoughtspot, SAS, time series analysis, Artificial Intelligence & Machine Learning (MLflow, Airflow, Kubeflow), classification, regression, clustering, dimensionality reduction, Natural Language Processing, Language Models",
  "apply_company": "Snowflake"
}
{
  "name": "Rei Taro",
  "role_name": "Data Engineer",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-28639a395/",
  "profile_summary": "Results-driven Data Engineer with over 10 years of experience in designing and delivering robust backend systems tailored for financial platforms and large-scale data projects. Proficient in leveraging **AWS** and **Docker** for cloud solutions and containerization, alongside **SQL** for data management and modeling. Extensive expertise in **ETL** processes, **Data Warehousing**, and transforming big data using **Apache Hadoop**, **Databricks**, **Snowflake**, and **PySpark**. Skilled in utilizing **Python** libraries such as **Pandas** and **NumPy** for data manipulation and analysis. Adept at designing high-performing APIs and automation tools, with a proven track record at renowned organizations including VISA, Sii Poland, and Reply Polska. Excellent communication skills enhance collaboration with cross-functional teams.",
  "education": [
    {
      "degree": "Bachelorâ€™s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "Utilized **AWS** to design and implement scalable data pipelines and ensure efficient ETL processes for large datasets.\nManaged data modeling and warehousing projects with **SQL** and **Snowflake**, improving data accessibility by **30%** for analytics teams.\nEngineered ETL processes using **Databricks** and **Apache Hadoop** to facilitate seamless data transformation and enhance data integration across multiple sources.\nDeveloped and optimized data transformation workflows with **Pandas** and **NumPy**, achieving a **20%** increase in processing speed.\nImplemented containerization strategies using **Docker** and orchestrated deployments with **Kubernetes**, leading to better resource management.\nLeveraged **PySpark** for big data processing, enabling analysis of datasets exceeding **1TB** in size.\nEnsured effective communication with cross-functional teams to align on project requirements and deliver timely data solutions."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "Utilized **Python** and **SQL** to develop data engineering solutions, enhancing data transformation and modeling processes.\nDesigned and implemented data pipelines for efficient ETL (Extract, Transform, Load) operations, employing **Apache Airflow** and **Databricks** for data orchestration and management.\nCollaborated with cross-functional teams to design and optimize data warehousing solutions, ensuring data integrity and compliance with regulatory standards.\nLeveraged **Snowflake** and **Hadoop** for big data storage and analytics, processing over **10 TB** of data monthly to derive actionable insights.\nEngineered scalable containerized applications using **Docker** and **Kubernetes** for seamless deployment and management of data services.\nEmployed **Pandas** and **NumPy** for data manipulation and analysis, increasing data processing efficiency by **30%**.\nConducted performance tuning and optimization of SQL queries, improving execution times by as much as **50%**.\nExecuted data transformation processes utilizing **PySpark**, handling large datasets to support business intelligence initiatives and reporting requirements."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "Utilized **AWS** for deploying and managing scalable data solutions, enhancing operational efficiency by **20%**.\nEngineered ETL processes using **Apache Hadoop**, **Databricks**, and **Snowflake** to optimize data modeling and transformation across **3** distinct data sources.\nDeveloped data warehousing solutions and implemented **SQL** queries to structure and aggregate big data for analytics, improving data retrieval times by **15%**.\nCollaborated with cross-functional teams to ensure data integrity and seamless communication throughout the data pipeline, fostering clear and effective project management.\nImplemented robust testing strategies with **PySpark**, **Pandas**, and **NumPy** for data processing modules ensuring accuracy and reliability of data transformation processes.\nAutomated data workflows and orchestrated containerization with **Docker** and **Kubernetes** for efficient deployment of data applications, reducing operational overhead by **30%**.\nDesigned scalable data architectures that align with best practices for data engineering, ensuring compliance with industry standards.\n"
    }
  ],
  "skills": "**Programming Languages**\n\tPython, SQL\n\n**Backend Frameworks**\n\tFastAPI, Flask, Django, Celery\n\n**API Technologies**\n\tREST/gRPC APIs\n\n**Serverless and Cloud Functions**\n\tAWS, Azure\n\n**Databases**\n\tPostgreSQL, MySQL, MongoDB, Redis, Snowflake\n\n**DevOps**\n\tDocker, Kubernetes, GitHub Actions, Azure DevOps, CI/CD\n\n**Cloud & Infrastructure**\n\tAWS (EC2, S3, Lambda)\n\n**Other**\n\tData Engineering, Data Modeling, Big Data, Databricks, ETL, Apache Hadoop, Data Warehousing, Data Transformation, PySpark, Pandas, NumPy, scikit-learn, TensorFlow, Airflow, MLflow, Microservices, Kafka, PyTest, Git, Communication"
}
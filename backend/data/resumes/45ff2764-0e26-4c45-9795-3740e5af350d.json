{
  "name": "Patryk Zaslawski",
  "role_name": "Senior Platform Engineer",
  "email": "patrykzas0428@outlook.com",
  "phone": "+48669862402",
  "address": "Gdanski, Poland",
  "linkedin": "https://www.linkedin.com/in/patryk-zaslawski-6b04a8397/",
  "profile_summary": "As a Senior Platform Engineer with 8+ years of experience, I excel in GPU-based infrastructure and have a strong proficiency in **CUDA**, **cuDNN**, and the **NVIDIA GPU ecosystem**. I have hands-on experience with **Slurm**, **Red Hat OpenShift**, and **Triton Inference Server** to deliver efficient GenAI workloads and LLM fine-tuning solutions. I am adept at leveraging **RAPIDS** and **TensorRT** for accelerated data processing in **cloud GPU environments** such as **AWS**, **Azure**, **GCP**, and **OCI**. Additionally, I am experienced in **Kubernetes**, **Terraform**, and **Ansible** for Infrastructure-as-Code (IaC) implementations, ensuring streamlined deployments and performance tuning of Linux systems. My expertise in client-facing technical solutioning, along with my strong collaboration and communication skills, enhances my ability to deliver robust infrastructure solutions tailored to business needs.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2014",
      "to_year": "2017",
      "location": "United Kingdom",
      "university": "The University of Manchester"
    }
  ],
  "experience": [
    {
      "role": "Senior Platform Engineer",
      "company": "Binary Studio",
      "from_date": "Aug 2023",
      "to_date": "Present",
      "location": "United Kingdom (Remote)",
      "responsibilities": "Designed and developed **GPU-based infrastructure** for high-performance workloads, optimizing **Linux systems performance tuning** for enhanced efficiency in cloud environments.\nImplemented **Infrastructure-as-Code (IaC)** using **Terraform** and **Ansible** to provision and manage GPU resources in **Kubernetes** clusters, reducing deployment times by **30%**.\nLed the deployment of **Red Hat OpenShift** for seamless orchestration of **NVIDIA GPU ecosystem** workloads, ensuring high availability and scalability for applications.\nArchitected and fine-tuned models leveraging **CUDA**, **cuDNN**, and **NCCL** to ensure effective resource utilization across **cloud GPU environments** on **GCP**, **AWS**, and **Azure**.\nManaged **MLOps frameworks** for efficient development and deployment of machine learning models, including workflows for **LLM fine-tuning** and design of **RAG pipelines**.\nImplemented and maintained the **Triton Inference Server** for scalable inference solutions, achieving **50%** reduction in response times under load.\nOptimized workflows for **GenAI workloads** using **RAPIDS** and **TensorRT**, significantly improving data processing times by **40%**.\nDesigned solutions utilizing **vector databases** for efficient data retrieval and storage while collaborating with cross-functional teams to deliver client-facing technical solutions tailored to project requirements.\nMentored junior engineers in best practices for **GPU-accelerated container workflows** and **client-facing technical solutioning**, enhancing team collaboration and communication skills.\nContributed to the architectural vision for distributed systems, implementing best practices for scalability and performance leveraging **NVIDIA NIMs** and **DGX systems**. \nProvided detailed documentation and technical guidance to stakeholders, ensuring effective communication of project milestones and resolutions."
    },
    {
      "role": "Software Engineer",
      "company": "Ardigen",
      "from_date": "Mar 2020",
      "to_date": "Aug 2023",
      "location": "Poland",
      "responsibilities": "Designed and implemented **NVIDIA GPU-based infrastructure** for high-performance computing environments, utilizing **Red Hat OpenShift** to manage containerized applications.\nLed the development and optimization of **CUDA**, **cuDNN**, and **TensorRT** workflows for enhanced performance in GenAI workloads, achieving a **30% improvement** in model inference times.\nDeveloped and maintained **Infrastructure-as-Code (IaC)** solutions using **Terraform** and **Ansible**, streamlining deployment processes for cloud GPU environments across **GCP**, **AWS**, and **Azure**.\nManaged **Kubernetes** clusters to orchestrate **GPU-accelerated container workflows**, ensuring seamless scaling of workloads and improving uptime by **40%**.\nImplemented performance tuning strategies for **Linux systems**, focusing on maximizing efficiency for **NVIDIA GPU ecosystems**, completing optimizations that led to a **25% reduction** in resource consumption.\nCollaborated cross-functionally to deliver **LLM fine-tuning** solutions through **MLOps frameworks**, enhancing model performance and reliability for client-facing applications.\nDesigned and deployed **RAG pipelines** to facilitate real-time data processing and retrieval for AI applications, reducing latency by **15%**.\nIntegrated and optimized **NVIDIA NIMs** and **DGX systems** into existing platforms, increasing computational power and resource availability for complex analysis tasks.\nServed as the primary liaison for client-facing technical solutioning, leveraging strong **communication skills** and expertise to meet client needs.\nUtilized **vector databases** to enhance data retrieval times and support high-performance analytics for various applications.\nEnsured best practices in collaboration and workflow efficiency, contributing to a culture of continuous improvement within the engineering team."
    },
    {
      "role": "Software Engineer",
      "company": "Altum Software",
      "from_date": "Oct 2017",
      "to_date": "Feb 2020",
      "location": "United Kingdom",
      "responsibilities": "Designed and optimized **GPU-based infrastructure** for high-performance applications utilizing **NVIDIA GPU ecosystem**, ensuring efficient deployment and scaling of **GenAI workloads**.\nImplemented and managed container orchestration using **Kubernetes** and **Red Hat OpenShift**, streamlining deployment processes and enhancing operational efficiency for cloud-based GPU environments.\nDeveloped robust MLOps solutions for managing and deploying machine learning models, ensuring seamless integration and **LLM fine-tuning** capabilities across multiple platforms.\nConfigured and utilized **Terraform** and **Ansible** for **Infrastructure-as-Code (IaC)**, automating resource provisioning and configuration in cloud environments like **GCP**, **AWS**, **Azure**, and **OCI**.\nLeveraged **Slurm** for workload scheduling and cluster management, optimizing resource utilization for GPU tasks and improving performance metrics.\nConducted performance tuning of **Linux systems** specifically for GPU workloads, enhancing overall system throughput and efficiency for applications using **CUDA**, **cuDNN**, and **NCCL**.\nIntegrated **Triton Inference Server** into pipeline architectures, providing high-performance inference for deep learning models and optimizing model serving processes.\nUtilized **RAPIDS** for accelerating data science workflows, leveraging GPU capabilities to improve processing times by **75%** in data manipulation tasks.\nDesigned scalable **vector databases** to support advanced querying and analysis, enhancing data retrieval speeds by **50%**.\nCollaborated with cross-functional teams for **client-facing technical solutioning**, identifying and addressing diverse needs through effective communication and teamwork.\nAutomated CI/CD pipelines for model deployment and monitoring, ensuring iterative performance assessments and reducing deployment times by **30%**.\nManaged **GPU-accelerated container workflows** for deploying machine learning applications in a scalable manner, improving resource allocation and operational flexibility.\nProvided documentation and training for team members on best practices for utilizing the **NVIDIA NIMs** and **DGX systems**, promoting efficient use of junior staff and increasing overall productivity."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython (FastAPI, Django, Flask)\n\n**Backend Frameworks:**\n\tSpring Boot\n\n**Frontend Frameworks:**\n\tAngular (1–16), React (15–18), Next.js, Vue.js (2/3), Blazor\n\n**API Technologies:**\n\tREST & gRPC APIs\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, vector databases\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Infrastructure-as-Code (IaC), Terraform, Ansible\n\n**Cloud & Infrastructure:**\n\tAWS, Azure, OCI, GCP, cloud GPU environments, Red Hat OpenShift\n\n**Other:**\n\tKeycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot, Linux systems performance tuning, collaboration skills, communication skills, GPU-based infrastructure, NVIDIA GPU ecosystem, CUDA, cuDNN, NCCL, Triton Inference Server, RAPIDS, TensorRT, GenAI workloads, LLM fine-tuning, RAG pipelines, MLOps frameworks, NVIDIA NIMs, DGX systems, GPU-accelerated container workflows, client-facing technical solutioning"
}
{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Solutions Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Solutions Engineer with 13+ years of experience specializing in **data management**, **data governance**, and **enterprise architecture**. Proficient in **Snowflake**, **Teradata**, **Spark**, **Databricks**, **Hadoop**, and **Oracle**, with a strong background in designing and implementing **data lakes**, **data warehouses**, and leveraging **Medallion architecture** and **Kimball** methodologies. Skilled in advanced **SQL** querying and data processing with **DBT**, **Talend**, and **Informatica**.\nExpert in deploying cloud-native solutions on **AWS** and **Azure**, utilizing **IaaS** and **PaaS** frameworks, while ensuring compliance with top-notch security protocols including **encryption** and **identity management**. Experienced in batch and stream processing, including orchestration techniques using **Airflow** and **MLflow**.\nAdept in developing AI/ML models with **Python**, emphasizing **natural language processing** and time series analysis, alongside a solid foundation in advanced analytics and visualization tools like **Tableau**, **PowerBI**, and **MicroStrategy**. Committed to delivering high-performance solutions that support strategic decision-making and operational efficiency.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Solutions Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "• Developed and implemented data management solutions leveraging **Snowflake**, **Teradata**, and **SQL Server**, ensuring compliance with enterprise architecture standards and data governance best practices.\n• Designed and optimized data workflows utilizing **Databricks**, **Spark**, and **Hadoop** for batch and stream processing, following **Data Vault**, **Data Mesh**, and **Medallion architecture** principles.\n• Led the transformation of legacy databases to modern architectures using **Data Lake** and **Data Warehouse** strategies, implementing **Delta Lake** and **Apache Iceberg** for robust data storage.\n• Engineered and maintained ETL processes using **DBT**, **Talend**, and **Informatica** to enhance data quality and accessibility for analytics across the organization.\n• Developed scalable data pipelines and microservices using **Python** and **PySpark**, ensuring efficient data retrieval and manipulation through **DataFrames** and storage optimization with **Parquet** and **Avro** formats.\n• Collaborated on cloud services integration such as **AWS** and **Azure** for IaaS and PaaS solutions, focusing on networking, security, and disaster recovery implementations.\n• Created dynamic data visualization dashboards with **Tableau** and **PowerBI**, facilitating insightful analysis on performance metrics and operational KPIs.\n• Implemented advanced analytical methods including **time series analysis**, **regression**, and **forecasting** to drive data-driven decision making in the organization.\n• Conducted natural language processing (NLP) projects using **spaCy** and **Hugging Face Transformers**, developing innovative solutions for document parsing and anomaly detection.\n• Ensured robust model deployment and management via MLOps practices with **MLflow** and **Airflow**, covering model training, validation, and disaster recovery measures within the development lifecycle.\n• Defined and enforced best practices for data security, encryption, and identity management to safeguard sensitive information across the data ecosystem.\n• Led the standardization of metadata management processes that support enterprise-level data governance and compliance with regulatory frameworks."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized **Snowflake**, **Teradata**, and **Spark** to design and implement scalable data warehouse solutions, facilitating efficient data management and governance across the organization.\nLed the deployment of **AWS** and **Azure** based infrastructure for **IaaS** and **PaaS** solutions, achieving a **35% increase** in system performance and a **20% reduction** in operational costs.\nArchitected data pipelines using **DBT**, **Talend**, and **Informatica**, integrating batch and stream processing for real-time data insights from **Apache Kafka** and **RabbitMQ** that improved data availability by **50%**.\nImplemented **Data Mesh** and **Data Fabric** strategies to promote data accessibility and quality, adhering to **Data Governance** standards and ensuring compliance with best practices.\nOptimized existing databases including **Oracle** and **SQL Server** through advanced **SQL** querying techniques and performance tuning, resulting in a **30% decrease** in query response times.\nDeveloped machine learning models for predictive analytics using **Python** and **PySpark**, encompassing regression, classification, and clustering techniques, which informed critical business decisions based on model-driven insights.\nEstablished a comprehensive **Data Lake** and **Data Warehouse** architecture applying the **Medallion architecture** approach, enhancing data reliability and analytical capabilities, driven by automation tools like **Apache Airflow** to orchestrate workflows efficiently.\nFacilitated cross-functional collaboration to ensure successful alignment on **Enterprise Architecture** principles, improving data-related project delivery by **25%**."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Implemented and optimized data management strategies using **Snowflake**, **Teradata**, and **SQL Server** to ensure efficient data processing and storage across **data lakes** and **data warehouses**, maintaining high performance in e-commerce analytics.\nDesigned robust **ETL pipelines** using **Databricks**, **Talend**, and **Informatica**, enhancing **data governance** and ensuring compliance with **enterprise architecture** principles, while managing **Hadoop** and **Apache Iceberg** frameworks to support scalable data solutions.\nUtilized **Python** with **PySpark** and **Snowpark** to process large datasets, leveraging **DataFrames** for seamless **batch processing** and **stream processing**, achieving up to **50%** improvement in data processing efficiency.\nDeveloped predictive models using **linear regression** and **time series analysis**, resulting in forecast accuracy enhancements of over **30%**, and integrated findings into operational workflows to optimize business decisions.\nLeveraged **AWS** and **Azure** for **IaaS** and **PaaS** solutions, ensuring robust disaster recovery strategies, enhanced security protocols, and efficient **networking** practices for data accessibility and compliance with **GDPR**.\nApplied **DevOps** practices to streamline the deployment of data pipelines with **DBT** and **orchestrated tasks** using **Apache Airflow**, improving team collaboration and reducing deployment times by **40%**.\nCreated interactive dashboards and reports utilizing **Tableau** and **PowerBI**, enabling real-time data visualization and strategic insights for stakeholders, leading to a **25%** increase in data-driven decision making.\nExecuted strong principles of **data mesh** and **data fabric** architecture, fostering a culture of ownership across teams for improved data connectivity and access, ultimately enhancing cross-functional collaboration.\nEngaged in detailed **data modeling** using **Data Vault** and **Kimball** methodologies, ensuring accuracy and relevance in data that supports critical business functions.\nDeveloped security measures including **encryption**, **identity management**, and role-based access control to protect sensitive customer data and ensure compliance with industry standards."
    }
  ],
  "skills": "****Backend Frameworks:\n\tFastAPI, Flask, Django\n\n****Frontend Frameworks:\n\tReact, Vue, Angular\n\n****Programming Languages:\n\tPython, JavaScript/TypeScript\n\n****API Technologies:\n\tKeycloak (OIDC, RBAC), OAuth2, JWT\n\n****Serverless and Cloud Functions:\n\tAWS: ECS, Lambda, RDS, S3, Azure: App Services, Blob Storage, SQL Database\n\n****Databases:\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, Snowflake, Teradata, Oracle, SQL Server\n\n****Data Technologies:\n\tDatabricks, Hadoop, Spark, SQL, PySpark, Snowpark, DataFrames, Parquet, Avro, Apache Iceberg, Delta Lake, DBT, Talend, Informatica\n\n****Data Architecture:\n\tData Mesh, Data Vault, Data Fabric, Data Governance, Data Management, Enterprise Architecture, Data Lake, Data Warehouse, Medallion architecture, Kimball, 3NF\n\n****DevOps:\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n****Cloud & Infrastructure:\n\tAWS, Azure, IaaS, PaaS\n\n****Other:\n\tMicroStrategy, Thoughtspot, Tableau, PowerBI, Streamlit, SAS, time series analysis, Advanced SQL, linear regression, variance analysis, modeling, forecasting, classification, regression, clustering, dimensionality reduction, Natural Language Processing, Language Models, Disaster Recovery, batch processing, stream processing, Networking, Security, Encryption, Identity Management",
  "apply_company": "Snowflake"
}
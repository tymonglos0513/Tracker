{
  "name": "DANIEL HE",
  "role_name": "Senior Data Engineer",
  "email": "daniel.he8@outlook.com ",
  "phone": "+48 730 743 032",
  "address": "Rzesz√≥w, Poland",
  "linkedin": "https://www.linkedin.com/in/daniel-he-044a76397/",
  "profile_summary": "Results-driven Senior Data Engineer with extensive experience in handling large-scale data processing using **Apache Hadoop**, **Apache Spark**, and **Apache Flink**. Proficient in building and orchestrating data pipelines with **Apache Kafka**, and managing workflows using **Apache Airflow**. Adept at utilizing **AWS** for cloud-based solutions and skilled in coding with **Python**, **SQL**, and **Scala**. Expertise in containerization with **Docker** and orchestration using **Kubernetes**, ensuring robust deployment processes through efficient **CI/CD** practices using **GitHub** and Version Control. Proven track record of driving significant improvements in data architecture and automation across various sectors including healthcare and finance, leveraging a strong commitment to clean, efficient coding. Recognized for creating scalable, reliable solutions that enhance user experiences.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2012",
      "to_year": "2016",
      "location": "Beijing, China ",
      "university": "Tsinghua University "
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Britenet",
      "from_date": "Feb 2023 ",
      "to_date": "Present",
      "location": "Warsaw, Poland",
      "responsibilities": "Utilized **AWS** cloud services for managing data processing workflows, ensuring scalable architecture with **Kubernetes** and maintaining **99.9%** uptime across all platforms.\nEmployed **Docker** for containerization, enhancing the deployment strategy and enabling seamless orchestration with **Kubernetes**.\nDeveloped data pipelines using **Apache Kafka** for real-time data streaming, achieving a throughput increase of **50%** in data ingestion rates.\nImplemented **Apache Spark** for big data processing tasks, reducing processing time by **60%** and facilitating complex transformations on large datasets.\nEstablished **CI/CD** pipelines using **GitHub Actions**, streamlining the development cycle and decreasing the time to production by **80%**.\nApplied **Python** and **SQL** for data manipulation and querying, enhancing data retrieval speed and accuracy by **40%**.\nIntegrated **dbt** for data modeling and analytics, leading to improved data reliability across different reporting sources.\nMonitored and optimized data workflows with **Apache Airflow**, responsibly managing dependencies and scheduling for efficient data processing."
    },
    {
      "role": "Software Engineer",
      "company": "Alibaba Group",
      "from_date": "Oct 2020 ",
      "to_date": "Dec 2022",
      "location": "Hangzhou, China",
      "responsibilities": "- Leveraged **AWS** services and **Docker** for deploying scalable data pipelines, reducing infrastructure setup time by **50%**.\n- Developed and optimized ETL processes using **Apache Spark** and **Flink**, enhancing data processing speed by **30%**.\n- Implemented a robust data orchestration system with **Apache Airflow**, achieving a **95%** success rate in data pipeline execution.\n- Automated CI/CD processes using **Jenkins** and **GitHub**, ensuring seamless integration and delivery of data solutions.\n- Developed real-time data streaming applications with **Kafka** and **Confluent**, maintaining a **98%** timely notification rate for system alerts.\n- Utilized **Python** and **SQL** for data transformation and analysis, improving data reporting accuracy by **20%**.\n- Collaborated in a version-controlled environment (Git) and implemented rigorous version control practices to track changes effectively."
    },
    {
      "role": "Software Engineer ",
      "company": "Huawei",
      "from_date": "May 2016 ",
      "to_date": "Sep 2020",
      "location": "Shenzhen, China ",
      "responsibilities": "Utilized **AWS** for cloud solutions, ensuring scalable data infrastructure and achieving **99.9%** reliability.\nDeveloped data pipelines integrating **Apache Kafka** for real-time data processing, improving data throughput by **50%**.\nEmployed **Apache Spark** for large-scale data processing, reducing data processing time by **30%**.\nImplemented CI/CD practices with **Docker** and **Kubernetes**, automating deployment workflows and enhancing system stability.\nCollaborated with cross-functional teams to integrate data solutions using **Airflow** for orchestration, streamlining data workflows and reducing operational overhead.\nUtilized **Python** and **SQL** for data manipulation and analysis, ensuring data accuracy and integrity.\nLeveraged **dbt** for data transformation, optimizing query performance and enhancing reporting capabilities."
    }
  ],
  "skills": "Programming Languages: Python, SQL, Java, JavaScript (React, Vue.js, Angular, Pixi.js, Vanilla JS, Node.js, TypeScript), Go, Swift, Kotlin, Scala\n\n\tBackend Frameworks: Spring Boot, Spring Security, Hibernate, Node.js, Express.js, JPA, Kafka, RabbitMQ\n\n\tFrontend Frameworks: React, Vue.js, Angular, Pixi.js, Redux, Next.js, React Router, Material-UI, Ant Design, D3.js, Webpack, Babel, Jest\n\n\tAPI Technologies: RESTful API design\n\n\tServerless and Cloud Functions: AWS (EKS, RDS, CloudFront, S3, Lambda), API Gateway, CloudWatch\n\n\tDatabases: MySQL, PostgreSQL, Redis, MongoDB\n\n\tDevOps: Docker, Kubernetes (AWS EKS), Jenkins, GitHub Actions, Terraform, AWS CodePipeline, Git, Version Control\n\n\tCloud & Infrastructure: ELK Stack\n\n\tOther: Microservices, CI/CD, Infrastructure as Code (IaC), Data structures and algorithms, Agile/Scrum methodologies, Apache, Hadoop, Spark, Flink, Confluent, Airflow, dbt",
  "apply_company": "Cloudbeds"
}
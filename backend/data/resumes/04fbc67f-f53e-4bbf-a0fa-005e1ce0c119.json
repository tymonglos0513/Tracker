{
  "name": "Patryk Zaslawski",
  "role_name": "Senior Data Engineer",
  "email": "patrykzas0428@outlook.com",
  "phone": "+48669862402",
  "address": "Gdanski, Poland",
  "linkedin": "https://www.linkedin.com/in/patryk-z-107978395/",
  "profile_summary": "As a Senior Data Engineer with 8+ years of experience in software engineering, I excel in API development, Data Analytics, and ETL processes, ensuring the delivery of high-quality data solutions. Proficient in **Python** and **TypeScript**, I leverage powerful tools like **BigQuery** and **Google Cloud Platform** to optimize data workflows and enhance analytics capabilities. My strong background in frontend frameworks, including **React** and **Vue**, complements my technical skill set, enabling me to create efficient user-facing applications. I am experienced with authentication protocols such as **OAuth 2.0** and **OpenID Connect**, and I prioritize security best practices throughout my projects. Additionally, my familiarity with BI tools and data modeling enhances my ability to drive data-driven decision-making processes. Committed to maintaining high performance and reliability, I apply monitoring practices and strategies aligned with industry standards.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2014",
      "to_year": "2017",
      "location": "United Kingdom",
      "university": "The University of Manchester"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Binary Studio",
      "from_date": "Aug 2023",
      "to_date": "Present",
      "location": "United Kingdom (Remote)",
      "responsibilities": "• Developed and maintained data pipelines utilizing **Python** and **ETL** processes to ensure efficient handling of large datasets.\n• Designed and implemented **API development** strategies incorporating **OAuth 2.0** and **OpenID Connect** for secure access to healthcare and financial data.\n• Conducted data analytics and modeling in **BigQuery** on the **Google Cloud Platform**, optimizing data extraction and transformation for reporting.\n• Integrated **BI tools** with data services, delivering key insights through visualizations and reports to stakeholders.\n• Standardized security best practices across projects, ensuring compliance with industry standards such as **SOC 2**, **HIPAA**, and **PCI DSS**.\n• Collaborated with front-end teams using **React** and **Vue**, ensuring fluid data interactions and user experiences.\n• Implemented monitoring solutions using tools like Prometheus and Grafana to enhance system observability and performance tracking.\n• Led the transition of legacy data systems to cloud-native solutions, improving data access and reliability by **30%**.\n• Mentored junior engineers on best practices in **Python**, **data modeling**, and cloud architecture, fostering a culture of continuous learning.\n• Drove the adoption of agile methodologies in project management, increasing team productivity and delivery speed by **20%**."
    },
    {
      "role": "Software Engineer",
      "company": "Ardigen",
      "from_date": "Mar 2020",
      "to_date": "Aug 2023",
      "location": "Poland",
      "responsibilities": "• Developed and maintained scalable data pipelines in **Python** and **TypeScript** for efficient ETL processes, ensuring reliability and performance.\n• Implemented data analytics solutions leveraging **BigQuery** on **Google Cloud Platform**, optimizing data queries and enhancing reporting capabilities.\n• Designed and built interactive user interfaces using **React** and **Vue**, integrating them seamlessly with backend data services for improved user experience.\n• Developed and secured RESTful APIs using **Python**, with robust implementation of **OAuth 2.0** and **OpenID Connect** to ensure secure access and data integrity.\n• Managed data modeling and analytics frameworks, integrating various **BI tools** to provide actionable insights and advanced reporting features.\n• Configured monitoring systems to track data pipeline performance, ensuring high availability and timely data processing in line with company SLAs.\n• Employed security best practices in data handling, including encryption and access control measures to safeguard sensitive information.\n• Collaborated with cross-functional teams to design data integration solutions that facilitate smooth data flow and compliance with industry standards.\n• Conducted performance tuning of data processing workflows, achieving a **30%** reduction in processing times.\n• Maintained automated testing frameworks using **PyTest** for backend services, fostering a culture of quality assurance with **continuous integration** and **continuous delivery** methodologies."
    },
    {
      "role": "Software Engineer",
      "company": "Altum Software",
      "from_date": "Oct 2017",
      "to_date": "Feb 2020",
      "location": "United Kingdom",
      "responsibilities": "Leveraged **Google Cloud Platform** and **BigQuery** for efficient data analytics and management, improving data accessibility and decision-making processes for the organization.\nDesigned and developed secure APIs in **Python** with OAuth 2.0 and OpenID Connect for seamless integration and authentication, ensuring quick access to data services.\nImplemented ETL processes for large datasets, utilizing **Python** to transform raw data into structured formats, boosting data quality and analytics capabilities.\nUtilized **TypeScript** alongside **React** and **Vue** to create interactive data visualization dashboards, enabling stakeholders to analyze trends and insights dynamically.\nDeveloped monitoring solutions using **Python** to track the performance of data pipelines, ensuring efficient operation and swift problem resolution.\nEnsured adherence to security best practices in all data-related projects, protecting sensitive information and maintaining compliance with regulations.\nConducted data modeling exercises to optimize storage and retrieval with cloud-based solutions, resulting in a **30%** reduction in query times.\nCollaborated with cross-functional teams to align data solutions with business intelligence needs, delivering actionable insights and improving decision-making efficiency by **25%**.\nAutomated data processing tasks with CI/CD workflows using tools like GitHub Actions and Jenkins, improving feature delivery times for data projects by **40%**.\nEmployed modern BI tools to visualize and present data findings, facilitating informed business strategies across the organization.\nEngineered responsive data reporting systems in **React** and **Vue**, empowering end-users to access critical information in real-time.\nMigrated and optimized legacy data systems into cloud-based infrastructures, modernizing processes while maintaining data integrity and availability.\nDesigned robust backup and recovery strategies using **BigQuery**, ensuring data protection and minimizing downtime during adverse situations."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Django, Flask, Spring Boot\n\n **Frontend Frameworks:**\n\tReact, Vue.js (2/3), Angular (1–16), Next.js, Blazor\n\n **API Technologies:**\n\tREST & gRPC APIs, OAuth 2.0, OpenID Connect\n\n **Serverless and Cloud Functions:**\n\tAWS (Lambda), Google Cloud Platform\n\n **Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, BigQuery\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD\n\n **Cloud & Infrastructure:**\n\tAWS (ECS, RDS, S3), Azure (App Services, Blob, SQL)\n\n **Other:**\n\tSQLAlchemy, Pydantic, Celery, ETL, Data Analytics, BI tools, Data modeling, Monitoring, Security best practices, Keycloak (OIDC, RBAC), JWT, Let’s Encrypt, Nginx, Certbot",
  "apply_company": "Signify"
}
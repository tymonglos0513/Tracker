{
  "name": "Tomasz Lee",
  "role_name": "Senior Data Engineer",
  "email": "tomasz.lee8@outlook.com",
  "phone": "+48 732 145 942",
  "address": "Warsaw, Poland",
  "linkedin": "https://www.linkedin.com/in/tomasz-lee-b9a25a391/",
  "profile_summary": "Results-oriented Senior Data Engineer with 9+ years of experience in leveraging **AWS**, **Azure Data Factory**, and **Databricks** for building scalable data pipelines and ETL processes. Proficient in using **Python** for data manipulation, along with expertise in **SQL**, **Spark**, and data modeling techniques to derive actionable insights. Skilled in deploying CI/CD practices using **Git** to enhance development efficiency. Familiar with visualization tools like **Power BI** and **Looker** for translating complex data into user-friendly reports. Proven track record in integrating messaging platforms such as **Kafka** to ensure reliable data streams and applying machine learning techniques to improve data quality. Original strengths include a solid foundation in full stack development, particularly with **.NET**, **C#**, and front-end frameworks such as **ReactJS** and **NextJS**, coupled with strong leadership and collaboration skills to drive projects to successful completion.",
  "education": [
    {
      "degree": "Master of Computer Science",
      "category": "",
      "from_year": "Apr 2012",
      "to_year": "Oct 2014",
      "location": "",
      "university": "National University of Singapore"
    },
    {
      "degree": "Bachelor of Computer Science",
      "category": "",
      "from_year": "Apr 2008",
      "to_year": "Mar 2012",
      "location": "",
      "university": "National University of Singapore"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "CD Projekt RED",
      "from_date": "May 2022",
      "to_date": "Oct 2025",
      "location": "Warsaw, Poland",
      "responsibilities": "Utilized **AWS** and **Azure Data Factory** to orchestrate ETL processes, ensuring effective data integration across various sources.\nDeveloped and maintained data pipelines using **Databricks** and **Spark**, optimizing data processing speed by **30%**.\nImplemented **CI/CD** pipelines utilizing **Git** for version control, enhancing deployment efficiency by **40%**.\nApplied **SQL** for data querying and transformation, directly enhancing database performance by **20%** through optimized queries and indexing.\nLeveraged **Power BI** and **Looker** for data visualization and reporting, enabling real-time insights and influencing strategic decision-making.\nCollaborated with data scientists to integrate **Python** for machine learning models, improving predictive accuracy by **15%**.\nDesigned and managed data models to facilitate effective storage and retrieval, utilizing best practices for data structuring and governance.\nImplemented **Airflow** for orchestrating complex data workflows, increasing operational efficiency and monitoring capabilities.\nEnsured data quality and reliability by using tools like **Great Expectations** and **Soda**, significantly reducing errors during data processing.\nIntegrated **Kinesis** for real-time analytics and monitoring, enhancing data streaming capabilities for instant insights."
    },
    {
      "role": "Software Engineer",
      "company": "Sea Group",
      "from_date": "Mar 2018",
      "to_date": "Apr 2022",
      "location": "Shopee, Singapore",
      "responsibilities": "Leveraged **AWS**, **Azure Data Factory**, and **Databricks** to design and implement scalable ETL pipelines, resulting in a 30% reduction in data processing time.\nUtilized **Apache Kafka** and **Spark** for real-time data streaming and analytics, enhancing data processing capabilities by 50%.\nDeveloped and maintained data models and transformation processes using **SQL** and **Python**, ensuring data integrity and quality throughout.\nImplemented CI/CD practices using **Git**, significantly improving deployment frequency by 25% and reducing errors by 30%.\nCreated dynamic visualizations and dashboards with **Power BI** and **Looker** for insightful data reporting, increasing stakeholder engagement by 40%.\nIntegrated **Airflow** to orchestrate and manage workflows, optimizing task scheduling and execution times.\nApplied **Great Expectations**, **Soda**, and **Snowplow** for data quality checks, ensuring compliance and accuracy in the dataset.\nCollaborated with cross-functional teams to identify and implement machine learning solutions, driving 20% improvements in predictive analytics accuracy.\nConducted data validation and testing using **Kinesis**, ensuring seamless integration of data across platforms.\nMentored junior team members on best practices in data engineering, improving overall team efficiency and skill levels."
    },
    {
      "role": "Software Developer",
      "company": "Grab Holdings Inc",
      "from_date": "Jan 2015",
      "to_date": "Feb 2018",
      "location": "Singapore",
      "responsibilities": "Utilized **Azure Data Factory** for efficient data integration processes, ensuring streamlined ETL workflows across various platforms.\nDeveloped data pipelines leveraging **Databricks** and **Spark**, executing transformations that improved data processing time by **35%**.\nImplemented CI/CD practices with **Git** and **Airflow**, automating deployment schedules and increasing productivity by **25%**.\nDesigned and optimized SQL queries to improve data retrieval efficiency, enhancing performance by **18%**.\nWorked collaboratively with data analysts to create dashboards in **Power BI** and **Looker**, providing insightful data visualizations that improved decision-making.\nConfigured and managed data ingestion processes with **Kafka** and **Kinesis**, achieving real-time data processing capabilities.\nIntegrated machine learning models into data pipelines to enhance data quality and predictive analysis, using frameworks like **Great Expectations** and **Soda**.\nParticipated in code reviews and agile ceremonies, fostering a culture of continuous improvement and collaboration across teams.\nDelivered comprehensive technical documentation and training materials to support knowledge transfer among team members."
    }
  ],
  "skills": "**Programming Languages**\n\tPython, TypeScript, JavaScript, C#, SQL\n\n**Backend Frameworks**\n\tNodeJS, ExpressJS, NestJS, .NET, Entity Framework, Microservices\n\n**Frontend Frameworks**\n\tReactJS, NextJS, VueJS, NuxtJS, Angular, Material UI, Three.js, D3.js, React Native, Flutter, Chakra UI, TailwindCSS\n\n**API Technologies**\n\tRESTful API, GraphQL\n\n**Serverless and Cloud Functions**\n\tAWS, Azure, Databricks, Azure Data Factory, Kinesis\n\n**Databases**\n\tMSSQL Server, MySQL, PostgreSQL, MongoDB, DynamoDB, CosmosDB, SQL\n\n**DevOps**\n\tCI/CD pipelines, Git, GitHub\n\n**Cloud & Infrastructure**\n\tAWS, Azure\n\n**Other**\n\tUX/UI Design, Testing Tools: NUnit, xUnit, Selenium, Moq, Postman, Cypress, JMeter, Jest, Messaging & Caching: Apache Kafka, RabbitMQ, Redis, Blockchain: Solidity, Ether.js, Web3.js, Ethereum, ETL, Spark, Power BI, Looker, Airflow, data modeling, Great Expectations, Soda, Snowplow, machine learning",
  "apply_company": "Publitas.com"
}
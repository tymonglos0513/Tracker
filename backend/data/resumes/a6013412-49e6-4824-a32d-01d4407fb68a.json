{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in Data Engineering, specializing in ETL and ELT processes, Data Warehousing, and Data Modeling. Proficient in **Python**, **dbt**, **Snowflake**, and **Databricks**, with extensive expertise in building and optimizing data pipelines and ensuring Data Quality.\nSkilled in leveraging cloud services including **Amazon Web Services (AWS)**, and employing IaC tools like **Terraform** and **CloudFormation** for robust deployment strategies. Proven ability to collaborate effectively within teams to deliver high-quality data solutions that drive business insights.\nIn addition to data engineering, I bring a wealth of experience in application development using **JavaScript/TypeScript**, **Flutter**, and backend technologies such as **Node.js**, **FastAPI**, and **Django**. My comprehensive knowledge of compliance-driven development in the healthcare and financial sectors ensures alignment with regulatory standards like HIPAA and PCI DSS.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Designed and implemented efficient **ETL** and **ELT** processes for large-scale data warehousing projects using **Python** and **dbt**, ensuring high data quality and reliability across systems.\nDeveloped complex data models and pipelines utilizing **Snowflake** and **Databricks**, achieving up to **40%** faster data retrieval and processing times for analytical queries.\nCollaborated with cross-functional teams to integrate customer data from various platforms, leveraging tools in **Amazon Web Services (AWS)** to enhance data accessibility and usability.\nBuilt and deployed infrastructure using **Terraform** and **CloudFormation**, automating the provisioning of data environments to increase deployment efficiency by **50%**.\nImplemented data quality checks and monitoring systems, achieving a reduction in data inconsistency issues by **30%** through automated validation scripts.\nOptimized data processing workflows to support real-time analytics, facilitating immediate insights and decision-making for business stakeholders.\nConducted data warehousing solutions architecture, enabling seamless data integration and reporting across multiple departments, leading to improved collaboration.\nDesigned comprehensive documentation and training resources, fostering team knowledge sharing and adherence to best practices in data engineering.\n"
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "- Engineered robust **ETL** pipelines for extracting, transforming, and loading financial data leveraging **Python**, **Apache Airflow**, and **Azure Data Factory**, achieving a processing efficiency increase of **30%**.\n- Designed and implemented data warehousing solutions in **Snowflake** and **Databricks**, enhancing data accessibility and analytics capabilities for large datasets, supporting data queries up to **10TB**.\n- Developed automated **data modeling** strategies with **dbt** to uphold high **data quality** standards across multiple datasets, improving data reliability by **25%**.\n- Collaborated effectively with cross-functional teams to enhance data integration from internal and third-party sources, ensuring streamlined workflows and timely delivery of insights.\n- Explored partnerships with **Amazon Web Services** to deploy scalable data solutions, optimizing costs by leveraging serverless architectures with **Terraform** and **CloudFormation**.\n- Applied advanced analytical techniques, such as predictive modeling for customer behavior and data classification tasks, resulting in enhanced decision-making processes within the organization.\n- Drove initiatives to maintain and enhance data integrity and quality assurance protocols, securing trust in data-driven strategies and operations."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Utilized **Python** for ETL and ELT processes, ensuring seamless data integration and transformation for a data warehousing solution, optimizing workflows to handle **9+ TB** of data daily.\nEngineered data models and pipelines using **dbt** and **Snowflake**, enabling structured storage and analysis of customer data with an average query response time of under **100 ms**.\nImplemented data quality checks to maintain high standards, effectively reducing data errors by **30%**, enhancing the reliability of the data provided to stakeholders.\nCollaborated across teams leveraging **Amazon Web Services (AWS)** for scalable cloud architecture, employing **Terraform** for infrastructure as code, ensuring reproducibility and easy environment management.\nDesigned and maintained data pipelines using **Databricks**, facilitating large-scale real-time analytics and reporting to support business intelligence initiatives.\nCreated a Customer Data Platform (CDP) for centralized data access, enhancing user data insights and segmentation for targeted marketing strategies, improving campaign effectiveness by **25%**.\nImplemented best practices for data security and compliance, ensuring adherence to regulations like GDPR and providing role-based access control (RBAC) for sensitive information management.\nFostered a culture of collaboration and continuous improvement by conducting regular code reviews, pair programming sessions, and knowledge-sharing workshops, promoting the adoption of data engineering best practices across the organization."
    }
  ],
  "skills": " **Programming Languages:**\n\t•\tPython\n\n**Backend Frameworks:**\n\t•\tFastAPI\n\t•\tFlask\n\t•\tDjango\n\n**Frontend Frameworks:**\n\t•\tJavaScript/TypeScript: React\n\t•\tVue\n\t•\tAngular\n\n**API Technologies:**\n\t•\tKeycloak (OIDC, RBAC)\n\t•\tOAuth2\n\t•\tJWT\n\t•\tNginx\n\n**Serverless and Cloud Functions:**\n\t•\tAWS: ECS\n\t•\tLambda\n\t•\tRDS\n\t•\tS3\n\t•\tAzure: App Services\n\t•\tBlob Storage\n\t•\tSQL Database\n\n**Databases:**\n\t•\tPostgreSQL (Fintech)\n\t•\tMySQL (Healthcare)\n\t•\tMongoDB (Gaming)\n\t•\tRedis\n\t•\tSnowflake\n\n**DevOps:**\n\t•\tDocker\n\t•\tKubernetes\n\t•\tGitHub Actions\n\t•\tGitLab CI/CD\n\n**Cloud & Infrastructure:**\n\t•\tTerraform\n\t•\tCloudFormation\n\t•\tAnsible\n\t•\tHelm\n\t•\tDocker Compose\n\n**Other:**\n\t•\tData Engineering\n\t•\tETL\n\t•\tELT\n\t•\tData Warehousing\n\t•\tData Modeling\n\t•\tdbt\n\t•\tDatabricks\n\t•\tCustomer Data Platforms\n\t•\tData Quality\n\t•\tCollaboration",
  "apply_company": "Olo"
}
{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-187801395/",
  "profile_summary": "Data Engineer with 13+ years of experience specializing in **Python**, **AWS** (including **S3**, **Glue**, **Lambda**, **DynamoDB**, **Athena**), and data processing pipelines. Highly skilled in designing and implementing ETL/ELT processes, data modeling, and developing Generative AI solutions. Proficient in utilizing **Airflow**, **Step Functions**, **Prefect**, **Spark**, and **Hadoop** to streamline data workflows and optimize performance. Adept in ensuring data governance and security best practices while driving agile methodologies and problem-solving strategies.\n\nDemonstrated expertise in building high-performance applications in healthcare and financial industries. Strong background in cloud-native systems deployment and working with compliance-driven development principles, aligning solutions with industry standards.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** for developing efficient **data processing pipelines** to manage and transform large datasets, ensuring secure and compliant solutions across multiple environments.\nDesigned and implemented ETL and ELT processes involving **AWS services** such as **S3**, **Glue**, **Lambda**, and **DynamoDB**, resulting in a **30%** increase in data processing efficiency.\nBuilt robust **SQL** and **NoSQL** **data models** using PostgreSQL, MongoDB, and Athena, facilitating seamless data retrieval and analytics for healthcare and fintech applications.\nLed the integration of **Airflow** for orchestrating complex workflows, enhancing operational reliability and enabling automated data loading processes with a **40%** reduction in manual tasks.\nEmployed **Spark** and **Hadoop** for distributed data processing, handling large-scale datasets and improving computing performance by **50%**.\nDeveloped and maintained data governance strategies to ensure data integrity, security, and compliance with regulations such as HIPAA and GDPR across all data pipelines.\nApplied **Generative AI** techniques alongside machine learning frameworks to enhance data accuracy and extract insights from unstructured data sources.\nPromoted **agile practices** within the team, streamlining project delivery and fostering collaboration through iterative development and continuous feedback.\nImplemented rigorous security best practices across all data storage and processing layers, safeguarding sensitive information and maintaining compliance standards.\nDemonstrated strong **problem-solving** skills through the identification and resolution of data-related issues, enabling timely project deliverables and positive stakeholder outcomes."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Designed and implemented data processing pipelines utilizing **Python**, **Apache Airflow**, and **AWS Lambda** to manage and orchestrate ETL workflows efficiently across varied data sources.\nDeveloped scalable data models in **DynamoDB** and **SQL**, ensuring optimal performance for both transactional and analytical workloads with a focus on **data governance** and security best practices.\nLeveraged **AWS S3** for effective data storage, enabling robust **data processing** capabilities that support batch and real-time data ingestion by integrating **AWS Glue** for ETL operations.\nCreated and maintained data lakes with **AWS Athena**, allowing for seamless querying and analytics on large datasets while optimizing resource utilization and costs.\nUtilized **Apache Spark** and **Hadoop** to process extensive datasets, achieving significant performance enhancements in data transformation operations and supporting **generative AI** initiatives across business units.\nEmployed effective **problem-solving** techniques to troubleshoot data issues, guaranteeing high data reliability and integrity in dynamic environments.\nActively participated in agile practices, contributing to sprint planning and retrospectives to enhance team performance and delivery outcomes while meeting project deadlines effectively.\nDeployed data models and pipelines with **Prefect** for orchestrating complex workflows, ensuring compliance with **data governance** standards and enhancing operational efficiency.\n"
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Designed and optimized data processing pipelines using **Python** and **AWS services** such as **Glue**, **Lambda**, and **DynamoDB**, ensuring efficient ETL and ELT processes across diverse datasets.\nEngineered data modeling solutions leveraging **SQL** and **NoSQL** technologies to facilitate robust data governance and improve data accessibility for analytical tasks.\nDeveloped automated workflows for data integration and transformation utilizing **Apache Airflow** and **Prefect**, resulting in a **30%** reduction in processing time and increased operational efficiency.\nImplemented responsive solutions using **Generative AI** methodologies to improve data extraction and processing, optimizing performance across multiple data streams.\nApplied security best practices and role-based access control (RBAC) to safeguard sensitive data, ensuring compliance with GDPR and other data protection regulations.\nUtilized **AWS S3** for secure data storage and integrated **Athena** for querying large datasets with high performance and flexibility.\nBuilt data lakes using **Hadoop** and **Spark** frameworks to manage unstructured data, achieving a **40%** improvement in data retrieval speed.\nCollaborated with cross-functional teams in an agile environment to address complex data challenges, enhancing problem-solving capabilities and fostering a culture of continuous improvement.\nConducted data quality assessments and validation checks, leading to a **25%** increase in data accuracy across platforms."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, JavaScript/TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tReact, Vue, Angular\n\n **API Technologies:**\n\tOAuth2, JWT\n\n **Serverless and Cloud Functions:**\n\tAWS: Lambda, S3, Glue, Step Functions, DynamoDB, Athena\n\n **Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, NoSQL, SQL\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n **Cloud & Infrastructure:**\n\tAWS: ECS, Azure: App Services, Blob Storage, SQL Database\n\n **Other:**\n\tMLflow, Airflow, Kubeflow, data processing pipelines, data modeling, ETL, ELT, Generative AI, Spark, Hadoop, data governance, security best practices, problem-solving, agile practices"
}
{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Pentaho Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-152840397/",
  "profile_summary": "Results-driven Senior Pentaho Engineer with 8 years of experience in data integration and analytics, specializing in technologies like **Pentaho**, **Python**, **Databricks**, and **Airflow**. Adept at utilizing **SQL** and **PL/SQL** for data manipulation and reporting across Oracle and other databases, I have a proven track record in developing and optimizing ETL processes using **Powercenter**, **DataStage**, and **SSIS**. My experience extends to creating insightful data visualizations with tools like **MicroStrategy**, **QLIK**, and **Superset**. \n\nIn addition to my technical expertise, I have led cross-functional teams in implementing innovative solutions that enhance data-driven decision-making. I excel in environments that require strong collaboration, ensuring compliance with industry standards while consistently delivering high-quality solutions.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Pentaho Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Utilized **Pentaho Data Integrator** to architect and develop ETL processes for scalable healthcare and financial data management, leveraging **SQL** and **PL/SQL** for efficient data processing and transformation.\n- Designed, implemented, and maintained reporting solutions using **MicroStrategy**, **QLIK**, and **Superset** to deliver insightful analytics and visualizations for key stakeholders.\n- Collaborated with cross-functional teams to innovate data integration solutions, ensuring compliance with healthcare regulations and high performance in **Oracle** and **Databricks** environments.\n- Developed and orchestrated data pipelines with **Apache Airflow** for pipeline scheduling, real-time data ingestion, and automated workflows, enhancing delivery times by **30%**.\n- Integrated various data sources and systems using **PowerCenter** and **DataStage** for seamless data flow management, significantly improving data reliability across different platforms.\n- Worked collaboratively in agile teams, fostering a culture of teamwork and innovation to drive project success and deliver high-quality outputs consistently.\n- Leveraged metadata injection strategies for efficient ETL design, streamlining the development process and reducing deployment time by **25%**.\n- Developed complex SQL queries and optimized database performance for operational efficiency, which improved reporting times and reduced query execution by **15%**.\n- Provided technical leadership and mentorship to junior engineers on data integration best practices, enhancing team capabilities and fostering knowledge sharing."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Developed and optimized ETL pipelines using **Pentaho Data Integrator** and **Apache Airflow**, ensuring efficient data processing and integration with internal and third-party sources for robust financial reporting.\nLeveraged **SQL** and **PL/SQL** for querying and analyzing large datasets from **Oracle** databases, facilitating data-driven decision-making in financial applications.\nCollaborated with cross-functional teams to implement **Databricks** solutions, enhancing the data analytics capabilities and improving data visualization through **MicroStrategy** and **Power BI** dashboards.\nUtilized **SSIS** and **DataStage** for effective data transformation and migration tasks to meet high data accuracy and timeliness standards.\nEngaged in teamwork and innovation to actively propose and execute enhancements to existing data systems, increasing efficiency by **30%** in data processing workflows.\nCreated and maintained detailed documentation of data pipelines and integration processes to facilitate knowledge sharing across teams and uphold best practices in data management.\nConducted training sessions for team members on **Pentaho** framework capabilities and best practices, fostering knowledge growth and teamwork within the engineering team.\nImplemented metadata injection strategies to streamline data handling in various environments, resulting in more dynamic and responsive data processing solutions."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Utilized **Pentaho Data Integrator** and **PowerCenter** for efficient data extraction, transformation, and loading (ETL) processes, optimizing workflows to handle **9 TB** of data monthly across multiple data sources.\nDeveloped and maintained SQL queries and PL/SQL scripts for data manipulation and retrieval, ensuring high performance and reliability in data operations.\nImplemented **Apache Airflow** for workflow automation, scheduling **150+** data pipelines, and monitoring data integrity for real-time analytics.\nCollaborated with cross-functional teams to design and innovate data solutions that enhance business intelligence capabilities, utilizing **MicroStrategy** and **QLIK** for dashboard development and data visualization.\nExecuted data integration strategies with **Databricks** for large-scale data processing and analytics, improving data accessibility and usability.\nManaged legacy systems and databases, including **Oracle** and SQL databases, ensuring seamless data flow and availability across the organization.\nRegularly applied metadata injection techniques to streamline ETL processes, which contributed to a **30%** reduction in processing time.\nMaintained documentation of ETL processes, data models, and workflows, promoting teamwork and knowledge sharing within the team.\nEnsured compliance with GDPR and data governance standards, implementing role-based access control (RBAC) and secure data handling practices.\nFostered a culture of innovation and continuous improvement, enhancing existing data systems and processes to meet evolving business needs."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tJavaScript, TypeScript, React, Vue, Angular\n\n **API Technologies:**\n\tJWT, OAuth2, Keycloak (OIDC, RBAC)\n\n **Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n **Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, SQL, Oracle, PL/SQL\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n **Cloud & Infrastructure:**\n\tLet’s Encrypt, Nginx, Certbot\n\n **Other:**\n\tMLflow, Airflow, Kubeflow, Databricks, Pentaho, Powercenter, DataStage, SSIS, MicroStrategy, QLIK, Superset, metadata injection, teamwork, innovation",
  "apply_company": "Best in Bi Solutions"
}
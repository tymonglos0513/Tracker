{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior BI Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-623a26390/",
  "profile_summary": "Senior BI Engineer with 13+ years of experience in data engineering and backend development for high-performance applications in the healthcare and financial sectors. Expert in **SQL**, **dbt**, **Databricks**, **Kafka**, and **.NET Core**, with hands-on proficiency in data modeling, ETL, and ELT processes, supporting robust data-driven decision-making. Proven track record in developing and implementing **CI/CD** pipelines and optimizing data workflows to enhance operational efficiency. \nDemonstrated ability to build event streaming architectures and deploy cloud-native systems primarily using **Azure**, creating scalable solutions that meet compliance standards such as HIPAA and PCI DSS. Strong analytical skills aligned with a product-oriented mindset, complemented by a foundational knowledge in AI/ML platforms using **Python** and frameworks such as **FastAPI** and **Django**. Cultivates curiosity and autonomy to drive innovative solutions across diverse projects.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior BI Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **SQL** for data modeling and analysis, ensuring accurate reporting and insights across healthcare and fintech domains.\nImplemented ETL and ELT processes using **dbt** and **Databricks** to streamline data transformation and loading, effectively handling high data volumes.\nDesigned and built data pipelines incorporating **Kafka** for event streaming, facilitating real-time data processing and analytics.\nDeveloped backend services using **.NET Core** and integrated with **Azure** for scalable data solutions, ensuring high availability and performance.\nSpearheaded the establishment of CI/CD pipelines to automate deployment workflows, utilizing tools such as **GitHub Actions** and **CircleCI** for efficient development cycles.\nCreated and managed data infrastructure using modern frameworks, ensuring robust data engineering practices tailored to BI needs.\nFostered a product-oriented mindset, addressing stakeholder requirements and driving the implementation of insightful data solutions.\nLeveraged curiosity and autonomy to explore innovative techniques for enhancing data visualization and reporting capabilities with tools like **GoodData**.\nConducted extensive testing and validation across data models and pipelines to guarantee accuracy and reliability in BI outputs.\nCollaborated with cross-functional teams to automate and optimize data processes, enhancing overall operational efficiency."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized **SQL** and **dbt** to enhance data modeling and transformation processes, enabling efficient data retrieval and reporting in a financial environment.\nImplemented **ETL** and **ELT** processes using **Databricks** for streamlined data ingestion and processing, achieving a **30%** reduction in data processing time.\nDeveloped and optimized **Kafka**-based event streaming solutions to support high volume data workflows, ensuring resilience and performance in data pipelines.\nDesigned and executed CI/CD pipelines for deployment automation, enhancing code quality and deployment efficiency by **40%**.\nLeveraged **Azure** cloud services to build scalable and secure data solutions, taking advantage of features like Azure Synapse and Azure Data Lake Storage for enhanced data management.\nCollaborated on the creation and maintenance of data models to support analytical and reporting requirements, contributing to a **25%** increase in reporting accuracy and speed.\nDrove the integration of **GoodData** for advanced business intelligence reporting, providing stakeholders with actionable insights and visualizations.\nContributed to backend development efforts using **.NET Core** to ensure robust application performance and maintainability, aligned with a product-oriented mindset.\nEmbraced curiosity and autonomy in exploring new technologies and methodologies to continuously improve data engineering practices and outcomes."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "• Executed data modeling and ETL processes efficiently using **SQL**, **dbt**, and **Databricks**, ensuring high data integrity and optimized data flows for business intelligence insights.\n• Developed and maintained backend services in a data engineering environment leveraging **.NET Core** and **Azure**, focusing on scalability and robust performance for reports and dashboards.\n• Managed event streaming solutions with **Kafka** for real-time data processing, improving latency for actionable insights in BI tools and reports.\n• Implemented CI/CD pipelines to automate deployments and enhance operational efficiency in data workflows, resulting in a **30% reduction** in time-to-production.\n• Cultivated a product-oriented mindset through actively collaborating with cross-functional teams to define analytics requirements and deliver tailored BI solutions.\n• Demonstrated autonomy and curiosity by exploring new data technologies and methodologies to improve existing data engineering processes.\n• Collaborated on the design and execution of complex data models that reduced reporting latency by **40%**, leading to enhanced data retrieval speeds for business users.\n• Improved data quality and consistency with rigorous testing procedures and documentation, elevating overall user trust in data-driven decisions.\n• Maintained performance dashboards utilizing **GoodData**, providing stakeholders with up-to-date metrics crucial for strategic planning.\n• Coordinated with data warehouse teams to streamline **ELT** processes, maximizing data accessibility across the organization."
    }
  ],
  "skills": "  **Programming Languages:**\n\tPython\n\tJavaScript/TypeScript\n\n  **Backend Frameworks:**\n\tFastAPI\n\tFlask\n\tDjango\n\t.NET Core\n\n  **Frontend Frameworks:**\n\tReact\n\tVue\n\tAngular\n\n  **API Technologies:**\n\tKeycloak (OIDC, RBAC)\n\tOAuth2\n\tJWT\n\n  **Serverless and Cloud Functions:**\n\tAWS: Lambda\n\tAzure: App Services\n\n  **Databases:**\n\tPostgreSQL\n\tMySQL\n\tMongoDB\n\tRedis\n\tSQL\n\n  **Data Engineering:**\n\tdbt\n\tDatabricks\n\n  **ETL/ELT:**\n\tApache Airflow\n\tMLflow\n\n  **Event Streaming:**\n\tKafka\n\n  **DevOps:**\n\tDocker\n\tKubernetes\n\tGitHub Actions\n\tGitLab CI/CD\n\n  **Cloud & Infrastructure:**\n\tAWS: ECS\n\tAWS: RDS\n\tAzure: Blob Storage\n\tSQL Database\n\tTerraform\n\tAnsible\n\tHelm\n\tDocker Compose\n\n  **Other:**\n\tCuriosity\n\tAutonomy\n\tProduct-oriented mindset\n\tGoodData\n"
}
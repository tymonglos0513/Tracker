{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-karol-kotlinski-bb2520397/",
  "profile_summary": "As a Senior Data Engineer with 8 years of experience, I leverage my expertise in **Python**, **SQL**, and **RDBMS** to design and implement efficient data pipelines and ETL processes. Proficient in **Pyspark**, **Hadoop**, and cloud services such as **GCP** and **Dataproc**, I excel in data warehousing, data wrangling, and data modeling. My technical background is complemented by experience in **Infrastructure-as-Code** practices, ensuring robust and scalable data solutions.\n\nI have a proven track record in the healthcare and financial sectors, delivering solutions that meet compliance standards and support complex analytical needs. My skills in coaching, communication, and stakeholder management empower teams to overcome challenges and achieve project goals effectively. Additionally, I bring strong problem-solving capabilities and have a collaborative approach to driving success in data-driven initiatives.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** for data processing and ETL functions, leveraging technologies such as **Pyspark** and **Hadoop** to efficiently handle large datasets across both healthcare and financial sectors.\nDesigned and implemented **RDBMS** solutions, integrating with **SQL** databases for robust data storage and retrieval, ensuring high performance and scalability of data pipelines.\nEngineered data workflows utilizing **GCP** technologies (**Dataproc** and **Dataflow**) to streamline data pipelines, enabling real-time data processing and analytics to meet business needs.\nLed the development of data warehousing solutions, crafting a unified data model that enhanced data accessibility and reporting capabilities for stakeholders by 30%.\nCoached team members on best practices in data wrangling, data modeling, and infrastructure-as-code, improving project efficiency and knowledge sharing.\nIntegrated with third-party tools like **Netsuite** and **SAP** for seamless data exchange and processing, enhancing overall workflow efficiency by 25%.\nImplemented problem-solving strategies for data quality issues, driving initiatives that improved data accuracy and reliability across the organization.\nFacilitated stakeholder management by effectively communicating data insights and findings, supporting decision-making processes in a timely manner.\nDesigned and maintained data processing frameworks using **Python**, ensuring compliance with regulatory standards while delivering actionable insights to stakeholders.\nOptimized existing data solutions, leading to a 40% reduction in query response time through rigorous testing and performance tuning of SQL queries."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Utilized **Python** and **SQL** to design and optimize ETL and ELT processes, handling over **5TB** of financial data to ensure accurate data warehousing and reporting.\nImplemented and maintained **RDBMS** using **Hadoop** and **Pyspark**, enhancing data wrangling and modeling capabilities to support high-volume transaction environments.\nDeveloped and deployed data pipelines on **GCP** using **Dataflow** and **Dataproc**, achieving processing efficiencies that improved data availability by **30%**.\nLed data governance initiatives and ensured compliance with financial regulations through effective stakeholder management and problem-solving strategies.\nCoached junior engineers on best practices in data management and modern data architectures, resulting in a **20%** increase in team productivity.\nCollaborated with cross-functional teams to define data requirements and implement solutions in **Netsuite** and **SAP**, improving data accessibility and insight generation.\nDesigned infrastructure as code for automated deployments, enhancing the reliability and scalability of data processing environments.\nFostered effective communication channels with stakeholders to understand business needs, translating them into technical requirements for data engineering initiatives."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Leveraged **Python** for data modeling, wrangling, and ETL processes to enhance the efficiency of data pipelines, ensuring robust and scalable architectures that support data-driven decision-making.\nDesigned and maintained databases using **RDBMS** technologies, optimizing SQL queries for performance on high-volume datasets to guarantee fast data retrieval and reporting.\nDeveloped and implemented data warehousing solutions utilizing **GCP**, **Dataproc**, and **Dataflow** for migrating and transforming large datasets with up to **1TB+** of data daily, supporting business intelligence efforts and analytics.\nUtilized **Hadoop** for distributed data processing, enabling the extraction and transformation of complex datasets while handling up to **50M** records efficiently.\nCoached junior engineers on best practices in data integrity and security, fostering a culture of continuous improvement and ensuring compliance with **GDPR** and data governance regulations.\nCultivated strong communication and stakeholder management skills by collaborating with cross-functional teams to understand data needs and deliver tailored solutions.\nImplemented Infrastructure-as-Code for seamless deployments and resource management using **Java** and **ELT** strategies, improving deployment efficiency by **30%**.\nProblem-solved complex data challenges, leveraging advanced **Pyspark** techniques for large-scale data processing, yielding significant performance improvements in data workflows.\nEnhanced data accessibility and usability through clear documentation and effective communication, ensuring all stakeholders understand and utilize data assets effectively."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython, JavaScript/TypeScript, Java\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tJWT, OAuth2\n\n**Serverless and Cloud Functions:**\n\tAWS (Lambda), Azure (App Services)\n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, RDBMS, SQL\n\n**Cloud & Infrastructure:**\n\tGCP, Terraform, Ansible, Helm, Docker Compose\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD\n\n**Other:**\n\tHadoop, Pyspark, Dataproc, Dataflow, Infrastructure-as-Code, ETL, ELT, data warehousing, data wrangling, data modelling, MLflow, Airflow, Kubeflow, Netsuite, SAP, coaching, communication, problem-solving, stakeholder management, Keycloak (OIDC, RBAC), Let’s Encrypt, Nginx, Certbot"
}
{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Analytics Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "As a Senior Analytics Engineer with 8 years of experience, I excel in utilizing **SQL**, **Python**, and **Airflow** to derive actionable insights from complex data sets. My technical proficiency also includes **dbt**, **Clickhouse**, and **Metabase**, ensuring robust data transformation and visualization capabilities. I have a proven track record in the healthcare and financial sectors, where I built high-performance solutions that leverage data for predictive analytics and intelligent automation.\nFurthermore, I possess a solid foundation in **AI/ML** capabilities and cloud services such as **AWS** and **Azure**, which I integrate into my analytics processes. My experience includes developing and maintaining robust data pipelines, adhering to compliance standards such as HIPAA and PCI DSS, and implementing automation strategies to streamline CI/CD processes. I am committed to creating secure, scalable, and efficient data solutions that empower organizations to achieve their analytical objectives.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Analytics Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Developed complex SQL queries and reports to analyze healthcare and financial data, enhancing decision-making processes.\n- Designed and implemented data transformation workflows using **dbt** to streamline data modeling and ensure data quality for analytics.\n- Built and maintained ETL pipelines with **Airflow**, scheduling and monitoring data workflows to ensure timely data availability for business intelligence.\n- Utilized **Clickhouse** for high-performance analytics, handling large volumes of data with low latency, significantly improving reporting speed by **40%**.\n- Created insightful dashboards using **Metabase** to visualize key performance indicators and operational metrics, leading to a **25%** increase in actionable insights for cross-functional teams.\n- Collaborated with data scientists to integrate predictive analytics models into the analytics pipeline, enhancing forecasting capabilities for both healthcare outcomes and financial trends.\n- Ensured data accuracy and reliability, establishing robust testing frameworks for data quality checks.\n- Actively engaged with stakeholders to understand their analytics needs and provided tailored solutions that improved operational efficiency by **30%**."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Utilized **SQL** to efficiently manage and query large datasets for analytics and reporting, streamlining data retrieval processes and enhancing decision-making capabilities.\nDeveloped data transformation workflows using **Python** and **dbt**, optimizing data pipelines for accuracy and performance, leading to a **30% reduction** in data processing time.\nIntegrated **Apache Airflow** for orchestrating ETL processes, ensuring scheduled data flows and enhancing the reliability of data ingestion from various sources.\nDesigned and maintained dashboards using **Metabase**, facilitating real-time analytics and actionable insights for stakeholders, improving data accessibility by **50%**.\nPerformed data analysis using **Clickhouse** to facilitate deep dives into transaction data, yielding insights that guided product strategies and operational optimizations.\nCollaborated with cross-functional teams to define key performance indicators (KPIs) and metrics, enabling data-driven decisions that enhanced operational efficiency by **25%**.\nBuilt and executed data validation strategies using automated tests to ensure the integrity of data across platforms, protecting against discrepancies in analytics reporting.\nLeveraged modern data visualization techniques in conjunction with business intelligence tools to communicate findings effectively to non-technical stakeholders."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Leveraged **SQL** for database querying and analysis, ensuring data accuracy and integrity for various analytics projects.\nEmployed **Python** for data transformation and analysis, integrating **Airflow** for workflow orchestration and automating data pipelines.\nUtilized **dbt** for data modeling and transformation, creating robust data models to support analytics and reporting needs.\nImplemented data visualizations using **Metabase**, providing stakeholders with actionable insights through dynamic dashboards and reports.\nOptimized data queries and performance on **Clickhouse**, achieving significant improvements in data retrieval times by over **30%**.\nCollaborated with cross-functional teams to extract, transform, and load (ETL) large datasets, enhancing data quality standards and processing efficiency by **25%**.\nDesigned and maintained data pipelines that supported critical business functions, improving data availability and reliability for end-users.\nEnsured compliance with GDPR and data protection regulations while managing sensitive information within analytical workflows.\nEstablished best practices for documentation and data governance, fostering a culture of transparency and accountability in data management."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular)\n\n**API Technologies:**\n\t\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda), Azure (App Services)\n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, SQL, Clickhouse\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS (RDS, S3), Azure (Blob, SQL)\n\n**Other:**\n\tMLflow, Airflow, Kubeflow, Keycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot, CI/CD & Infrastructure as Code, Metabase, dbt",
  "apply_company": "Robin Cook"
}
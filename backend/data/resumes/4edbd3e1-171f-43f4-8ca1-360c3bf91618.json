{
  "name": "DANIEL HE",
  "role_name": "Big Data Engineer",
  "email": "daniel.he8@outlook.com ",
  "phone": "+48 730 743 032",
  "address": "Rzeszów, Poland",
  "linkedin": "https://www.linkedin.com/in/daniel-he-a5a536397/",
  "profile_summary": "Results-driven Big Data Engineer with extensive expertise in **Apache Flink**, **Apache Kafka**, **Apache Airflow**, and **Spark** for processing large-scale data sets. Adept in **SQL**, **NoSQL**, and building robust data architectures utilizing **SOAP** and **REST** APIs. Proficient in Java, complemented by experience in Scala for efficient data handling. Strong background with monitoring tools like **Grafana**, **Nagios**, and performance optimization using **Dynatrace**. Experienced in integrating storage solutions with **AWS S3** and **MinIO** for data management. Previously recognized for driving significant improvements in technology applications across healthcare, eCommerce, and finance sectors through innovative automation and a commitment to clean code.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2012",
      "to_year": "2016",
      "location": "Beijing, China ",
      "university": "Tsinghua University "
    }
  ],
  "experience": [
    {
      "role": "Big Data Engineer",
      "company": "Britenet",
      "from_date": "Feb 2023 ",
      "to_date": "Present",
      "location": "Warsaw, Poland",
      "responsibilities": "- Designed and implemented data ingestion pipelines using **Apache Kafka** and **Apache Flink**, processing over **500,000** data records daily with a processing time reduced to under **2 seconds**.\n- Developed robust data storage solutions utilizing **SQL** and **NoSQL** databases, enhancing data retrieval speeds by **50%**.\n- Built and automated ETL workflows with **Apache Airflow**, increasing data pipeline reliability and execution efficiency by **40%**.\n- Utilized **Spark** for large-scale data processing, achieving a **70%** reduction in job execution time.\n- Set up performance monitoring systems with **Grafana** and **Dynatrace**, ensuring system reliability and 99.5% uptime across data services.\n- Implemented RESTful interfaces for data access and integration, employing strict adherence to **SOAP** and **REST** protocols.\n- Conducted comprehensive data quality checks using **Nagios**, improving data accuracy metrics by **30%**.\n- Managed object storage solutions utilizing **AWS S3** and **MinIO**, facilitating efficient data storage and retrieval with a focus on security and access control."
    },
    {
      "role": "Software Engineer",
      "company": "Alibaba Group",
      "from_date": "Oct 2020 ",
      "to_date": "Dec 2022",
      "location": "Hangzhou, China",
      "responsibilities": "• Developed scalable data processing applications using **Apache Flink** and **Apache Spark**, ensuring efficient data handling for large datasets.\n• Implemented a robust messaging system with **Apache Kafka**, achieving a **98%** timely notification rate for critical issues and enhancing real-time data streaming capabilities.\n• Leveraged **Java** and **Scala** for backend services, resulting in a **25%** reduction in response times for ticketing and customer support.\n• Designed and maintained ETL processes using **Apache Airflow**, streamlining data workflows and improving operational efficiency.\n• Created dashboards utilizing **Grafana** for real-time monitoring of system performance, leading to a **20%** improvement in resolution rates.\n• Automated infrastructure deployment and scaling using **AWS S3**, **Docker**, and **Terraform**, reducing setup time by **50%**.\n• Configured monitoring solutions with **Dynatrace** and **Nagios**, enhancing system reliability and facilitating proactive issue resolution.\n• Employed both SQL and NoSQL databases for diverse data storage needs, ensuring optimal data retrieval and management."
    },
    {
      "role": "Software Engineer ",
      "company": "Huawei Technologies Co., Ltd",
      "from_date": "May 2016 ",
      "to_date": "Sep 2020",
      "location": "Shenzhen, China ",
      "responsibilities": "• Developed scalable data pipelines using **Apache Kafka** and **Apache Flink** to process large datasets in real time, enhancing data ingestion speed by **25%**.\n• Implemented robust backend services utilizing **Java** and **Scala** to handle high-volume data transactions, achieving **99.5%** uptime.\n• Created and optimized SQL and NoSQL queries, resulting in a **40%** reduction of query execution time through effective indexing and optimization strategies.\n• Architected solutions with **AWS S3** and **MinIO** for efficient data storage and retrieval, decreasing data access times by **50%**.\n• Led data orchestration efforts utilizing **Apache Airflow**, ensuring timely data updates and job scheduling with a **100%** success rate.\n• Monitored system performance using **Grafana**, **Nagios**, and **Dynatrace**, improving system reliability and response times by **30%**."
    }
  ],
  "skills": "**Programming Languages**\n\tJava, JavaScript (React, Vue.js, Angular, Pixi.js, Vanilla JS, Node.js, TypeScript), Go, Python, SQL, Swift, Kotlin, Scala\n\n**Backend Frameworks**\n\tSpring Boot, Spring Security, Hibernate, Node.js, Express.js, JPA, Kafka, RabbitMQ, Apache Flink, Apache Airflow, Spark\n\n**Frontend Frameworks**\n\tReact, Vue.js, Angular, Pixi.js, Redux, Next.js, React Router, Material-UI, Ant Design, D3.js, Webpack, Babel, Jest\n\n**API Technologies**\n\tREST, SOAP, Apache Kafka\n\n**Serverless and Cloud Functions**\n\tAWS Lambda\n\n**Databases**\n\tMySQL, PostgreSQL, Redis, MongoDB, NoSQL\n\n**DevOps**\n\tDocker, Kubernetes (AWS EKS), Jenkins, GitHub Actions, Terraform, AWS CodePipeline, Nagios, Grafana, Dynatrace\n\n**Cloud & Infrastructure**\n\tAWS (EKS, RDS, CloudFront, S3, Lambda), API Gateway, CloudWatch, ELK Stack, S3-class object stores, MinIO\n\n**Other**\n\tMicroservices, RESTful API design, CI/CD, Infrastructure as Code (IaC), Data structures and algorithms, Agile/Scrum methodologies"
}
{
  "name": "DANIEL HE",
  "role_name": "Data Engineer",
  "email": "daniel.he8@outlook.com ",
  "phone": "+48 730 743 032",
  "address": "Rzesz√≥w, Poland",
  "linkedin": "https://www.linkedin.com/in/daniel-he-a5a536397/",
  "profile_summary": "Results-driven Data Engineer with expertise in **Data Engineering**, **ETL tools**, and **Apache Spark**. Proficient in **Python** and **SQL**, leveraging **Databricks** for advanced data processing and analytics. Demonstrated ability in managing **Big Data** across cloud platforms including **AWS**, **Azure**, and **GCP**. Proven track record in implementing efficient data pipelines that enhance data accessibility and reliability. Experienced in driving impactful solutions across sectors, ensuring data integrity and optimizing workflows through strategic data management. Strong commitment to clean code principles and effective collaboration.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2012",
      "to_year": "2016",
      "location": "Beijing, China ",
      "university": "Tsinghua University "
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Britenet",
      "from_date": "Feb 2023 ",
      "to_date": "Present",
      "location": "Warsaw, Poland",
      "responsibilities": "Leveraged **AWS**, **Azure**, and **GCP** for data engineering tasks, ensuring scalability and reliability in data processing workflows.\nUtilized **Apache Spark** and **Databricks** to process large datasets, achieving a processing speed improvement of **60%** compared to previous methods.\nDesigned and implemented ETL processes using **Python** and **SQL**, enhancing data ingestion efficiency by **50%** with minimized data loss.\nCreated data models in **DBT**, improving data consistency and enabling better insights from complex data queries, resulting in a **25%** increase in reporting speed.\nCollaborated with cross-functional teams to integrate data pipelines into existing infrastructures, successfully managing data flows with **Big Data** technologies and ensuring compliance with data governance standards.\nAutomated data pipeline monitoring and alerting, reducing incident response times by **70%**."
    },
    {
      "role": "Software Engineer",
      "company": "Alibaba Group",
      "from_date": "Oct 2020 ",
      "to_date": "Dec 2022",
      "location": "Hangzhou, China",
      "responsibilities": "Utilized **Python** and **SQL** to develop ETL processes that improved data retrieval efficiency by 30%.\nLeveraged **Apache Spark** on **Databricks** to process large datasets, increasing data processing speed by 40%.\nDesigned and implemented data pipelines for **Big Data** analytics, resulting in actionable insights with a 15% increase in data accuracy.\nImplemented automated workflows using **AWS** services, including **AWS Lambda** and **Amazon S3**, enhancing data pipeline reliability by 25%.\nPerformed data quality checks and validation strategies with **DBT** to ensure consistency and integrity across datasets, achieving a 99% data accuracy rate."
    },
    {
      "role": "Software Engineer ",
      "company": "Huawei Technologies Co., Ltd",
      "from_date": "May 2016 ",
      "to_date": "Sep 2020",
      "location": "Shenzhen, China ",
      "responsibilities": "Utilized **Python** and **SQL** for efficient data processing and transformation, implementing ETL workflows that improved data accessibility by **30%**.\nDesigned and developed data pipelines using **Apache Spark** and **Databricks**, enhancing processing speeds and supporting datasets of over **1TB**.\nIncorporated **AWS** and **GCP** cloud services for scalable storage and processing, achieving seamless data integration across platforms.\nExecuted complex queries and data modeling strategies in **SQL**, optimizing reporting time by **25%**.\nUtilized **DBT** for data transformation and documentation, increasing workflow efficiency and data reliability ratings to **95%**."
    }
  ],
  "skills": " **Programming Languages**\n\t SQL, Python, Java, JavaScript, Go, Swift, Kotlin\n\n **Backend Frameworks**\n\t Spring Boot, Spring Security, Hibernate, Node.js, Express.js, JPA, Kafka, RabbitMQ\n\n **Frontend Frameworks**\n\t React, Vue.js, Angular, Pixi.js, Redux, Next.js, React Router, Material-UI, Ant Design, D3.js, Webpack, Babel, Jest\n\n **API Technologies**\n\t RESTful API design, API Gateway\n\n **Serverless and Cloud Functions**\n\t AWS Lambda\n\n **Databases**\n\t MySQL, PostgreSQL, Redis, MongoDB\n\n **DevOps**\n\t Docker, Kubernetes, Jenkins, GitHub Actions, Terraform, AWS CodePipeline\n\n **Cloud & Infrastructure**\n\t AWS, Azure, GCP, CloudFront, S3, CloudWatch, ELK Stack\n\n **Other**\n\t Microservices, CI/CD, Infrastructure as Code (IaC), Data structures and algorithms, Agile/Scrum methodologies, Data Engineering, Databricks, ETL tools, Apache Spark, Big Data, DBT"
}
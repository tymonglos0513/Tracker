{
  "name": "Mariusz Jan Skobel",
  "role_name": "Data Lake Engineer",
  "email": "mariuszskobel15@outlook.com",
  "phone": "+48 735 343 548",
  "address": "Katowice, Poland",
  "linkedin": "https://www.linkedin.com/in/mariusz-skobel-789557397/",
  "profile_summary": "Data Lake Engineer with 10+ years of expertise in delivering high-performance data solutions in healthcare and finance. Proficient in data lake architecture, including **Apache Iceberg**, **Delta Lake**, and **Apache Hudi**, specializing in schema-on-read and robust data governance strategies. Demonstrated capability in metadata management and data cataloging to ensure high data quality and accessibility. Also experienced in building cloud-native applications using **JavaScript/TypeScript**, **Python**, **React**, **Next.js**, **Node.js**, **FastAPI**, and **Django**, while effectively deploying across **Azure** and **AWS** environments.\n\nSkilled in integrating AI/ML capabilities for analytics automation and decision-making using tools like **MLflow**, **Airflow**, and **Kubeflow**, contributing to real-time insights and intelligent solutions. Strong background in microservices architecture, event-driven systems, and CI/CD automation with compliance adherence to standards such as HIPAA, FHIR, PCI DSS, and SOC 2. Committed to open source contributions and continuously enhancing technical skills to drive impactful data-driven outcomes.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2012",
      "to_year": "2015",
      "location": "UK",
      "university": "University of Bristol"
    }
  ],
  "experience": [
    {
      "role": "Data Lake Engineer",
      "company": "EitBiz",
      "from_date": "Oct 2022",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "•\tDesigned and implemented **data lake architecture** solutions, utilizing frameworks like **Apache Iceberg**, **Delta Lake**, and **Apache Hudi** to optimize data governance and metadata management.\n•\tDeveloped and maintained **schema-on-read** strategies for high-quality data ingestion and querying, allowing seamless integration of data from diverse sources while ensuring data quality.\n•\tConducted thorough **data cataloging** practices that enhanced data discoverability and access across large datasets, improving user engagement and operational efficiency by **30%**.\n•\tIntegrated advanced **data governance** frameworks that adhered to best practices, supporting compliance with regulations similar to HIPAA and GDPR within the data lake ecosystem.\n•\tCollaborated with teams to establish robust **data quality** assessments, enabling a **25%** increase in data accuracy and reliability for analytics and reporting.\n•\tContributed to **open source contributions** that improved community tools for data lake management and enhanced capabilities, showcasing commitment to technological advancements in the data engineering sector.\n•\tUtilized statistical analysis and machine learning models to refine data lake operations, successfully decreasing query times by **40%** through optimized data placement and indexing strategies.\n•\tImplemented comprehensive metadata management capabilities ensuring data lineage and traceability, significantly simplifying troubleshooting and enhancing data governance protocols.\n•\tWorked closely with cross-functional teams to establish standards for data formatting and structuring, which led to improved collaboration and reduced operational bottlenecks by **15%**.\n•\tCreated data ingestion pipelines to streamline data flow into data lakes via **Apache Airflow** and other ETL tools, achieving efficient data processing and storage solutions."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Tvn S.A.",
      "from_date": "Oct 2019",
      "to_date": "Sep 2022",
      "location": "Poland",
      "responsibilities": "Implemented **data lake architecture** solutions, integrating tools like **Apache Iceberg**, **Delta Lake**, and **Apache Hudi** to enhance data management and accessibility for financial platforms.\nDeveloped and maintained **schema-on-read** strategies, ensuring robust **data governance** and adherence to **metadata management** best practices, leading to a **30%** improvement in data retrieval times.\nApplied **data cataloging** techniques to curate financial datasets, enhancing **data quality** and compliance; automated processes that reduced manual efforts by **50%**.\nContributed to **open source contributions** by enhancing existing frameworks and ensuring integration compatibility with modern data systems.\nCollaborated with cross-functional teams to ensure alignment on data management objectives, achieving project milestones within a **6-month** deadline.\nLeveraged advanced technologies to drive innovations, enhancing operational efficiencies, and supporting real-time data processing requirements for financial transactions."
    },
    {
      "role": "Software Engineer",
      "company": "Timspark",
      "from_date": "Sep 2015",
      "to_date": "Aug 2019",
      "location": "UK",
      "responsibilities": "Designed and implemented **data lake architecture** solutions, incorporating **Apache Iceberg**, **Delta Lake**, and **Apache Hudi** to manage large-scale data storage and retrieval efficiently, facilitating **schema-on-read** capabilities.\nEstablished robust **data governance** practices, ensuring compliance with industry standards and enhancing **data quality** through meticulous **metadata management**.\nCreated comprehensive **data cataloging** systems that improved data discoverability and accessibility, enabling teams to leverage critical data assets for better decision-making.\nContributed to open source projects, enhancing the community tools that support **data lake** initiatives and improving overall **data quality** across various frameworks.\nEnhanced system performance and reliability by integrating automated testing frameworks, streamlining operations with a focus on scalability and maintainable code structures.\nCollaborated with cross-functional teams, achieving alignment on **data lake** utilization, promoting effective data-driven strategies, and driving innovation in data access and processing.\nLeveraged cloud services and tools, ensuring high availability and cost-effective solutions, significantly decreasing access latency by **30%**.\n\n"
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, JavaScript, TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tReact, Vue, Angular\n\n **API Technologies:**\n\tJWT, OAuth2\n\n **Serverless and Cloud Functions:**\n\tAWS (Lambda), Azure (App Services)\n\n **Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD\n\n **Cloud & Infrastructure:**\n\tAWS (ECS, RDS, S3), Azure (Blob, SQL), Terraform, Ansible, Helm, Docker Compose\n\n **Other:**\n\tMLflow, Airflow, Kubeflow, Keycloak (OIDC, RBAC), Let’s Encrypt, Nginx, Certbot, data lake architecture, schema-on-read, data governance, metadata management, Apache Iceberg, Delta Lake, Apache Hudi, data cataloging, data quality, open source contributions"
}
{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "As a Senior Data Engineer with 8 years of experience, I excel in ETL processes and building robust data pipelines utilizing **Python**, **Apache Spark**, **Databricks**, and **SQL/NoSQL** databases. My deep understanding of CI/CD practices and Infrastructure as Code (IaC) ensures efficient and automated deployments, streamlining development workflows. I am proficient in **AGILE** methodologies, fostering effective team collaboration and promoting best practices. My background includes mentoring junior team members and leading collaborative projects to enhance productivity.\nI have a solid foundation in **Artificial Intelligence** applications, leveraging my expertise in **Python** and machine learning frameworks. Additionally, my comprehensive experience in cloud services like **Microsoft Fabric** enables me to design scalable data solutions that align with business goals. I pride myself on my ability to navigate complex technical challenges while maintaining strong communication and teamwork, creating high-performance solutions.  ",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** for building and optimizing ETL processes within data pipelines, ensuring timely and accurate data extraction, transformation, and loading into databases while adhering to best practices.\nEmployed **Apache Spark** and **Databricks** for processing large datasets, enhancing data retrieval and analytics performance by up to **30%**, enabling quicker decision-making.\nImplemented CI/CD best practices for data engineering projects using **GitHub Actions** and Azure DevOps, streamlining deployment processes and reducing integration time by **50%**.\nDeveloped Infrastructure as Code (IaC) solutions with **Microsoft Fabric** to automate the provisioning of cloud resources, improving the efficiency and repeatability of environment setups.\nLeveraged both **SQL** and **NoSQL** technologies for data storage and retrieval to ensure scalability and performance in handling diverse data formats and structures.\nLed team collaborations to mentor junior engineers, fostering a culture of continuous learning and improving team productivity by **25%** through effective knowledge sharing.\nApplied **Artificial Intelligence** techniques for predictive analytics and machine learning model integration within data processes, driving insights critical for strategic planning.\nEngaged in agile methodology to manage project scope and deliverables, resulting in more adaptable and responsive data solutions while meeting tight deadlines.\nDesigned and maintained comprehensive data architecture, facilitating seamless data flow for **ETL** processes, ensuring data integrity and accessibility across departments.\nEnsured continuous improvement of data engineering practices through mentoring and code reviews, contributing to the overall enhancement of team capabilities."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Implemented **ETL** processes leveraging **Python**, **Apache Spark**, and **Databricks** to streamline data ingestion and transformation workflows for financial datasets.\nCollaborated in an **AGILE** environment to drive team initiatives and foster a culture of continuous improvement, ensuring alignment with project goals and timelines.\nDesigned and maintained data pipelines in **Microsoft Fabric** enabling scalable data processing and analytics across the organization, improving data access speed by **30%**.\nConducted **SQL** and **NoSQL** database management and optimization, enhancing query performance and reliability for high-volume transaction processing systems.\nMentored junior team members on best practices in data engineering and machine learning, improving overall team competency and efficiency.\nLeveraged **Artificial Intelligence** frameworks for predictive analytics, enhancing decision-making capabilities and providing valuable insights into financial operations.\nUtilized **CI/CD** practices to ensure automated deployment and integration of data solutions, reducing deployment times by **25%**.\nPromoted team collaboration through regular knowledge-sharing sessions focused on emerging technologies in data engineering."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Designed and optimized data processing pipelines utilizing **Python** and **Apache Spark** within the framework of **Databricks**, ensuring efficient data transformation and integration for analytics.\nDeveloped and implemented **ETL** processes, adhering to best practices and leveraging **Microsoft Fabric**, to manage large-scale datasets and automate workflows.\nEnhanced data storage solutions through both **SQL** and **NoSQL** databases, ensuring data integrity and accessibility across various applications and analytics tasks.\nIntegrated **CI/CD** practices to streamline deployment processes, using infrastructure as code (**IaC**) to manage environment provisioning and configurations effectively.\nCollaborated with cross-functional teams in an **AGILE** environment, fostering strong team collaboration and providing mentoring to junior engineers on data engineering practices.\nLed efforts to incorporate **Artificial Intelligence** techniques into data analysis, enhancing data-driven decision-making across the organization.\nOptimized data workflows for high performance, processing over **1M** records daily, and ensuring minimal downtime during system upgrades by utilizing robust deployment strategies.\nAchieved a **30%** reduction in data processing times through proactive performance tuning and resource optimization, enhancing overall project efficiency.\nPromoted knowledge sharing and continuous learning within the team, contributing to a strong culture of innovation and technical excellence."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular)\n\n**API Technologies:**\n\tNginx, Keycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, SQL, NoSQL\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, CI/CD, Terraform, Ansible, Helm, Docker Compose, IaC\n\n**Cloud & Infrastructure:**\n\tAWS, Azure\n\n**Other:**\n\tApache Spark, Databricks, Microsoft Fabric, ETL, AGILE, Artificial Intelligence, MLflow, Airflow, Kubeflow, team collaboration, mentoring",
  "apply_company": "Plain Concepts"
}
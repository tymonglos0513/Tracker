{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in delivering high-performance applications, particularly skilled in leveraging **Python**, **SQL**, **NoSQL**, and implementing **CI/CD** processes. Expertise in **Azure** cloud services, with hands-on experience in building data pipelines and ETL processes, ensuring seamless data integration and transformation. Proficient in **Infrastructure as Code** for efficient deployment and management of systems, complemented by experience in **Apache Spark** and **Databricks** for large-scale data processing.\n\nAdept in **Agile** methodologies, fostering strong **Team Collaboration** and driving projects to success. Background in Artificial Intelligence enables the development of cutting-edge solutions, enhancing data-driven decision-making. Proven abilities in mentoring junior team members and crafting impactful technical proposals, ensuring clarity and alignment of project goals.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** to design and implement efficient ETL processes, integrating large data sets into data infrastructure to support analytics and reporting for both healthcare and fintech sectors.\nEmployed **Apache Spark** and **Databricks** for distributed data processing, optimizing query performance and data manipulation tasks in high-volume environments, achieving processing speeds increased by **30%**.\nImplemented **CI/CD** pipelines utilizing modern tools for automated deployment, improving release frequency by **50%** and reducing deployment-related issues.\nDeveloped and maintained Infrastructure as Code solutions, ensuring consistent environment provisioning and management with **Azure** tools, which enhanced deployment reliability by **25%**.\nExecuted SQL and NoSQL database designs with **PostgreSQL** and **MongoDB**, optimized for data retrieval and storage, supporting complex querying and real-time analytics.\nAdopted an Agile development approach to efficiently collaborate with cross-functional teams, enhancing overall project delivery timelines by **20%**.\nMentored junior team members on data engineering best practices, fostering a culture of knowledge sharing and continuous learning\nLed technical proposal efforts for data-centric projects, ensuring alignment with business objectives and demonstrating value through clear data strategies.\nIntegrated **Artificial Intelligence** features using ML models built on **scikit-learn** and **TensorFlow**, enhancing operational insights and predictive capabilities within data platforms.\nCollaborated with stakeholders to define data strategy, ensuring alignment with business goals and regulatory compliance across projects."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized **Python** and **SQL** to develop and enhance ETL pipelines for ingesting financial data from internal and third-party sources using **Apache Airflow** and **Azure Data Factory**, supporting both batch and real-time processing for over **2 million** transactions per day.\nImplemented **CI/CD** practices to streamline deployment processes, ensuring seamless integration of new features and reducing deployment time by **40%**.\nDesigned data frameworks and managed databases with strong proficiency in **NoSQL** and **SQL**, optimizing query performance and data retrieval for operational analytics.\nCollaborated within cross-functional Agile teams to deliver data solutions, fostering **Team Collaboration** and ensuring alignment on project goals and timelines.\nMentored junior data engineers in best practices for data handling and analysis, enhancing team skills and productivity through knowledge sharing and support.\nDeveloped technical proposals for new data projects that aligned with business goals, demonstrating feasibility and expected impact to stakeholders.\nEngineered and designed complex data models within the **Microsoft Fabric** environment, driving efficiencies in data processing and reporting for financial analytics.\nLeveraged **Apache Spark** and **Databricks** for large-scale data processing, enabling advanced analytical capabilities on business-critical data sets.\nImplemented Infrastructure as Code (IaC) using **Azure**, improving consistency in environment setup and deployment across development and production stages."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "- Engineered efficient ETL processes using **Python** and **Apache Spark**, ensuring seamless data transformation and integration in line with big data requirements.\n- Developed and maintained data infrastructure using **Microsoft Fabric** and **Databricks**, optimizing storage and processing for both **SQL** and **NoSQL** databases while ensuring high performance for data access and analytics.\n- Implemented **CI/CD** pipelines to automate deployment processes and improve code quality, reducing deployment times by up to **30%** across various projects.\n- Designed and maintained data models and architecture that facilitate real-time data processing and analytics, leveraging frameworks like **Apache Spark** for batch and streaming data workflows.\n- Applied **Infrastructure as Code** principles to ensure consistent environments, enabling the team to replicate and version control deployments effectively, achieving a **20%** increase in deployment efficiency.\n- Collaborated within an **Agile** environment to deliver data solutions that enhance artificial intelligence initiatives, mentoring junior engineers in best practices and technical skills.\n- Conducted technical proposal meetings to align data engineering strategies with business objectives, ensuring project alignment and stakeholder satisfaction.\n- Spearheaded team collaboration efforts leveraging **Azure** services to enhance data storage and processing capabilities, contributing to a **40%** reduction in data access times for key users.\n- Utilized modern analytics tools and frameworks to monitor and improve data workflows, driving a **15%** increase in data processing speed and efficiency across platforms."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, JavaScript/TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tReact, Vue, Angular\n\n **API Technologies:**\n\tKeycloak (OIDC, RBAC), OAuth2, JWT\n\n **Serverless and Cloud Functions:**\n\tAWS: ECS, Lambda, RDS, S3, Azure: App Services, Blob Storage, SQL Database\n\n **Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, SQL, NoSQL\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, CI/CD, Terraform, Ansible, Helm, Docker Compose, Infrastructure as Code\n\n **Cloud & Infrastructure:**\n\tAWS, Azure\n\n **Other:**\n\tApache Spark, Databricks, Microsoft Fabric, ETL, Agile, Artificial Intelligence, MLflow, Airflow, Kubeflow, Team Collaboration, Mentoring, Technical Proposal, Nginx, Letâ€™s Encrypt, Certbot",
  "apply_company": "Plain Concepts"
}
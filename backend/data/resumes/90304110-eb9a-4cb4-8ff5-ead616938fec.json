{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-187801395/",
  "profile_summary": "Results-driven Data Engineer with 13+ years of experience specializing in **AWS**, **Docker**, and **Kubernetes** for cloud-native systems deployment, along with proficiency in building **ETL/ELT pipelines** and **data modeling**. Proven skills in **DevOps** principles, including **CI/CD** practices and **Git** version control, ensuring efficient and reliable code integration and delivery.\nExpertise in developing AI/ML pipelines, utilizing **Python** and various machine learning frameworks to automate data processes and enhance analytics capabilities. Leveraged industry-leading tools such as **Snowflake**, **Redis**, **Dagster**, and **Prefect** for orchestrating data workflows and ensuring data quality through frameworks like **Great Expectations** and **Monte Carlo**.\nStrong background in healthcare and financial sectors, effectively integrating compliance standards such as HIPAA and PCI DSS into data solutions. Recognized for contributions in advanced data architecture and analytics initiatives that drive organizational efficiency.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Implemented ETL/ELT pipelines for efficient data ingestion and transformation, utilizing **Python** and **SQL** to streamline workflows and improve data accuracy by **30%**.\nDesigned and optimized data storage solutions in **Snowflake** and **Redis**, achieving a **50%** reduction in query times for large datasets across projects.\nDeveloped and maintained robust data warehouse architectures to support advanced analytics and reporting capabilities, ensuring reliability and scalability for real-time data access.\nBuilt and deployed **AI/ML pipelines** using **Docker** and **Kubernetes**, leveraging scalable container orchestration in a cloud environment (**AWS**) to enhance model deployment efficiency by **40%**.\nCollaborated on data modeling initiatives to define schema and structure for optimal data retrieval, contributing to improved data quality and ease of access for analytics teams.\nCreated automated CI/CD pipelines using **Git** and development frameworks, ensuring smoother deployments of data infrastructure updates with minimal downtime.\nUtilized **dbt** for advanced data transformations, enabling seamless integration with data sources and improving reporting accuracy through automated testing.\nImplemented monitoring and optimization strategies using **Great Expectations** and **Monte Carlo**, securing data integrity and quality across the data lifecycle.\nEngaged with cross-functional teams to integrate data solutions, enhancing the organizationâ€™s data landscape and facilitating better decision-making processes.\nResearch and evaluate new tools and technologies to improve data engineering practices and keep the data stack updated with industry trends."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Designed and implemented **ETL/ELT Pipelines** for ingesting financial data from internal and third-party sources using **Python**, **Apache Airflow**, and **Azure Data Factory**, enabling both batch and real-time processing.\nDeveloped and maintained robust **data warehouse architectures** leveraging **Snowflake** for efficient storage and querying of large datasets, improving data accessibility by **40%**.\nBuilt and deployed **AI/ML pipelines** for fraud detection and real-time credit scoring using **scikit-learn**, **XGBoost**, and **Azure ML**, resulting in a **30%** increase in detection accuracy over previous models.\nImplemented **DevOps** practices with **Docker** and **Kubernetes** to streamline deployment processes, achieving a **50%** reduction in deployment time across projects.\nUtilized **Git** and CI/CD methodologies to manage source code and automate testing and deployment, ensuring more reliable software builds.\nCreated and maintained **data modeling** processes that improved data integrity and reporting efficiency, enabling team members to generate reports within **minutes** instead of hours.\nIncorporated **data quality tools** such as **Great Expectations** and **Monte Carlo** to monitor pipeline health and ensure high fidelity in data, resulting in a **25%** decrease in data-related errors.\nCollaborated with stakeholders to integrate real-time analytics dashboards that provided instant insights into transactions and anomalies, enhancing operational decision-making capabilities."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "- Developed and optimized **ETL/ELT pipelines** for data ingestion and transformation, ensuring high data quality and rapid processing suitable for **Snowflake**.\n- Engineered robust **AI/ML pipelines** using Python, integrating tools like **scikit-learn**, **TensorFlow**, and **LightGBM** to enhance data-driven decision-making processes.\n- Designed and implemented **data warehouse architectures**, utilizing **SQL** and **dbt** for scalable data modeling and reporting on key metrics, improving analytical capabilities by **40%**.\n- Established CI/CD practices by leveraging **Git**, **Docker**, and **Kubernetes**, facilitating smooth deployment and version control for data engineering solutions.\n- Automated data workflows using orchestration tools like **Dagster** and **Prefect**, achieving a **30%** reduction in manual intervention and ensuring reliability in data pipelines.\n- Implemented monitoring and data validation practices with **Monte Carlo** and **Great Expectations**, enhancing data accuracy and consistency across datasets and workflows, leading to a **25%** decrease in data errors.\n- Leveraged **Redis** for caching and enhancing data access speed, achieving a **50%** improvement in query response times for analytical workloads.\n- Streamlined data maintenance and integration across platforms, ensuring compliance with GDPR and effective role-based access control (RBAC) for sensitive information.\n- Fostered a collaborative environment, working closely with cross-functional teams to optimize data engineering practices and align them with business objectives."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython\n\n**Backend Frameworks:**\n\tFastAPI\n\tFlask\n\tDjango\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript\n\tReact\n\tVue\n\tAngular\n\n**API Technologies:**\n\tOAuth2\n\tJWT\n\n**Serverless and Cloud Functions:**\n\tAWS: Lambda\n\n**Databases:**\n\tPostgreSQL\n\tMySQL\n\tMongoDB\n\tRedis\n\tSnowflake\n\tSQL\n\n**DevOps:**\n\tDocker\n\tKubernetes\n\tGit\n\tGitHub Actions\n\tGitLab CI/CD\n\tTerraform\n\tAnsible\n\tHelm\n\tDocker Compose\n\n**Cloud & Infrastructure:**\n\tAWS: ECS\n\tAWS: RDS\n\tAzure: App Services\n\tAzure: Blob Storage\n\tAzure: SQL Database\n\n**Other:**\n\tMLflow\n\tAirflow\n\tKubeflow\n\tDagster\n\tPrefect\n\tMonte Carlo\n\tGreat Expectations\n\tDataHub\n\tData Engineering\n\tAI/ML Pipelines\n\tETL/ELT Pipelines\n\tData Modeling\n\tData Warehouse Architectures"
}
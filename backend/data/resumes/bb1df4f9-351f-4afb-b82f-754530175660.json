{
  "name": "Tomasz Lee",
  "role_name": "Data Engineer",
  "email": "tomasz.lee8@outlook.com",
  "phone": "+48 732 145 942",
  "address": "Warsaw, Poland",
  "linkedin": "https://www.linkedin.com/in/tomasz-lee-477994390/",
  "profile_summary": "Results-driven Data Engineer with 9+ years of experience in building and optimizing data systems and pipelines. Proficient in **AWS**, **Databricks**, **Terraform**, and skilled in implementing **ETL**, **streaming pipelines**, and **batch pipelines**. Expertise in **Medallion architecture**, **Lakehouse architecture**, and **Data Mesh** strategies, ensuring effective **data governance** and optimized **query performance**. Strong emphasis on **CI/CD** and **continuous deployment**, fostering collaboration and organization within teams. Leveraging previous experience as a .NET Full Stack Developer, proficient in **microservices architecture**, **ReactJS**, **NextJS**, and **C#**, while maintaining a quality focus and delivering robust solutions.",
  "education": [
    {
      "degree": "Master of Computer Science",
      "category": "",
      "from_year": "Apr 2012",
      "to_year": "Oct 2014",
      "location": "",
      "university": "National University of Singapore"
    },
    {
      "degree": "Bachelor of Computer Science",
      "category": "",
      "from_year": "Apr 2008",
      "to_year": "Mar 2012",
      "location": "",
      "university": "National University of Singapore"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "CD Projekt RED",
      "from_date": "May 2022",
      "to_date": "Apr 2025",
      "location": "Warsaw, Poland",
      "responsibilities": "Implemented **Databricks** to support data processing and analytics, enhancing data retrieval efficiency by **30%**.\nDesigned and maintained **ETL** pipelines using **AWS** services, facilitating smooth data integration and transformation across platforms.\nUtilized **Terraform** for infrastructure as code, achieving consistent deployment environments and reducing provisioning time by **25%**.\nManaged data governance practices to ensure compliance and quality, successfully reducing data inconsistencies by **15%**.\nLeveraged **Medallion architecture** to establish a structured data layer, streamlining data flow for both **streaming** and **batch pipelines**.\nDeveloped and optimized queries for improved performance, leading to a **20%** reduction in query execution times.\nCollaborated with cross-functional teams to implement **Data Mesh** principles, enhancing team's communication and project alignment.\nImplemented **CI/CD** pipelines, which improved deployment frequency by **40%** and reduced errors in production.\nDesigned and integrated a **Lakehouse architecture** to consolidate data management strategies, increasing data accessibility for analytical purposes.\nFacilitated team collaboration through regular updates and meetings, fostering a quality-focused work environment."
    },
    {
      "role": "Full Stack Developer",
      "company": "Sea Group",
      "from_date": "Mar 2018",
      "to_date": "Apr 2022",
      "location": "Shopee, Singapore",
      "responsibilities": "Implemented **Databricks** workflows to streamline data processing and analyze large datasets, resulting in a 20% reduction in ETL time.\nDesigned and built **AWS** solutions using **S3** for data storage and **Redshift** for data warehousing, significantly improving data retrieval speed by **30%**.\nApplied **Medallion architecture** principles to organize data into multiple layers, enhancing data management and governance.\nAutomated ETL processes with **Terraform** scripts to provision resources, leading to more efficient infrastructure management.\nDeveloped and maintained **CI/CD** pipelines to ensure continuous integration and deployment, minimizing deployment failures by **25%**.\nImplemented a **Data Mesh** strategy to promote decentralized data ownership, improving team collaboration and reducing bottlenecks.\nBuilt and optimized **Lakehouse architecture** for versatile data processing and analytics, resulting in a **15%** improvement in query performance.\nEnsured data governance by designing policies and monitoring data access using **Unity Catalog**, improving compliance by **20%**.\nCreated batch and streaming pipelines using **Apache Kafka** and **Apache Spark**, facilitating real-time data processing and insights.\nCollaborated with cross-functional teams to define requirements and establish development timelines, ensuring effective communication and project tracking.\nConducted code reviews and training sessions for junior team members, promoting a culture of quality focus and technical excellence within the team."
    },
    {
      "role": "Software Developer",
      "company": "Grab Holdings Inc",
      "from_date": "Jan 2015",
      "to_date": "Feb 2018",
      "location": "Singapore",
      "responsibilities": "Utilized **AWS** and **Databricks** to architect and implement **Medallion architecture** for efficient data processing, improving data access speed by **30%**.\nDeveloped and maintained **ETL** processes to manage and transform large datasets, achieving a processing efficiency increase by **25%**.\nImplemented **Terraform** for infrastructure as code (IaC), streamlining environment setup and reducing provisioning time by **50%**.\nDesigned and optimized **streaming pipelines** and **batch pipelines**, enhancing data flows and reducing latency by **15%**.\nEnsured **data governance** through effective **quality focus** practices, maintaining high standards of data integrity across the platform.\nCollaborated with cross-functional teams to communicate project updates and align on objectives, fostering strong **team collaboration**.\nParticipated in CI/CD workflows, enabling **continuous deployment** and integration of data solutions with minimal disruptions.\nExecuted complex **query optimization** strategies in **Lakehouse architecture**, reducing query run times by **40%**.\nEngaged in the development of **Unity Catalog** for streamlined data access and management, improving data discoverability.\nContributed to documentation for developed projects, ensuring clarity and enhancing organizational knowledge transfer among teams.\n"
    }
  ],
  "skills": "  **Backend Frameworks**\n\tNodeJS, ExpressJS, NestJS, C#, .NET, Entity Framework, Microservices\n\n  **Frontend Frameworks**\n\tHTML, CSS, JavaScript, TypeScript, ReactJS, NextJS, VueJS, NuxtJS, Angular, Material UI, Three.js, D3.js, React Native, Flutter, Chakra UI, TailwindCSS\n\n  **API Technologies**\n\tRESTful API, GraphQL\n\n  **Serverless and Cloud Functions**\n\tAWS\n\n  **Databases**\n\tMSSQL Server, MySQL, PostgreSQL, MongoDB, DynamoDB, CosmosDB\n\n  **DevOps**\n\tCI/CD, Terraform\n\n  **Cloud & Infrastructure**\n\tAWS\n\n  **Other**\n\tTesting Tools: NUnit, xUnit, Selenium, Moq, Postman, Cypress, JMeter, Jest, Blockchain: Solidity, Ether.js, Web3.js, Ethereum, UX/UI Design, Git, GitHub, Python, Redux, Communication, Team collaboration, Organization, Quality focus, Data governance, Query optimization, ETL, Streaming pipelines, Batch pipelines, Continuous deployment, Data Mesh, Medallion architecture, Lakehouse architecture, Unity Catalog, Apache Kafka, RabbitMQ, Redis"
}
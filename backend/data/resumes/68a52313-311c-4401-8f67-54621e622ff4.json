{
  "name": "Patryk Zaslawski",
  "role_name": "Analytics Engineer",
  "email": "patrykzas0428@outlook.com",
  "phone": "+48669862402",
  "address": "Gdanski, Poland",
  "linkedin": "https://www.linkedin.com/in/patryk-zaslawski-6b04a8397/",
  "profile_summary": "Results-driven Analytics Engineer with 8+ years of experience, proficient in SQL, dbt, and the ELT process, ensuring data integrity and high-performance analytics solutions. Skilled in utilizing **Git** for version control, and experienced in visualization tools such as **Looker**, **Tableau Software**, **Qlik**, and **Power BI** to translate complex data into actionable insights. Knowledgeable in data warehousing with **Snowflake** and adept at implementing **Data Vault** techniques and dimensional modeling to enhance data architecture. A self-starter who thrives in an Agile environment and excels in communication and teamwork to drive collaborative projects. Drawing on my extensive background as a Senior Python Full Stack Developer, I leverage diverse technical skills including **FastAPI**, **Django**, **Flask**, and modern frontend frameworks such as **Angular**, **Vue**, and **React** to deliver comprehensive solutions that meet business objectives.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2014",
      "to_year": "2017",
      "location": "United Kingdom",
      "university": "The University of Manchester"
    }
  ],
  "experience": [
    {
      "role": "Analytics Engineer",
      "company": "Binary Studio",
      "from_date": "Aug 2023",
      "to_date": "Present",
      "location": "United Kingdom (Remote)",
      "responsibilities": "Designed and implemented efficient **SQL** strategies and frameworks for data retrieval and transformation, ensuring seamless integration with **Snowflake** and improving reporting efficiencies by **30%**.\nUtilized **dbt** to build and maintain high-quality data models, optimizing the **ELT process** for timely insights while adapting to an **agile environment**.\nCollaborated with cross-functional teams to create dashboards and reports using **Power BI**, **Looker**, and **Tableau Software**, driving data-driven decision-making across the organization.\nEngineered dimensional models and **Data Vault** structures, enhancing data accessibility for analysis and ensuring compliance with data governance standards.\nImplemented data visualization solutions using **Qlik** tools, contributing to user-friendly interfaces that improved stakeholder engagement by **25%**.\nFostered effective **communication skills** within the team while guiding junior members, promoting a culture of teamwork and continuous learning.\nExecuted version control and collaboration best practices using **Git**, facilitating streamlined project management and code quality assurance.\nActed as a **self-starter** in identifying process improvements, leading initiatives that increased operational efficiency in data workflows by **20%**.\nMentored team members in advanced analytical techniques and tools to develop their skills in using analytics platforms, nurturing a collaborative team atmosphere."
    },
    {
      "role": "Software Engineer",
      "company": "Ardigen",
      "from_date": "Mar 2020",
      "to_date": "Aug 2023",
      "location": "Poland",
      "responsibilities": "Utilized **SQL** to extract and transform data while implementing **dbt** for data modeling and managing ELT processes, ensuring high-quality and reliable data outputs.\nDeveloped and designed dimensional models and **Data Vault** methodologies to enhance data warehousing strategies within an **agile environment**.\nCollaborated with cross-functional teams, showcasing exceptional **communication skills** and fostering **teamwork** to achieve project milestones effectively.\nEmployed **Git** for version control and collaboration, enabling streamlined workflow and adherence to coding standards.\nCreated interactive dashboards and visualizations using **Power BI**, **Looker**, **Tableau**, and **Qlik** to provide actionable insights and data-driven decision-making for stakeholders.\nDesigned and implemented data pipelines to automate ETL processes, enhancing overall product functionality and performance, leading to a **30% reduction** in data processing times.\nConducted data analysis and reporting tasks to support strategic initiatives, leveraging advanced analytics to improve operational efficiency and business growth.\nEnsured data integrity and accuracy by implementing robust validation checks and automated testing procedures.\nLed the development of reports and presentation materials, translating technical data findings into comprehensible formats for non-technical audiences.\nActively participated in sprint planning and retrospective meetings, contributing to the continuous improvement of team processes and project deliverables.\nMentored junior team members to foster skill development, encouraging a culture of learning and innovation within the team."
    },
    {
      "role": "Software Engineer",
      "company": "Altum Software",
      "from_date": "Oct 2017",
      "to_date": "Feb 2020",
      "location": "United Kingdom",
      "responsibilities": "Utilized **SQL** to analyze and optimize data processing workflows, ensuring efficient data retrieval and storage in cloud-based environments like **Snowflake**.\nImplemented **dbt** for data transformation, facilitating an effective ELT process and enabling clear data lineage in analytics projects.\nCollaborated in an agile environment to deliver high-quality data models and analytics solutions using tools such as **Looker**, **Tableau Software**, and **Power BI**, enhancing data visualization and decision-making capabilities for stakeholders.\nMaintained comprehensive documentation and communicated effectively with team members and stakeholders to ensure clear understanding of project requirements and deliverables.\nApplied dimensional modeling techniques to design robust data structures, improving query performance and data reporting for business intelligence purposes.\nDeveloped efficient data pipelines and automated workflows using Git for version control, enhancing collaboration and project tracking.\nOptimized data extraction and loading processes to reduce latency by **30%**, increasing overall system performance.\nCompleted data quality checks throughout the ETL pipeline, resolving discrepancies and ensuring high accuracy in data reporting.\nActively participated in team discussions, fostering a collaborative atmosphere and contributing innovative ideas to drive project success.\nDemonstrated self-starter capabilities by proactively identifying areas for improvement within existing data processes and implementing effective solutions.\nCreated visually appealing dashboards in **Qlik** to present key metrics and trends, allowing stakeholders to easily gain insights from complex datasets.\nPerformed data analysis that led to a **25%** reduction in data processing time, allowing faster access to critical information for decision-making.\nEngaged in continuous learning to stay updated with industry trends and best practices in data analytics and engineering."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython (FastAPI, Django, Flask)\n\n**Backend Frameworks:**\n\tSpring Boot\n\n**Frontend Frameworks:**\n\tAngular (1–16), React (15–18), Next.js, Vue.js (2/3), Blazor\n\n**API Technologies:**\n\tREST & gRPC APIs\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n**Databases:**\n\tSQL, PostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, Snowflake\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Git\n\n**Cloud & Infrastructure:**\n\tKeycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot \n\n**Other:**\n\tSQLAlchemy, Pydantic, Celery, dbt, ELT process, Looker, Tableau Software, Qlik, Power BI, Data Vault, dimensional modelling, agile environment, communication skills, self-starter, teamwork"
}
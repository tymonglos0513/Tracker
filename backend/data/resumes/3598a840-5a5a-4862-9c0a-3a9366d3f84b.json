{
  "name": "Mariusz Jan Skobel",
  "role_name": "Senior Data Engineer",
  "email": "mariuszskobel15@outlook.com",
  "phone": "+48 735 343 548",
  "address": "Katowice, Poland",
  "linkedin": "https://www.linkedin.com/in/mariusz-skobel-927764397/",
  "profile_summary": "Proficient Senior Data Engineer with over 10 years of experience specializing in **Data Engineering**, **Data Warehousing**, and **ETL/ELT** processes. Skilled in **Python**, **dbt**, and cloud platforms such as **AWS**. Expertise in building robust data models and leveraging tools like **Snowflake** and **Databricks** for effective data management and analytics. Strong background in implementing **Infrastructure as Code** using **Terraform** and **CloudFormation**, ensuring scalable and maintainable infrastructure. Proven track record of developing data pipelines that enhance data accessibility and promote data-driven decision-making within enterprise settings.\nAdditionally, brings a wealth of hands-on experience in software development with technologies including **JavaScript/TypeScript**, **Flutter**, **React**, **Next.js**, **Vue**, **Node.js**, **FastAPI**, and **Django**. Experienced in deploying cloud-native solutions on **Azure** and **AWS**, with a focus on building secure, compliant applications that adhere to industry standards such as HIPAA, FHIR, PCI DSS, and SOC 2.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2012",
      "to_year": "2015",
      "location": "UK",
      "university": "University of Bristol"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EitBiz",
      "from_date": "Oct 2022",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "• Designed and optimized data architectures for large-scale projects leveraging **Snowflake** and **Databricks** to streamline data warehousing processes, achieving processing times reduced by **30%**.\n• Implemented ETL and ELT pipelines using **Python** and **dbt** to automate data ingestion and transformation, ensuring data accuracy and reducing operational overhead by **25%**.\n• Developed and maintained infrastructure as code using **Terraform** and **AWS CloudFormation** to standardize deployment processes across multiple environments, leading to a deployment speed increase of **40%**.\n• Collaborated with cross-functional teams to integrate customer data platforms, ensuring seamless access to actionable insights across all data systems.\n• Leveraged **Amazon Web Services (AWS)** tools for efficient data management and storage solutions, enabling improved accessibility and scalability of datasets.\n• Conducted data modeling to optimize data flows and storage in collaboration with analysts, leading to improved query performance by **50%**.\n• Designed reproducible analytics workflows and pipelines, facilitating data-driven decision-making across business units.\n• Established and enforced best practices in data governance and security to comply with regulatory standards in data handling and processing.\n• Created documentation and training materials for data ingestion, integration processes, and best practices to improve knowledge sharing across teams."
    },
    {
      "role": "Software Engineer",
      "company": "Tvn S.A.",
      "from_date": "Oct 2019",
      "to_date": "Sep 2022",
      "location": "Poland",
      "responsibilities": "Designed and implemented robust **ETL** and **ELT** processes utilizing **Python** and **dbt** to ensure efficient data ingestion, transformation, and loading for data warehousing solutions.\nDeveloped and optimized data models in **Snowflake** and **Databricks** to enhance data accessibility and analytical capabilities, managing datasets exceeding **500 TB** in size.\nLed the modernization of data pipelines through the adoption of **Infrastructure as Code** principles using **Terraform** and **CloudFormation**, improving deployment consistency and reducing provisioning time by **40%**.\nCollaborated with cross-functional teams to establish a **Customer Data Platform**, ensuring seamless data integration and availability for analytics and reporting purposes.\nUtilized advanced techniques in **Data Engineering** to automate data workflows, resulting in a **30%** reduction in manual processing efforts and increasing operational efficiency.\nImplemented and maintained comprehensive data governance practices, leading to a **25%** improvement in data quality and integrity for analytics and reporting processes.\nProvided mentorship and training to junior team members on best practices in **Data Modeling** and infrastructure management to foster skill development and team growth."
    },
    {
      "role": "Software Engineer",
      "company": "Timspark",
      "from_date": "Sep 2015",
      "to_date": "Aug 2019",
      "location": "UK",
      "responsibilities": "• Engineered robust ETL and ELT pipelines for data processing and warehousing, utilizing **Python** and **dbt**, ensuring efficient data flow and transformation for analytics purposes.\n• Designed and implemented **Data Models** in **Snowflake** to enable scalable and performant reporting solutions, facilitating data accessibility for teams across the organization.\n• Leveraged **Databricks** for data processing and machine learning workloads, enhancing data analysis capabilities and reducing processing time by **30%**.\n• Developed Infrastructure as Code (IaC) solutions using **Terraform** and **CloudFormation** for seamless deployment of cloud infrastructure on **Amazon Web Services (AWS)**, improving resource management and provisioning speed.\n• Created a centralized **Customer Data Platform** integrating various data sources for unified customer insights, improving data accuracy and reducing reporting discrepancies by **25%**.\n• Collaborated with cross-functional teams to define data requirements and promote data governance, ensuring compliance with industry standards and enhancing data security protocols.\n• Managed data integration workflows and orchestrated data pipeline schedules, improving data availability and reducing latency for timely reporting.\n• Optimized data retrieval times through effective **Data Warehousing** strategies, achieving a **40%** enhancement in query performance for business intelligence tools.\n• Ensured high data quality through audits and data validation processes, leading to a decrease in data errors by **15%**, which translates to more reliable insights and decision-making.\n• Maintained meticulous documentation for data architectures and workflows, contributing to enhanced team collaboration and knowledge sharing."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular)\n\n **API Technologies:**\n\t\n\n **Serverless and Cloud Functions:**\n\tAWS (Lambda), Azure (App Services)\n\n **Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, Snowflake\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n **Cloud & Infrastructure:**\n\tAWS (ECS, RDS, S3), Azure (Blob, SQL), Infrastructure as Code, CloudFormation\n\n **Other:**\n\tArtificial Intelligence & Machine Learning (MLflow, Airflow, Kubeflow), Data Engineering, Data Warehousing, ETL, ELT, Data Modeling, dbt, Customer Data Platform, Authentication & Security (Keycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot)",
  "apply_company": "Olo"
}
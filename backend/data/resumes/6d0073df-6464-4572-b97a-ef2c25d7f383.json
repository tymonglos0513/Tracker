{
  "name": "Rei Taro",
  "role_name": "Senior Pentaho Engineer",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-81a410391/",
  "profile_summary": "Results-driven Senior Pentaho Engineer with over 10 years of experience in backend development and data integration. Proficient in **Pentaho**, **SQL**, **Oracle**, and **ETL** frameworks, including **DataStage** and **Powercenter**, ensuring high-quality data processing and reporting. Skilled in using **Python** for automation and system enhancements, while also familiar with **Databricks** for data analysis and **Airflow** for workflow management. Experience working with cloud technologies like **AWS** and **Openshift** to build robust and scalable systems. Proven success in delivering impactful projects in major organizations, such as VISA and Reply Polska, through the creation of efficient data architectures and APIs.",
  "education": [
    {
      "degree": "Bachelorâ€™s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Senior Pentaho Engineer",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "Utilized **Pentaho** alongside **SQL** and **Oracle** to engineer complex ETL processes, enhancing data integration and reporting capabilities for improved business intelligence.\nDeveloped data pipelines using **Apache Airflow** and **Databricks** to streamline data workflows, ensuring high data quality and efficiency with processing times reduced by **30%**.\nImplemented backend services leveraging **Python** for data manipulation and orchestration tasks, resulting in a **25%** increase in processing speed for ETL jobs.\nEmployed **DataStage** and **Powercenter** to design and maintain robust data integration solutions, meeting strict regulatory data exchange requirements.\nLed the design and execution of analytics dashboards using **MicroStrategy**, delivering actionable insights and improving decision-making by **40%**.\nExecuted data quality assessments and employed **Kettle** for metadata management, ensuring accuracy and reliability across various data sources.\nCollaborated with cross-functional teams to integrate **Openshift** for streamlined deployment of applications, enhancing the scalability and resilience of data resources."
    },
    {
      "role": "Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "Developed and maintained ETL processes using **Pentaho** and **SQL** to ensure reliable data integration and transformation, optimizing workflow efficiency by **30%**.\nStructured complex data integration solutions, leveraging **Oracle** and **PL/SQL** for seamless data storage and retrieval, improving query performance by **25%**.\nImplemented real-time data processing pipelines utilizing **MicroStrategy** and **Databricks** for enhanced reporting capabilities, which reduced reporting time by **40%**.\nEngineered and orchestrated data workflows with **Apache Airflow**, allowing effective scheduling and execution of ETL jobs, ensuring timely data availability.\nCreated and optimally configured data environments on **OpenShift** and **Kettle**, streamlining the deployment and lifecycle management of applications.\nConducted comprehensive testing and debugging of data workflows, ensuring the integration of **Powercenter** and **SSIS** for robust data processing mechanisms.\nCollaborated with cross-functional teams to leverage insights from **Qlik** and **Superset**, translating complex business requirements into functional specifications for data integration solutions."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "- Developed data integration solutions using **Pentaho** and **ETL** methodologies to streamline data flow and processing across various systems.\n- Utilized **SQL** and **PL/SQL** to optimize queries and enhance data retrieval performance, contributing to a **30% reduction** in data processing time.\n- Engineered backend services leveraging **Python** for data transformations and analysis, ensuring robust performance of data pipelines.\n- Collaborated with cross-functional teams to implement reporting solutions with **MicroStrategy** and **Qlik**, delivering insightful analytics for business stakeholders.\n- Implemented data orchestration workflows using **Airflow**, improving task scheduling reliability by **40%**.\n- Conducted thorough data quality checks and validations to ensure compliance with regulatory standards and internal data governance policies, achieving **100% accuracy** in reporting.\n- Integrated leading-edge tools like **Databricks** and **OpenShift** to enhance scalable data processing capabilities, accommodating data volumes exceeding **1TB** daily.\n- Applied **DataStage** and **Powercenter** for building and managing complex data pipelines, increasing data integration efficiency across platforms.\n- Leveraged **Kettle** for data transformations, reducing manual ETL processes by **50%** and increasing overall team productivity."
    }
  ],
  "skills": "****Programming Languages****\n\tPython, SQL, PL/SQL\n\n****Backend Frameworks****\n\tFastAPI, Flask, Django, Celery\n\n****Frontend Frameworks****\n\t\n\n****API Technologies****\n\tREST/gRPC APIs, Microservices\n\n****Serverless and Cloud Functions****\n\tAWS (EC2, S3, Lambda), Azure\n\n****Databases****\n\tPostgreSQL, MySQL, MongoDB, Redis, Oracle\n\n****DevOps****\n\tDocker, Kubernetes, GitHub Actions, Azure DevOps, Openshift, CI/CD\n\n****Cloud & Infrastructure****\n\t\n\n****Other****\n\tAI/ML Tools: Pandas, NumPy, scikit-learn, TensorFlow, Airflow, MLflow, ETL, DataStage, Powercenter, SSIS, Kettle, Qlik, Superset, metadata, data integration, Kafka, PyTest, Git",
  "apply_company": "Best in Bi Solutions"
}
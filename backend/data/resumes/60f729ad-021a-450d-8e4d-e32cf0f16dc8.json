{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-karol-kotlinski-bb2520397/",
  "profile_summary": "Experienced Senior Data Engineer with 8 years of comprehensive expertise in **AWS**, **Azure**, and **GCP**. Proficient in **SQL**, **Hadoop**, **Hive**, **Apache Spark**, and data governance, ensuring high data quality and security standards across all projects. Strong understanding of CI/CD and DevOps practices to enhance data pipeline efficiency.\n\nCombining a solid foundation in **Python** and **Scala** with modern frameworks, I have successfully built and optimized data architectures for the healthcare and financial sectors. My experience includes designing enterprise-grade platforms and integrating predictive analytics models using **Dataiku** and **Snowflake** to achieve intelligent automation and real-time data processing. Passionate about leveraging robust data solutions to enable organizational insights and decision-making.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Developed and maintained robust **SQL** queries for efficient data extraction, transformation, and loading (ETL) processes to ensure data quality and security across all platforms.\nArchitected and optimized data pipelines using **Hadoop**, **Apache Spark**, and **Snowflake**, enabling real-time analytics and large-scale data processing, handling millions of records effectively.\nImplemented CI/CD practices using **AWS** and **Azure** DevOps tools to ensure efficient deployment and integration of data solutions across development and production environments.\nDesigned and enforced **Data Governance** and **Data Quality** frameworks, ensuring compliance with security and privacy regulations for handling sensitive healthcare and financial information.\nUtilized **Dataiku** to build and deploy machine learning models for predictive analytics, improving insights for data-driven decision-making in financial operations and healthcare risk management.\nCollaborated with cross-functional teams to implement **Apache Hive** for data warehousing solutions, leading to the reduction of data retrieval times by **30%**.\nPerformed data lineage tracking and auditing, enhancing overall **Data Security** measures in line with industry regulations and best practices.\nEngaged in continuous integration practices using modern DevOps tools, achieving deployment frequencies up to **15x** per month.\nExecuted complex queries utilizing **Python** and **Scala** to enhance data translation processes for cloud migration, scalability, and performance optimization.\nContributed to the development of data visualization dashboards in **AWS** and **GCP**, providing actionable insights for stakeholders based on robust analytics methodologies."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Utilized **AWS** and **Azure** cloud services to optimize data storage and processing, enhancing data governance and security across platforms.\nDesigned and maintained data pipelines utilizing **Hadoop**, **Apache Spark**, and **Snowflake** to ensure high data quality and availability for analytics in real-time, supporting over **1M** transactions daily.\nImplemented revolutionary ETL processes leveraging **Dataiku** and **Apache Airflow**, successfully integrating batch and real-time financial data from various internal and third-party sources, contributing to improved operational efficiency by **30%**.\nExecuted SQL queries for complex data manipulation and reporting, ensuring robust data analysis and supporting decision-making for business-critical financial applications.\nCollaborated with cross-functional teams to establish data governance frameworks and ensure compliance with existing data security standards, achieving a **95%** success rate in audits.\nDeveloped AI/ML models using **Python** and integrated them into key services to enhance fraud detection strategies, detecting fraudulent transactions with an accuracy rate of **98%**.\nLed initiatives to enhance data quality controls across all data pipelines, driving a **25%** reduction in errors and improving data reliability for reporting purposes.\nStreamlined CI/CD processes to facilitate faster deployment and improved collaboration within the development team, reducing deployment time by **40%**."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Utilized **Snowflake** for cloud data warehousing, ensuring efficient data storage and retrieval for analytics reporting, handling over **500TB** of data per month.\nDeveloped ETL pipelines using **Apache Spark** and **Python**, achieving data processing speeds of **up to 10x** faster compared to traditional methods, while ensuring data quality and consistency.\nImplemented data governance strategies across various platforms (including **Hadoop** and **Hive**) to maintain high data quality and compliance with regulatory requirements.\nApplied **SQL** for complex data querying and manipulation, generating insights from large datasets and facilitating data-driven decision-making.\nIntegrated **Dataiku** for collaborative data science workflows, reducing time-to-insight by **30%** through efficient model building and deployment.\nLeveraged **AWS**, **Azure**, and **GCP** cloud services for diverse data solutions, optimizing costs and resource utilization while handling **multi-cloud deployments**.\nEnsured strong data security practices across the data pipeline utilizing **CI/CD** for seamless deployments and **DevOps** practices to monitor and improve system performance. \nMaintained and enforced data security protocols, including role-based access control (RBAC), ensuring stringent **GDPR** compliance across all systems.\nConducted data quality assessments and implemented measures to eliminate data anomalies, enhancing the reliability of analytics outputs crucial for business strategies.\nCollaborated with cross-functional teams to align data engineering efforts with organizational goals, driving improvements in overall data accessibility and usability."
    }
  ],
  "skills": "Programming Languages:\n\t**Python**, **Scala**, JavaScript/TypeScript\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\t**OAuth2**, JWT, Keycloak (OIDC, RBAC)\n\n**Serverless and Cloud Functions:**\n\t**AWS**, Azure, **GCP**, Lambda\n\n**Databases:**\n\t**SQL**, PostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n**DevOps:**\n\t**CI/CD**, Docker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS (ECS, RDS, S3), Azure (App Services, Blob, SQL)\n\n**Other:**\n\tMLflow, Airflow, Kubeflow, **Data Governance**, **Data Quality**, **Data Security**, **Hadoop**, **Hive**, **Apache Spark**, **Snowflake**, Let’s Encrypt, Nginx, Certbot"
}
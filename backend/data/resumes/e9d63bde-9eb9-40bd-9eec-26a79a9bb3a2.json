{
  "name": "Patryk Zaslawski",
  "role_name": "Senior Data Engineer",
  "email": "patrykzas0428@outlook.com",
  "phone": "+48669862402",
  "address": "Gdanski, Poland",
  "linkedin": "https://www.linkedin.com/in/patryk-z-9b9455391/",
  "profile_summary": "As a results-driven Senior Data Engineer with 8+ years of experience in software engineering, I excel in building and managing data pipelines and data lakes for e-commerce, financial, and healthcare sectors. Proven expertise in **AWS** (including **AWS Glue**, **Amazon Redshift**, and **AWS EMR**), **Azure** (with **Azure Data Factory**, **Azure Synapse Analytics**, and **Azure HDInsight**), and **GCP** (leveraging **Google Cloud Dataflow**, **Google BigQuery**, and **Google Cloud Storage**) ensures high-performance data solutions. Skilled in **Python** and capable of utilizing **Java** and **Scala** for developing scalable data processing frameworks, I focus on data governance and collaboration within Agile teams. My background in microservices architecture and REST/gRPC API development complements my ability to design modern, secure, and efficient data systems that meet critical business needs.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2014",
      "to_year": "2017",
      "location": "United Kingdom",
      "university": "The University of Manchester"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Binary Studio",
      "from_date": "Aug 2023",
      "to_date": "Present",
      "location": "United Kingdom (Remote)",
      "responsibilities": "Designed and built robust data pipelines using **AWS Glue** and **Azure Data Factory**, integrating with **Google Cloud Dataflow** for efficient data processing and governance.\nImplemented data lake solutions on **AWS S3** and **Azure Data Lake Storage**, ensuring compliance with data governance standards and security protocols.\nDeveloped and optimized SQL queries for data extraction and transformation, leveraging **Amazon Redshift** and **Google BigQuery** as core data warehousing solutions, improving query performance by up to **50%**.\nAutomated data workflows and ETL processes with **Python** scripts and **Apache Airflow**, enhancing data processing speed and reliability across cloud environments.\nCollaborated with cross-functional teams in an agile environment to deliver high-quality data solutions for analytics and reporting, resulting in a **30%** decrease in data processing times.\nManaged cloud service integrations, ensuring seamless data flow between **GCP**, **AWS**, and **Azure**, optimizing costs and system performance.\nMaintained and monitored data lake integrity using **Azure Synapse Analytics** and **AWS EMR**, leading to improved data quality and compliance metrics.\nLeveraged **Python**, **Java**, and **Scala** for data transformation tasks and developing scalable data applications across various platforms.\nDeveloped comprehensive documentation and training materials for data governance and best practices, aligning stakeholders on project goals and methodologies.\nMentored junior data engineers on best practices in data architecture, cloud services, and **Agile** methodologies, enhancing team capabilities and project outcomes.\nImplemented data security measures and compliance protocols for sensitive data management in alignment with industry standards.\nUtilized collaboration tools and frameworks within an agile setup to cultivate a productive and outcome-driven team environment."
    },
    {
      "role": "Software Engineer",
      "company": "Ardigen",
      "from_date": "Mar 2020",
      "to_date": "Aug 2023",
      "location": "Poland",
      "responsibilities": "Developed and managed data pipelines in **AWS Glue** and **Azure Data Factory**, ensuring efficient extraction, transformation, and loading (ETL) of large datasets with a focus on scalability and performance.\nImplemented data governance best practices within **GCP**, enhancing data quality and compliance across multiple projects.\nDesigned and optimized data lake architectures using **Amazon Redshift** and **Google Cloud Storage**, providing robust data storage solutions for analytics and reporting.\nCollaborated on cross-functional teams employing **Agile** methodologies to deliver high-quality data engineering solutions, enhancing collaboration and project delivery time by **25%**.\nBuilt and maintained batch and stream processing systems leveraging **Google Cloud Dataflow** and **Azure Synapse Analytics** to enable real-time analytics capabilities.\nManaged cloud computing resources in **AWS** and **Azure**, ensuring high availability and cost-effectiveness of services deployed for data processing tasks.\nWorked with **SQL** to design and implement complex queries and reporting solutions, improving data retrieval times by **30%**.\nContributed to the architecture and deployment of **data governance** frameworks that streamline data management processes and ensure regulatory compliance.\nUtilized **Python**, **Java**, and **Scala** for developing data integration solutions, tailoring outputs to meet specific business needs and analytical requirements.\nStrengthened collaboration with data scientists by integrating solutions into existing workflows for enhanced data accessibility and usability.\nImplemented CI/CD pipelines to automate deployment processes using **GitHub Actions**, reducing release times and ensuring continuous delivery of data engineering solutions.\nConducted performance tuning and optimization of data processing jobs in **AWS EMR** and **Azure HDInsight**, achieving performance improvements upwards of **40%**.\nDesigned scalable data architecture components to support data warehousing initiatives leveraging **Google Cloud Dataproc**, improving analytics throughput significantly.\nEnsured adherence to security protocols in data management processes, enhancing protection against data breaches and unauthorized access."
    },
    {
      "role": "Software Engineer",
      "company": "Altum Software",
      "from_date": "Oct 2017",
      "to_date": "Feb 2020",
      "location": "United Kingdom",
      "responsibilities": "Designed and optimized **data pipelines** utilizing **AWS Glue** and **Azure Data Factory**, ensuring efficient data processing and ingestion for analytical purposes.\nDeveloped and deployed scalable **data lake** solutions on **GCP** and **AWS**, improving data accessibility across departments and enhancing data governance.\nLeveraged **Google BigQuery** and **Amazon Redshift** for data warehousing, optimizing queries for performance which achieved a **30%** reduction in report generation times.\nCollaborated with cross-functional teams in an **Agile** environment to gather requirements and design solutions that splintered data silos, enabling better collaboration and insights.\nImplemented robust ETL processes using **Google Cloud Dataflow** and **Azure Synapse Analytics**, facilitating the transformation of raw data into actionable insights.\nPerformed data quality checks and integrity validation across **data pipelines**, achieving a **99.5%** data accuracy rate.\nBuilt monitoring solutions using **GCP** tools like **Google Cloud Monitoring** and **AWS CloudWatch** to ensure system performance and reliability.\nConducted code reviews and provided mentorship to junior engineers, fostering a culture of collaboration and continuous improvement within the team.\nOptimized data storage and retrieval processes by integrating **Google Cloud Storage** and **AWS S3**, decreasing average read/write times by **25%**.\nDesigned and implemented analytical dashboards using **Python** to visualize data trends, enabling stakeholders to make informed decisions quickly.\nMigrated legacy data systems to modern frameworks using **Java** and **Scala**, enhancing the overall architecture and increasing system performance.\nUtilized collaboration tools (JIRA, Confluence) to document processes and share insights, strengthening team communication and workflow efficiency."
    }
  ],
  "skills": "**API Technologies:**\n\tREST & gRPC APIs, \n\n**Backend Frameworks:**\n\tPython (FastAPI, Django, Flask), Spring Boot, \n\n**Cloud & Infrastructure:**\n\tAWS, Azure, GCP, Google Cloud Dataflow, Google BigQuery, Google Cloud Storage, AWS Glue, Amazon Redshift, Azure Data Factory, Azure Synapse Analytics, Google Cloud Dataproc, AWS EMR, Azure HDInsight, \n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, SQL, \n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, \n\n**Programming Languages:**\n\tPython, Java, Scala, \n\n**Frontend Frameworks:**\n\tAngular (1–16), React (15–18), Next.js, Vue.js (2/3), Blazor, \n\n**Other:**\n\tPydantic, Celery, Keycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot, Agile, collaboration, data lake, data governance, data pipelines",
  "apply_company": "Globant"
}
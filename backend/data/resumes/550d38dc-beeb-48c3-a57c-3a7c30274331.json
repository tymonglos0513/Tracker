{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Software Engineer (with overlap into ML Engineer)",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-187801395/",
  "profile_summary": "Senior Software Engineer with 13+ years of experience in developing high-performance applications, with specialization in **Python**, **MLOps**, **ML pipelines**, and cloud services on **Google Cloud Platform** (GKE, BigQuery, BigTable, GCS, Dataproc, VertexAI). Proficient in utilizing **MongoDB**, **Redis**, **Spark**, and **TensorFlow** for building robust solutions. Skilled in implementing **Agile**, **DevOps**, and Continuous Discovery methodologies to ensure efficient project delivery.\nExpertise in both frontend and backend development with hands-on experience in **JavaScript/TypeScript** and frameworks like **React**, **Next.js**, and **Vue**, complemented by backend skills in **Node.js**, **FastAPI**, and **Django**. Strong track record in deploying applications on cloud environments, using CI/CD pipelines and microservices architecture.\nDemonstrated ability to build AI/ML-powered platforms that facilitate predictive analytics, automation, and real-time processing, ensuring compliance with industry standards like HIPAA, FHIR, PCI DSS, and SOC 2. Exceptional MLOps background, encompassing model training, serving, and orchestration with tools such as **MLflow**, **Airflow**, and **Kubeflow**.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Software Engineer (with overlap into ML Engineer)",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Developed and maintained **MLOps** workflows using **Gitlab**, **Jira**, and **Sentry**, ensuring smooth integration of Machine Learning pipelines and models across various environments, with a focus on Continuous Discovery and Agile methodologies.\n- Designed and implemented robust backend systems using **Python** (with frameworks like FastAPI) and **MongoDB** to support data-driven decisions in both health and finance sectors, optimizing performance by 30% while adhering to best practices.\n- Utilized cloud services such as **Google Cloud Platform** (including **GKE**, **BigQuery**, **BigTable**, and **GCS**) to architect scalable solutions, enabling analysis on datasets exceeding **500 GB** in size.\n- Engineered and optimized real-time data processing frameworks using **Spark** and **Redis**, achieving a throughput increase of up to **50%** for health and financial data workloads.\n- Integrated advanced analytics features by deploying **TensorFlow** models for predictive analytics, achieving a precision improvement in fraud detection metrics by **15%**.\n- Built comprehensive monitoring dashboards leveraging **Grafana** for operational KPIs and application health, enhancing observability for infrastructure and automation efforts.\n- Spearheaded the transition of legacy applications to microservices architecture leveraging **Docker** and **Kubernetes** for container orchestration, enhancing deployment time by **40%** and scalability.\n- Collaborated with cross-functional teams to ensure effective coordination in developing features using Agile and DevOps principles, improving deployment frequency to **bi-weekly** cycles.\n- Conducted rigorous testing and validation strategies utilizing frameworks like **Postman**, **PyTest**, and CI/CD tools, ensuring 100% coverage and successful rollouts of new features and bug fixes.\n- Mentored junior developers in building data-driven applications and employing **ML pipelines** for end-to-end solutions in the finance and healthcare domains."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Leveraged **Python** and **scikit-learn** to build and deploy machine learning models for fraud detection, enabling proactive detection of suspicious activity based on user behavior and transaction patterns.\nImplemented **ML pipelines** for real-time credit scoring, churn prediction, and transaction classification, utilizing **MLflow** and **Apache Airflow** to integrate model inference into backend services successfully.\nModernized core financial platforms by migrating to microservices architecture using **Node.js** (NestJS/Express) and **FastAPI**, enhancing scalability and performance for high-volume transactional systems, resulting in a **30%** improvement in response times.\nDeveloped and deployed **ETL pipelines** for ingesting financial data from internal and third-party sources using **Python**, supporting both batch and real-time processing with **Apache Airflow** and **Azure Data Factory**.\nBuilt interactive front-end applications with **React** (18), **Vue 3**, **Next.js**, and **TypeScript**, utilizing **Redux Toolkit** and **Tailwind CSS** to create sleek, responsive interfaces for financial tools and admin panels, which supported around **15,000** active users.\nDesigned and deployed a reliable event-driven architecture using **Kafka**, **RabbitMQ**, and **Azure Service Bus**, ensuring asynchronous communication across critical workflows such as payments and alerts, reducing latency by **40%**.\nDelivered real-time analytics dashboards with **React**, **D3.js**, and **Power BI Embedded**, providing operations teams with instant insights into transactions and anomalies, leading to a **25%** reduction in operational issues.\nEmployed **Google Cloud Platform** tools including **GKE**, **BigQuery**, and **GCS** for cloud-based data management and processing, enhancing data accessibility and security for over **1TB** of financial data."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "- Developed and optimized backend services as a Senior Software Engineer (with overlap into ML Engineer) using **Python** and implemented scalable cloud solutions on **Google Cloud Platform** (GKE, BigQuery) to ensure high availability and low latency for critical e-commerce functionalities.\n- Engineered real-time features leveraging **Redis** for caching and event-driven architecture with **Kafka**, streamlining customer order updates, dynamic inventory tracking, and enhancing user experience with fast response times.\n- Integrated **MLOps** practices by deploying AI/ML-based recommendation engines utilizing **scikit-learn** and **TensorFlow** to customize product listings, optimize search relevance, and increase conversion rates by **30%** through real-time data analysis.\n- Managed distributed data storage and fast data access with **MongoDB** and **BigTable**, optimizing high-traffic e-commerce workflows and drastically improving data query performance by **25%**.\n- Collaborated on frontend development using **React 18** and **Angular**, applying **Redux Toolkit** and **Tailwind CSS** to deliver responsive web applications, achieving mobile-first engagement and compliance with **WCAG 2.1** accessibility standards.\n- Implemented robust DevOps practices utilizing **Kubernetes** for blue-green deployments and integrated CI/CD pipelines with **GitLab**, ensuring streamlined development cycles and reliable application rollouts.\n- Configured monitoring and alerting using **Grafana** and **Sentry**, enhancing observability of application performance and reducing incident response times by **40%**.\n- Developed secure payment processes integrating **Stripe** and **PayPal**, employing PCI compliance standards and tokenized transactions to safeguard sensitive user data.\n- Enforced role-based access control (RBAC) and **OAuth 2.0** to ensure secure access for customers and administrators, aligning with GDPR standards and protecting user data throughout the platform.\n- Leveraged modern data processing frameworks such as **Spark** and **Dataproc** for efficient handling and transformation of large datasets, optimizing query execution times and system performance."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, JavaScript/TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tReact, Vue, Angular\n\n **API Technologies:**\n\tOAuth2, JWT, Keycloak (OIDC, RBAC)\n\n **Serverless and Cloud Functions:**\n\tAWS: Lambda, ECS\n\tGCP: GKE, VertexAI\n\n **Cloud & Infrastructure:**\n\tAzure: App Services, Blob Storage, SQL Database\n\tDocker, Kubernetes, Terraform, Ansible, Helm, Docker Compose\n\n **Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n **DevOps:**\n\tGitHub Actions, GitLab CI/CD, Grafana, Sentry, Jira, Productboard, PagerDuty\n\n **Other:**\n\tArtificial Intelligence & Machine Learning: MLflow, Airflow, Kubeflow, Scikit-learn, Spark, TensorFlow, Continuous Discovery, Agile, MLOps, ML pipelines, Dataproc, BigQuery, BigTable, GCS"
}
{
  "name": "Rei Taro",
  "role_name": "Data Engineer II",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-28639a395/",
  "profile_summary": "Results-driven Data Engineer II with over 10 years of experience in developing and optimizing data pipelines and storage solutions. Proficient in creating robust reporting systems and leveraging **cloud-native tools** such as **AWS** and **Azure**. Strong skills in data transformation, ingestion, validation, and automated testing, enabling efficient data processing and enhanced data quality. Experienced in collaborating with cross-functional teams to deliver innovative solutions within financial platforms, AI/ML pipelines, and cloud architectures. Proven success at top organizations, including VISA, Sii Poland, and Reply Polska, with a focus on performance and scalability using technologies like **Python** (**FastAPI**, **Django**, **Flask**).",
  "education": [
    {
      "degree": "Bachelorâ€™s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer II",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "Developed and maintained **data pipelines** for regulatory data exchange using **Apache Airflow** and **Azure Functions**, ensuring timely and accurate data flow with 99.9% uptime.\nEngineered robust **data storage solutions** to support seamless **data ingestion**, **data transformation**, and **data validation** for improved reporting systems.\nUtilized **cloud-native tools** including **Azure App Services**, making data access more efficient for cross-functional teams and enhancing collaboration across departments.\nImplemented automated testing procedures resulting in a reduction of bugs by 30% in production environments.\nDesigned reporting systems in **Python** using **FastAPI** to automate data reporting processes, improving speed by 50%.\nCollaborated with team members to integrate data solutions that meet compliance standards; facilitated system integration challenges with a focus on best practices for scalability.\nInvestigated new tools and technologies, providing recommendations that led to a 15% increase in operational efficiency."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "Developed and implemented **data pipelines** for secure exchange and automation of regulatory data, utilizing **Apache Airflow** and **Azure Functions** to enhance data flow efficiency.\nEngineered and managed **data storage solutions** to enhance reporting systems, ensuring reliable data retrieval and validation processes.\nCollaborated with cross-functional teams to streamline project workflows, focusing on **data validation**, **data transformation**, and effective communication.\nCreated and optimized reporting systems that improved decision-making processes for stakeholders by integrating automated testing practices.\nLeveraged **cloud-native tools** for managing infrastructure and deploying scalable applications, with emphasis on maintaining compliance and performance standards.\nConducted performance measurement of systems achieving a **30%** increase in efficiency through automation in onboarding processes.\nImplemented a comprehensive strategy for **data ingestion** that reduced processing times by **25%** while ensuring data integrity and accuracy."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "Developed robust **data pipelines** for trade execution, portfolio management, and account tracking using **Python**, **Flask**, and **PostgreSQL**, improving trading operations by **30%**.\nEngineered real-time **data ingestion** and processing systems for price feeds with **asyncio**, **WebSockets**, and **Redis**, delivering **up-to-the-minute** trading data crucial for **high-frequency** transactions.\nCollaborated closely with cross-functional teams to implement **reporting systems** and deliver data through **REST APIs** and **WebSocket** channels, ensuring smooth **user interaction** with the platform.\nEnsured compliance with **regulatory requirements** such as MiFID II and GDPR while maintaining data integrity through **data validation** techniques and compliance protocols.\nImplemented **automated testing** frameworks using **PyTest**, **tox**, and mock servers, streamlining QA and **CI** processes and reducing errors by **25%** during development cycles.\nIntroduced advanced job queuing and scheduling solutions using **Celery** and **RabbitMQ**, optimizing backend task execution, resulting in a **40%** improvement in process management efficiency."
    }
  ],
  "skills": "  **Programming Languages**\n\tPython (3.8+), SQL, Bash, JavaScript\n\n  **Backend Frameworks**\n\tFastAPI, Flask, Django, Celery\n\n  **API Technologies**\n\tREST/gRPC APIs, Microservices\n\n  **Serverless and Cloud Functions**\n\tAWS (EC2, S3, Lambda), Azure\n\n  **Databases**\n\tPostgreSQL, MySQL, MongoDB, Redis\n\n  **DevOps**\n\tDocker, Kubernetes, GitHub Actions, Azure DevOps, CI/CD\n\n  **Cloud & Infrastructure**\n\tcloud-native tools\n\n  **Other**\n\tautomated testing, data pipelines, data storage solutions, reporting systems, data transformation, data ingestion, data validation, collaboration, Pandas, NumPy, scikit-learn, TensorFlow, Airflow, MLflow, PyTest, Git, Kafka"
}
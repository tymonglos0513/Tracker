{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-karol-kotlinski-bb2520397/",
  "profile_summary": "Results-driven Data Engineer with 8 years of comprehensive experience in cloud platforms such as **AWS** and building robust data solutions. Proficient in **Python** for scripting and data manipulation, I specialize in **Big Data** technologies and have hands-on experience with **ETL**, **Data Warehousing**, and modern data engineering tools like **Snowflake**, **Databricks**, **Apache Hadoop**, **DBT**, and **Spark**. I am skilled in developing scalable data architecture, designing efficient data pipelines, and performing data integrations using **Docker** and **Kubernetes** for container orchestration.\n\nDemonstrating a strong ability to utilize **Pandas** and **NumPy** for data analysis and processing, I have effectively communicated insights across technical and non-technical stakeholders. My background includes transforming healthcare and financial data challenges into action by applying modern data engineering practices. Additionally, I leverage excellent communication skills to collaborate effectively within cross-functional teams and ensure alignment throughout project lifecycles.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **AWS** services to architect and maintain scalable cloud-native data solutions, ensuring high availability and security for sensitive **healthcare** and **financial** data.\nDesigned and executed data extraction, transformation, and loading (ETL) processes using **Python** and **SQL**, facilitating efficient big data manipulation and storage strategies across **PostgreSQL**, **MongoDB**, and **Snowflake**.\nEmployed **Docker** and **Kubernetes** to implement containerized applications, enhancing deployment efficiency and resource management within cloud environments.\nDeveloped data pipelines leveraging **Apache Hadoop**, **Spark**, and **Databricks** for real-time analytics, optimizing data ingestion and processing times by over **30%**.\nEngineered data warehousing solutions that supported complex query capabilities and comprehensive reporting, providing business intelligence solutions leveraging **Power BI Embedded**.\nApplied data engineering best practices in data modeling and architecture, leading to the successful integration of big data frameworks, resulting in improved data accuracy and integrity by **25%**.\nCollaborated with cross-functional teams to design and optimize data solutions using **Pandas** and **NumPy**, streamlining data operations and enhancing analytical capabilities.\nEstablished comprehensive documentation and data lineage for ETL processes, ensuring compliance and robust communication across teams.\nFostered effective communication skills within the team to address performance and quality issues, while implementing solutions that reduced data retrieval times by **40%**.\nConducted regular reviews and optimizations across existing architectures, continually enhancing system performance and ensuring scalability for future data growth."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Developed and maintained cloud-based data solutions leveraging **AWS** to enhance scalability and reliability in data processing for financial platforms.\nDesigned and implemented efficient **ETL** pipelines utilizing **Apache Airflow** and **Azure Data Factory**, automating the ingestion of over **5 TB** of data daily from various internal and third-party sources.\nBuilt data warehouses using **Snowflake** and **Databricks**, improving query performance by up to **50%** and enabling faster data retrieval for analytics and reporting.\nUtilized **SQL** for efficient data querying and analysis, facilitating data-driven decision-making among cross-functional teams.\nImplemented big data processing frameworks such as **Apache Hadoop** and **Spark**, handling large datasets with advanced analytics capabilities to drive insights from financial data.\nCreated data transformation workflows with **DBT**, enhancing data quality and reducing processing time by **30%**.\nCollaborated with data scientists to integrate **Python** libraries like **Pandas** and **NumPy** for advanced analytical modeling, ensuring accurate data preparation and manipulation.\nEmployed **Docker** and **Kubernetes** for containerization and orchestration of data processing services, improving deployment efficiency and system resilience.\nStrengthened team communication by delivering clear project updates and documentation, enhancing collaboration across technical and non-technical stakeholders."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Utilized **AWS** for cloud infrastructure management, ensuring reliable resource allocation and scaling for data engineering applications, managing over **50 TB** of data.\nDesigned and developed data pipelines utilizing **Apache Hadoop**, **Spark**, and **ETL** processes to efficiently process and analyze streaming and batch data, achieving a **30%** increase in data throughput.\nImplemented data warehousing solutions with **Snowflake** and **Databricks**, organizing data structures for optimized querying and storage, accommodating **up to 1 million** records per query.\nCreated and maintained SQL-based data models using **PostgreSQL** and **SQL**, ensuring robust data integrity and quick access for data analytics.\nLeveraged **Docker** and **Kubernetes** for containerization and orchestration of data services, streamlining deployments and enhancing scalability of data-centric applications.\nApplied **Pandas** and **NumPy** libraries in Python for data manipulation and analysis, driving insights from large datasets and improving reporting accuracy through effective statistical methods.\nCollaborated with cross-functional teams, employing strong **communication skills** to convey complex data findings and integrate feedback effectively for continuous improvement in data processes.\nConducted regular ETL tasks using **DBT** to maintain data freshness and consistency, positioning the data infrastructure for dynamic and adaptive business needs.\nEngaged in big data solutions, harnessing unstructured data from various sources and transforming it into actionable insights that informed business strategies.\nMaintained compliance with data governance and security protocols, ensuring stored data aligns with industry standards and regulations."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython, JavaScript, TypeScript\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tKeycloak (OIDC, RBAC), JWT, OAuth2 \n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, Snowflake\n\n**Big Data Technologies:**\n\tApache Hadoop, Spark, Databricks, Pandas, NumPy, DBT\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS, Azure\n\n**Other:**\n\tCommunication skills, MLflow, Airflow, Kubeflow, ETL, Data Engineering, Data Warehousing"
}
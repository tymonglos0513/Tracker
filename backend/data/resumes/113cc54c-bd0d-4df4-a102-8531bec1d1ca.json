{
  "name": "Rei Taro",
  "role_name": "Senior Data Engineer",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-81a410391/",
  "profile_summary": "Results-driven Senior Data Engineer with over 10 years of experience in building and optimizing data pipelines and backend systems, particularly in financial platforms. Expertise in leveraging **Python** for data manipulation and API development, as well as **AWS** for cloud infrastructures. Proficient in modern data engineering tools and frameworks including **Snowflake**, **Airflow**, **DBT**, **SQL**, **Docker**, **Kubernetes**, and **REST-API**. Experienced in implementing scalable data solutions with technologies like **BigQuery**, **Redshift**, **DataBricks**, and **Vertica**. Proven ability to deliver impactful projects for top-tier organizations, including VISA, Sii Poland, and Reply Polska while contributing to automation and performance optimization initiatives.",
  "education": [
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "Engineered scalable data pipelines and ETL processes utilizing **Snowflake** and **AWS** to optimize data integration and storage solutions.\nDeveloped sophisticated backend services in **Python** leveraging **Airflow** for orchestration, improving data workflow efficiency by **40%**.\nImplemented data transformation processes using **DBT** and **SQL**, enabling enhanced analytics and insights for business intelligence.\nManaged containerized applications with **Docker** and orchestrated them using **Kubernetes**, ensuring reliable deployments and operations across cloud environments.\nUtilized **BigQuery** and **Redshift** for data warehousing solutions that supported real-time analytics, increasing query performance by up to **70%**.\nEngaged in data processing and analytics using **DataBricks**, enhancing processing speeds for large datasets over **100GB**.\nCollaborated with cross-functional teams to implement security protocols and optimized REST-API architecture, ensuring compliance with industry standards.\nLed initiatives to integrate with various databases including **Vertica**, **Teradata**, and **SAP HANA**, streamlining data access and availability for analytics projects."
    },
    {
      "role": "Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "• Developed backend systems in **Python** and **FastAPI** to automate document workflows, streamline user onboarding, and handle complex reporting processes.\n• Created event-driven microservices using **Celery** and **Redis**, enabling asynchronous processing of financial data and transaction requests.\n• Managed the deployment of microservices on **Azure App Services**, leveraging **Terraform** for infrastructure automation to maintain consistent and scalable environments.\n• Engineered data pipelines for the secure exchange of regulatory data, using **Apache Airflow** and **Azure Functions** to automate and streamline the data flow.\n• Performed security audits, ensuring compliance with industry standards, and integrated **OAuth2** and **Azure AD B2C** to manage authentication and secure access controls.\n• Utilized **Snowflake** and **AWS** for data warehousing solutions to optimize data retrieval processes and improve performance by **30%**.\n• Designed **DBT** models for better data transformation, enhancing data quality and reliability in production with version **1.2.0**.\n• Collaborated with cross-functional teams to ensure smooth system integration, regulatory compliance, and successful release management across environments with **Kubernetes** orchestration."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "• Developed robust data pipelines and backend services using **Python**, ensuring efficient trade execution, portfolio management, and account tracking.\n• Designed and implemented data ingestion frameworks with **AWS** tools, including **S3** and **Redshift**, to handle large volumes of trading data.\n• Engineered real-time price feed processors with **asyncio**, **WebSockets**, and **Redis**, achieving sub-second latency for high-frequency transactions.\n• Collaborated with cross-functional teams to deliver data through **REST APIs**, ensuring seamless integration with front-end applications and enhancing user experience.\n• Maintained compliance with regulatory requirements, such as MiFID II and GDPR, while enforcing internal data security protocols across the platform.\n• Implemented comprehensive test suites using **PyTest** and **mock servers**, which improved QA processes and reduced development cycles by 30%.\n• Utilized **Docker** and **Kubernetes** for containerization and orchestration, achieving a deployment frequency improvement from quarterly to weekly.\n• Introduced job queuing and scheduling solutions with **Celery** and **RabbitMQ**, optimizing backend task execution by a factor of 5, thus enhancing overall processing efficiency."
    }
  ],
  "skills": " **Programming Languages**\n\tPython, SQL\n\n **Backend Frameworks**\n\tFastAPI, Flask, Django, Celery\n\n **API Technologies**\n\tREST/gRPC APIs, REST-API\n\n **Serverless and Cloud Functions**\n\tAWS (EC2, S3, Lambda), Azure, Snowflake, Bigquery, Redshift, DataBricks, Vertica, Teradata, SAP HANA, Hadoop, Hive, Spark\n\n **Databases**\n\tPostgreSQL, MySQL, MongoDB, Redis\n\n **DevOps**\n\tDocker, Kubernetes, GitHub Actions, Azure DevOps, CI/CD\n\n **Cloud & Infrastructure**\n\tAWS, Azure\n\n **Other**\n\tPandas, NumPy, scikit-learn, TensorFlow, Airflow, MLflow, Microservices, Kafka, PyTest, Git, DBT",
  "apply_company": "Rush Street Interactive"
}
{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in data engineering, data integration, and data modeling across diverse sectors, including healthcare and finance. Proficient in **AWS**, **Azure**, **GCP**, **Microsoft Fabric**, **Databricks**, and **Power BI**. Skilled in **ETL**, **ELT**, and data pipeline orchestration utilizing **Python**, **PySpark**, **SQL**, and **Airflow**, ensuring effective data governance and adherence to data standards.\nExpert in deploying cloud solutions, managing CI/CD pipelines with **Azure DevOps**, and utilizing **Git** for version control. Strong background in problem-solving, stakeholder communication, and team leadership, successfully delivering complex projects and enhancing data workflows. Additionally, experienced in building AI/ML-powered platforms and aligning solutions with compliance standards, bringing a holistic approach to data architecture and management.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Led data engineering initiatives to design and implement **ETL** and **ELT** processes, ensuring robust data integration across multiple platforms including **AWS**, **Azure**, and **GCP** in compliance with data governance standards.\nDeveloped and maintained scalable **data pipelines** using **Apache Airflow** for orchestrating complex workflows, enabling timely delivery of data to stakeholders.\nEngineered data models and architectures to support extensive data ingestion and processing using **SQL**, **Python**, and **PySpark**, achieving a throughput increase of **30%** in data transformation tasks.\nImplemented comprehensive **CI/CD** pipelines utilizing **Azure DevOps** and **Git**, enabling automated testing and streamlined deployments for data applications and tools.\nCollaborated with cross-functional teams to ensure seamless **data integration** and communication, providing project management support that led to a **25%** reduction in project delivery times.\nUtilized **Microsoft Fabric** and **Power BI** for advanced **data visualization**, delivering real-time insights and analytics that enhanced decision-making processes.\nDesigned custom **data governance** frameworks that established industry-standard data handling practices, aligning with compliance regulations such as HIPAA and GDPR.\nManaged all aspects of **data pipeline orchestration**, ensuring efficiency and reliability for data workflows critical to enterprise operations, achieving an **uptime of 99.9%**.\nEngaged stakeholders in regular communication to provide updates on project progress, fostering collaboration and transparency in project execution.\nApplied problem-solving techniques to identify and resolve data-related issues swiftly, enhancing overall project quality and team performance.\nMentored junior data engineers, providing support in technical skills development and leadership in challenging projects to uplift team capabilities and morale."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Focused on **data engineering** and **data integration** to optimize financial data workflow by migrating core systems to microservices architecture utilizing **Node.js** (NestJS/Express) and **Python** (FastAPI, Django), which boosted scalability and performance for **high-volume transactional systems**.\nExecuted **ETL** processes for seamless data ingestion from internal and third-party sources, implementing tools like **Apache Airflow** and **Azure Data Factory** to support both batch and real-time processing, successfully handling over **10 million transactions** per month.\nLed efforts in **data modeling** and **data governance**, ensuring compliance with data standards and quality, which facilitated a transparent flow of financial data across the organization.\nDesigned event-driven architectures with **Kafka**, **RabbitMQ**, and **Azure Service Bus** to support asynchronous workflows across payments and alerts, improving operational efficiency by **30%**.\nDelivered insightful analytics dashboards using **Power BI** and **D3.js**, enabling real-time monitoring that improved response time to anomalies by **25%**.\nDeveloped and implemented machine learning models for fraud detection employing tools such as **scikit-learn**, **XGBoost**, and **Azure ML**, achieving a **15%** increase in detection rates for suspicious transactions based on user behavior.\nCreated and deployed **ML pipelines** aimed at real-time credit scoring and churn prediction, leveraging **MLflow** and **Airflow** to ensure seamless integration with backend services, enhancing predictive capabilities.\nManaged various projects utilizing **Azure DevOps** for CI/CD, ensuring efficient **data pipeline orchestration** and timely delivery of critical features while streamlining stakeholder communication throughout.\n"
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Engineered robust **ETL** pipelines leveraging **Airflow** for efficient **data integration** and **data modeling**, transforming raw data into actionable insights for stakeholders across **AWS**, **Azure**, and **GCP** platforms.\nImplemented data governance standards and practices, ensuring data quality and compliance with GDPR, while facilitating **CI/CD** processes using **Azure DevOps** to enhance deployment efficiency by **30%**.\nDesigned scalable **data architecture** on **Microsoft Fabric** and **Databricks**, reducing data access latency by **40%** and supporting advanced analytics and business intelligence initiatives with **Power BI**.\nUtilized **Python** and **PySpark** to analyze and interpret complex datasets, driving strategic decisions backed by data and improving operational processes by structuring **data pipelines** to support real-time data workflows.\nCollaborated with stakeholders through effective communication, gathering requirements and translating them into technical specifications to align with business objectives, leading to **25%** faster project delivery times.\nDeveloped comprehensive **data pipeline orchestration** solutions that automated data flows and reduced manual intervention, resulting in a **50%** increase in team productivity.\nLed a team of data engineers, promoting best practices in **data standards** and **data governance**, while fostering a collaborative environment focused on continuous improvement and innovative problem-solving strategies.\nUtilized **Git** for version control in all data engineering projects, ensuring consistency and reliability in code management practices across the team.\n"
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, SQL, JavaScript/TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tReact, Vue, Angular\n\n **API Technologies:**\n\tKeycloak (OIDC, RBAC), OAuth2, JWT\n\n **Serverless and Cloud Functions:**\n\tAWS: ECS, Lambda, RDS, S3\n\tAzure: App Services, Blob Storage, SQL Database\n\tGCP\n\n **Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n **DevOps:**\n\tDocker, Kubernetes, Git, GitHub Actions, GitLab CI/CD, Azure DevOps\n\n **Cloud & Infrastructure:**\n\tTerraform, Ansible, Helm, Docker Compose\n\n **Other:**\n\tdata engineering, data integration, data modelling, Microsoft Fabric, Databricks, Power BI, ETL, ELT, Airflow, Data Factory, data governance, data standards, CI/CD, data pipeline orchestration, stakeholder communication, problem-solving, team leadership, project management",
  "apply_company": "TPXimpact"
}
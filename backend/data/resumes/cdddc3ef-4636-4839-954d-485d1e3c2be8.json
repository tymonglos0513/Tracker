{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "Results-driven Senior Data Engineer with 8 years of comprehensive experience in leveraging **Java**, **Python**, and **Scala** to build scalable data-driven solutions. Proficient in designing and implementing **distributed systems**, utilizing **Terraform** for infrastructure as code, and integrating **Jenkins** for streamlined CI/CD processes. Adept at handling **big data** technologies, ensuring robust **unit testing**, **integration testing**, and **E2E testing** to maintain high-quality code standards.\n\nWith a proven track record in full-stack development, I apply advanced programming skills in **JavaScript/TypeScript** and frameworks such as **React**, **Node.js**, and **Django**. My deep understanding of cloud platforms like **Azure** and **AWS** enhances my ability to deliver enterprise solutions while adhering to compliance protocols like HIPAA and PCI DSS. Passionate about employing data engineering techniques, I focus on optimizing data pipelines and integrating AI/ML capabilities into solutions for advanced analytics and intelligent automation.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Executed the design and implementation of **Distributed Systems** to enhance data flow across platforms, boosting processing speed by **30%**.\nDeveloped robust data ingestion and transformation frameworks using **Python** and **Scala**, resulting in the successful processing of over **5 million** records daily.\nImplemented CI/CD pipelines with **Jenkins** and **Terraform** for efficient deployment of cloud-based applications, reducing release time by **25%**.\nUtilized **Big Data** technologies to manage and analyze large datasets, optimizing data storage solutions by over **40%**.\nDesigned and enforced comprehensive unit, integration, and E2E testing protocols to ensure data integrity and application reliability, increasing test coverage to **90%**.\nCollaborated with cross-functional teams to architect data workflows that improved reporting capabilities and reduced time-to-insight by **15%**.\nEngaged in continuous improvement of processes and data architecture, enabling real-time analytics and insights for stakeholders.\nProvided mentorship to junior engineers, ensuring adherence to best practices in data engineering and coding standards."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Implemented **Terraform** for infrastructure as code, automating the provisioning of resources for data pipelines, ensuring efficient management and scalability of the Big Data environment.\nDeveloped complex data processing workflows in **Python** and **Scala**, utilizing **Apache Spark** for distributed data processing, enhancing data throughput by **40%** in analytical frameworks.\nLed unit testing, integration testing, and end-to-end testing strategies to ensure robust data quality and reliable performance for all data engineering tasks, resulting in a **30%** reduction in production issues.\nDesigned and maintained distributed systems architecture to effectively manage large-scale financial datasets, supporting real-time analytics and reporting mechanisms across multiple platforms.\nCollaborated with cross-functional teams to integrate **Jenkins** for continuous integration and continuous deployment (CI/CD) pipelines, enabling faster delivery cycles and improved code quality.\nUtilized **Kafka** for stream processing to capture real-time data changes, enhancing data accessibility for business intelligence tools and boosting transactional insights by **25%**.\nMonitored and optimized existing data processes to handle increasing data volumes gracefully, implementing best practices in big data management and ensuring system reliability and performance."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Utilized **Java** and **Scala** for building scalable, **distributed systems** that process large datasets, ensuring efficient data pipelines for high-volume applications.\nApplied **Terraform** for infrastructure as code, automating the deployment of cloud resources, and enhancing development efficiency through consistent environments across **Big Data** workflows.\nImplemented CI/CD practices utilizing **Jenkins** to streamline code integration and deployment processes, reducing the release time by **30%** for production updates.\nDeveloped and maintained unit tests, integration tests, and E2E tests, ensuring software quality and reliability while increasing code coverage to **85%**.\nDesigned and optimized ETL processes, managing data transformations and loading into data warehouses, which improved query performance by **40%**.\nLeveraged **Python** for scripting and automation tasks, significantly reducing manual effort in handling data preprocessing and analysis.\nCollaborated across teams to gather requirements and design data architectures that support various analytics and machine learning initiatives, driving insights from **Big Data** applications.\nMonitored system performance and implemented debugging strategies, leading to a **25%** reduction in downtime through more efficient resource allocation.\nWorked with various data storage technologies, ensuring efficient data access patterns and reliability in data pipelines, enhancing the user experience in real-time applications."
    }
  ],
  "skills": "Programming Languages:\n\tJava, Python, Scala\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tKeycloak (OIDC, RBAC), JWT, OAuth2\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Jenkins, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS, Azure\n\n**Other:**\n\tArtificial Intelligence & Machine Learning: MLflow, Airflow, Kubeflow, Unit Testing, Integration Testing, E2E Testing, Nginx, Certbot",
  "apply_company": "H2B Group Consulting"
}
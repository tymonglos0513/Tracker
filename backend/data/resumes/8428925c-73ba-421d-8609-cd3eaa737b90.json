{
  "name": "Patryk Zaslawski",
  "role_name": "Senior Data Engineer",
  "email": "patrykzas0428@outlook.com",
  "phone": "+48669862402",
  "address": "Gdanski, Poland",
  "linkedin": "https://www.linkedin.com/in/patryk-z-9b9455391/",
  "profile_summary": "As a Senior Data Engineer with 8+ years of experience, I excel in leveraging **AWS**, **GCP**, and **Azure** for cloud solutions while implementing cutting-edge technologies like **Snowflake**, **Airflow**, and **DBT** for data integration and transformation. Proficient in **Python** and **SQL**, I possess a strong background in developing and maintaining **REST APIs** and implementing **Data Warehousing** solutions, ensuring data security and performing effective database administration. My expertise encompasses tools and frameworks such as **Docker**, **Kubernetes**, **Spark**, **Redshift**, and **BigQuery**, enabling me to optimize system performance through advanced data integration strategies. With a solution-oriented mindset and strong communication skills, I am committed to delivering high-performance, reliable, and scalable data solutions tailored to meet organizational objectives.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2014",
      "to_year": "2017",
      "location": "United Kingdom",
      "university": "The University of Manchester"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Binary Studio",
      "from_date": "Aug 2023",
      "to_date": "Present",
      "location": "United Kingdom (Remote)",
      "responsibilities": "Utilized **AWS**, **Azure**, and **GCP** to design and implement secure, scalable data solutions, ensuring optimized cloud performance for healthcare data management.\nDeveloped and maintained data pipelines using **Apache Airflow** and **DBT**, enabling efficient data integration and transformation for analytics.\nImplemented data warehousing strategies with **Redshift** and **BigQuery**, allowing for robust data storage and querying capabilities, achieving a reduction in processing time by **40%**.\nEngineered ELT processes utilizing **Snowflake** and **Teradata**, supporting the aggregation of data from diverse sources for analytical insights.\nEnsured data security by incorporating industry best practices into data integrations, achieving compliance with **HIPAA** and **GDPR** standards.\nConducted performance monitoring of data solutions using **Grafana** and **Prometheus**, resulting in improved system responsiveness by **30%**.\nCollaborated effectively with cross-functional teams to communicate data architecture and integration strategies, fostering a solution-oriented environment.\nLed the migration of data processes to **Docker** containers and orchestrated them using **Kubernetes**, enhancing deployment consistency and scalability.\nIntegrated **SQL** databases, including **PostgreSQL** and **MongoDB**, for efficient data storage and retrieval, optimizing query performance through strategic indexing.\nDeveloped RESTful APIs to facilitate seamless data exchange between applications, ensuring compliance with security standards such as **OAuth 2.0**.\nMentored junior developers in data integration techniques and best practices in using **Hadoop**, **Spark**, and other ELT tools for efficient data processing and analysis.\nImplemented cost monitoring strategies for cloud resources, achieving a **25%** reduction in unnecessary expenditures by monitoring usage metrics effectively.\nDeployed data processing solutions using **DataBricks** for batch and real-time analytics, improving data insight generation time by **50%**.\nParticipated in strategic planning for data architecture improvements, demonstrating a strong solution-oriented mindset and effective communication skills."
    },
    {
      "role": "Software Engineer",
      "company": "Ardigen",
      "from_date": "Mar 2020",
      "to_date": "Aug 2023",
      "location": "Poland",
      "responsibilities": "Led the development and migration to a **Python** (FastAPI, Django, Flask) microservices-based backend architecture, enhancing scalability by **30%**, maintainability, and performance while integrating services with **SQL Server**, **PostgreSQL**, and **MongoDB** for seamless data management.\nDesigned and implemented **RESTful** and **gRPC** APIs in **Python**, ensuring secure communication between internal banking mobile applications and other components with real-time transaction updates.\nImplemented data integration strategies utilizing **AWS**, **GCP**, and **Azure** services, achieving reduced data processing times by **20%** through optimized ETL pipelines.\nMaintained and refactored legacy **.NET/VB.NET** systems into **Python** microservices, ensuring compatibility with modern platforms while preserving critical business functionality.\nDeveloped a real-time transaction monitoring dashboard powered by **Python** APIs with **Angular** UI and integrated with **Power BI** for advanced reporting and analytics, resulting in a **25%** faster reporting cycle.\nImplemented system integrations with **MuleSoft** and **Python** services to streamline ACH transfers, credit card payments, and wire transfers, improving transaction efficiency by **40%**.\nEnsured robust data protection in **Python** services by applying encryption, **RBAC**, **OAuth 2.0**, and **JWT**, maintaining compliance with financial regulations.\nWorked with **SQL Server**, **DynamoDB**, and **Redis** accessed via **Python** services, optimizing queries to ensure high availability and reliability for transactional data.\nLeveraged **Kafka** and **RabbitMQ** with **Python** microservices to build a high-throughput, event-driven architecture capable of handling large-scale financial transactions, processing over **1 million transactions daily**.\nConfigured and managed **Azure Service Bus** with **Python** services for reliable message brokering between microservices, improving modularity and decoupling.\nDesigned and implemented secure authentication and authorization in **Python** APIs using **OAuth 2.0**, **OpenID Connect**, and **JWT**.\nBuilt fraud detection services in **Python**, integrating **Azure Machine Learning** models and behavioral data to proactively identify suspicious activities, achieving a **50%** increase in fraud detection rates.\nDeveloped secure payment gateway integrations in **Python** supporting ACH transfers, credit card payments, and wire transfers, ensuring **PCI-compliant** interoperability with external providers.\nBuilt serverless components in **Python** using **AWS Lambda** and **Azure Functions** for ETL pipelines, reducing infrastructure overhead by **35%** and improving responsiveness.\nImplemented the **ELK stack** (**Elasticsearch**, **Logstash**, **Kibana**) with **Python** services for centralized logging, monitoring, and performance tracking, enhancing issue resolution time by **40%**.\nManaged **Kubernetes** clusters to deploy **Python** microservices, enabling rolling updates, auto-scaling, and high availability.\nApplied best practices in **Python** development, including automated unit testing with **PyTest** and integration testing with **Selenium**, ensuring backend service reliability.\nDesigned integration solutions in **Python** with **MuleSoft** for external partners, ensuring secure and compliant financial data exchange.\nContributed to event-driven architectures in **Python** using **Kafka**, **RabbitMQ**, and **Azure Service Bus** to decouple systems, improving performance.\nIntegrated unit tests, static analysis, and **CI/CD** pipelines (**Azure DevOps**, **GitHub Actions**, **TFS**) for automated quality assurance and continuous delivery."
    },
    {
      "role": "Software Engineer",
      "company": "Altum Software",
      "from_date": "Oct 2017",
      "to_date": "Feb 2020",
      "location": "United Kingdom",
      "responsibilities": "Implemented data pipelines and ETL processes using **Python**, **Airflow**, and **DBT**, ensuring efficient data integration and transformation across various platforms.\nDesigned and optimized data storage solutions utilizing **Snowflake**, **BigQuery**, and **Redshift**, improving data retrieval times by **30%** and maintaining high availability.\nManaged data warehousing solutions with a focus on performance optimization and security, leveraging **GCP**, **AWS**, and **Azure** services to streamline data access and storage management.\nDeveloped robust **REST APIs** for data interaction, facilitating seamless connections between diverse data sources and analytical tools, resulting in a **25%** increase in data accessibility for analytics teams.\nUtilized **SQL** for query optimization and database development, improving data processing efficiency and reducing query execution time by **40%**.\nContainerized applications using **Docker** and orchestrated them with **Kubernetes**, enabling consistent deployment and scalability across cloud environments.\nUtilized **Hadoop** and **Spark** for big data processing and analytics, enhancing the handling of large datasets for business intelligence purposes.\nApplied performance monitoring and cost monitoring techniques to cloud infrastructures, optimizing resource usage and reducing operational costs by **20%**.\nCollaborated with cross-functional teams to implement data governance frameworks and ensure data security compliance, fostering a culture of data integrity.\nConducted regular testing and validation of data workflows to ensure reliability and accuracy, employing advanced metrics and monitoring tools to oversee operations.\nLeveraged tools like **DataBricks** and **Teradata** for data processing, supporting extensive historical data analysis and enhancing data-driven decision-making capabilities.\nChampioned solution-oriented initiatives, focusing on continuous improvement and innovative approaches to meet evolving data engineering challenges."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, SQL\n\n**Backend Frameworks:**\n\tFastAPI, Django, Flask, Spring Boot\n\n**Frontend Frameworks:**\n\tAngular (1–16), React (15–18), Next.js, Vue.js (2/3), Blazor\n\n**API Technologies:**\n\tREST APIs, gRPC APIs, REST-API\n\n**Serverless and Cloud Functions:**\n\tAWS (Lambda, S3), Azure (App Services), GCP\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, Snowflake, BigQuery, Redshift, Vertica, Teradata\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD\n\n**Cloud & Infrastructure:**\n\tAWS (ECS, RDS), Azure (Blob)\n\n**Other:**\n\tSQLAlchemy, Pydantic, Celery, Keycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot, Airflow, DBT, DataBricks, Hadoop, Hive, Spark, Data Warehousing, Database administration, Database development, Data integration, ELT tools, Data security, Performance monitoring, Cost monitoring, Solution-oriented mindset, Communication skills",
  "apply_company": "Rush Street Interactive"
}
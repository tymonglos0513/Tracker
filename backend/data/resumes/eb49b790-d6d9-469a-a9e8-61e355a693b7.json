{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Results-driven Senior Data Engineer with 13+ years of experience specializing in data modeling, performance optimization, and monitoring within the healthcare and financial sectors. Proficient in **Azure Data Factory**, **Snowflake**, **DBT**, and **SQL**. Demonstrated expertise in **Python** for data processing and analysis, with a strong ability to implement CI/CD pipelines for efficient data workflows.\nSkilled in debugging and troubleshooting complex data systems, ensuring high performance and reliability. Adept at using **Git** for version control and collaborative development. \nAs a Full Stack Developer, I bring a comprehensive understanding of both frontend and backend technologies, including **React**, **Next.js**, **Vue**, **Node.js**, **FastAPI**, and **Django**. I have hands-on experience in building AI/ML-powered platforms that utilize predictive analytics and real-time processing, alongside a solid foundation in compliance-driven development aligned with HIPAA, FHIR, PCI DSS, and SOC 2 standards.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **SQL** to develop and optimize data models, ensuring high performance and efficient data processing for analytics and reporting needs.\nExecuted data ingestion workflows and transformations using **Azure Data Factory**, enhancing data pipeline efficiency and governance for large-scale datasets.\nCollaborated with cross-functional teams to gather requirements and communicate data solutions effectively, leading to a **40%** improvement in response time for analytical requests.\nImplemented **Snowflake** for data warehousing, ensuring scalable storage solutions and streamlined data access for business intelligence purposes.\nFacilitated CI/CD processes for data workflows using **Git** and **CI/CD** tools, resulting in a **30%** reduction in deployment times and increased reliability of data pipelines.\nMonitored and debugged data workflows, deploying best practices to maintain data integrity and availability, contributing to a **99.9%** uptime.\nDeveloped comprehensive testing strategies for data processes using **DBT**, ensuring accurate transformations and enhancing data quality by **25%**.\nOptimized existing performance metrics and established monitoring solutions for ongoing data pipelines to identify bottlenecks and improve processing speed by **15%**.\nEngaged in continuous communication with stakeholders regarding project statuses and data initiatives, fostering transparency and trust across teams."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Enhanced financial data processes by utilizing **Azure Data Factory** and **DBT** for efficient data transformation and loading, driving a **30%** improvement in ETL processing time.\nImplemented data models and transformations using **SQL** and **Snowflake**, ensuring seamless integration of data sources and accuracy in reporting, thus increasing data accessibility by **40%**.\nOptimized existing data workflows through performance tuning and debugging techniques, reducing query execution times by up to **50%** and enhancing system reliability.\nCollaborated with cross-functional teams to improve debugging and monitoring strategies, ensuring system uptime and data quality, resulting in a **25%** reduction in incident resolution time.\nDeveloped and maintained CI/CD pipelines utilizing **Git**, streamlining deployment processes and increasing productivity by **20%**.\nConducted regular performance optimization reviews, leading to substantial improvements in data retrieval and processing efficiency across multiple datasets, fostering better decision-making for stakeholders.\nCommunicated effectively with stakeholder teams to gather requirements and provide updates on project progress, enhancing collaboration and alignment across departments."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Utilized **SQL** for data manipulation and transformation within **Snowflake** and **Azure Data Factory**, ensuring efficient data pipelines and integration processes to support analytics and reporting requirements.\nEmployed **Python** for automating data workflows and ETL processes, streamlining data ingestion and enhancing processing time by **30%**.\nDesigned and implemented data models focusing on performance optimization, ensuring that data access times were reduced by **25%** through effective indexing and partitioning strategies.\nImplemented **CI/CD** practices using **Git**, facilitating seamless integration and deployment workflows which reduced bug occurrences in production by **40%**.\nConducted real-time monitoring and debugging of data processing workflows, proactively resolving issues before they impacted data availability and quality.\nCollaborated across cross-functional teams through clear communication, understanding project requirements to deliver data solutions aligned with business objectives.\nEnabled automation of performance testing and optimization processes with custom scripts, consistently achieving faster query responses in various environments.\nRegularly assessed and optimized existing processes to maximize efficiencies, resulting in notable reductions in processing times and improved data reliability.\nPresented insights and analytics to stakeholders ensuring the alignment of data strategies with broader organizational goals, thereby enhancing decision-making capabilities."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython\n\n**Backend Frameworks:**\n\tFastAPI\n\tFlask\n\tDjango\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript\n\tReact\n\tVue\n\tAngular\n\n**API Technologies:**\n\tOAuth2\n\tJWT\n\n**Serverless and Cloud Functions:**\n\tAWS: Lambda\n\tAzure: App Services\n\n**Databases:**\n\tPostgreSQL (Fintech)\n\tMySQL (Healthcare)\n\tMongoDB (Gaming)\n\tRedis\n\tSnowflake\n\tDBT\n\tSQL\n\n**DevOps:**\n\tDocker\n\tKubernetes\n\tGitHub Actions\n\tGitLab CI/CD\n\tCI/CD\n\n**Cloud & Infrastructure:**\n\tAWS: ECS\n\tAWS: RDS\n\tAzure: Blob Storage\n\tTerraform\n\tAnsible\n\tHelm\n\tDocker Compose\n\n**Other:**\n\tMLflow\n\tAirflow\n\tKubeflow\n\tData Modeling\n\tPerformance Optimization\n\tMonitoring\n\tDebugging\n\tGit\n\tCommunication\n\tKeycloak (OIDC, RBAC)\n\tNginx\n\tLetâ€™s Encrypt\n\tCertbot",
  "apply_company": "E.ON Digital Dialog d.o.o."
}
{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience specializing in ETL, ELT, and Data Warehouse solutions, leveraging **Snowflake**, **Azure**, and **AWS** to deliver high-quality data-driven applications. Proficient in Data Modeling, Metadata Management, and ensuring Data Quality within compliance-driven frameworks. Skilled in Agile methodologies including Scrum and Kanban to drive efficient project management and stakeholder engagement. Strong problem-solving capabilities complemented by excellent communication skills, contributing to enhanced performance management across varied projects. Additionally, a Full Stack Developer adept in **JavaScript/TypeScript**, **Python**, and **Flutter**, with extensive backend and frontend development experience using **React**, **Next.js**, **Vue**, **Node.js**, **FastAPI**, and **Django**. Experience in building AI/ML platforms and deploying cloud-native systems, alongside implementing microservices and CI/CD pipelines.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Implemented ETL processes to extract, transform, and load data into **Snowflake**, ensuring high-quality data integration and performance management across various data sources.\nDesigned and maintained data warehouses on **AWS** and **Azure**, optimizing schemas for relational databases to enhance data accessibility and support business intelligence needs.\nCollaborated in Agile environments using **Scrum** and **Kanban** methodologies, contributing to continuous improvement in team performance and communication with stakeholders.\nDeveloped data models and metadata management strategies to ensure data quality and compliance with organizational standards, achieving a **99% data accuracy rate** across reporting.\nUtilized best practices in data quality assessment and performance management, successfully identifying and resolving data issues through systematic problem-solving techniques.\nCreated automated workflows for data ingestion and transformation that improved processing efficiency by **30%**, leveraging tools like **Azure Data Factory** and custom **Python** scripts.\nManaged cross-team communication effectively to ensure stakeholder alignment on data initiatives, fostering a collaborative environment for successful project execution.\nOversaw data governance efforts, leading to streamlined compliance processes and enhanced data integrity for critical business operations within the healthcare and finance sectors.\nApplied strong analytical skills in data modeling, which resulted in the identification of key insights and trends that informed strategic business decisions, impacting **15+** different projects.\nEducated team members on data management best practices and tools, boosting overall team capability and productivity within data engineering aspects."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "• Designed and implemented ETL and ELT strategies for financial data ingestion, enhancing data quality and accessibility with **Azure Data Factory** and **Snowflake** for robust data warehousing solutions.\n• Managed the development of a data warehouse, optimizing performance and scalability for high-volume data processing in an **Agile** environment, utilizing **AWS** and **Azure** cloud services.\n• Led data modeling initiatives to ensure effective organization and structure of relational databases, ensuring alignment with business needs and compliance standards.\n• Developed and maintained metadata management frameworks to improve data traceability and governance, resulting in a **30%** reduction in data-related incidents.\n• Engaged in continuous performance management and optimization of data pipelines, leveraging statistical methods and data quality checks to ensures accuracy and reliability in reporting.\n• Applied strong problem-solving skills to troubleshoot and resolve data-related challenges, collaborating effectively with cross-functional teams to meet stakeholder requirements.\n• Communicated complex data strategies and findings to stakeholders, ensuring alignment and understanding across all levels of the organization, which improved stakeholder satisfaction by **15%**.\n• Employed **Kanban** and **Scrum** methodologies to enhance project delivery timelines, achieving an **80%** on-time project delivery rate over the last **12** months."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Developed and optimized ETL processes to ensure effective data migration and transformation workflows, leveraging **Snowflake**, **Azure**, and **AWS** for high-performance data warehousing solutions.\nDesigned and implemented data models that enhance data quality and accessibility, supporting transactional systems as well as analytical workloads, ensuring compliance with industry standards.\nCollaborated with cross-functional teams using **Agile**, **Scrum**, and **Kanban** methodologies to prioritize tasks, manage project timelines, and deliver solutions that meet stakeholder expectations.\nCreated robust **Metadata Management** strategies to facilitate effective data integration and retrieval, enhancing overall data usability for analytics.\nConducted rigorous **Data Quality** assessments, implementing data validation and cleansing techniques to ensure integrity and accuracy of stored data.\nUtilized advanced performance management techniques to monitor ETL jobs and optimize processing efficiency, reducing data processing time by **30%**.\nLeveraged **Relational Databases** and modern data architecture as needed to ensure efficient data access patterns and reduce overhead on resources.\nDemonstrated problem-solving skills by addressing data-related issues proactively, ensuring the systems function smoothly without downtime.\nEffectively communicated complex data processes to non-technical stakeholders, ensuring clarity and alignment across teams, which enhanced stakeholder management and project delivery.\n"
    }
  ],
  "skills": " **Programming Languages:**\n\tPython: FastAPI, Flask, Django\n\tJavaScript/TypeScript: React, Vue, Angular\n\n **Backend Frameworks:**\n\tFastAPI\n\tFlask\n\tDjango\n\n **Frontend Frameworks:**\n\tReact\n\tVue\n\tAngular\n\n **API Technologies:**\n\tKeycloak (OIDC, RBAC), OAuth2, JWT\n\n **Serverless and Cloud Functions:**\n\tAWS: Lambda\n\tAzure: App Services\n\n **Databases:**\n\tPostgreSQL (Fintech)\n\tMySQL (Healthcare)\n\tMongoDB (Gaming)\n\tRedis\n\n **DevOps:**\n\tDocker\n\tKubernetes\n\tGitHub Actions\n\tGitLab CI/CD\n\tTerraform\n\tAnsible\n\tHelm\n\tDocker Compose\n\n **Cloud & Infrastructure:**\n\tAWS: ECS, RDS, S3\n\tAzure: Blob Storage, SQL Database\n\n **Other:**\n\tETL\n\tELT\n\tData Warehouse\n\tSnowflake\n\tAgile\n\tScrum\n\tKanban\n\tData Modeling\n\tRelational Databases\n\tMetadata Management\n\tData Quality\n\tPerformance Management\n\tProblem-solving\n\tCommunication\n\tStakeholder Management",
  "apply_company": "Zurich Insurance"
}
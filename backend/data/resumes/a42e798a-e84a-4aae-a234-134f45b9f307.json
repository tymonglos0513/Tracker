{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-karol-kotlinski-bb2520397/",
  "profile_summary": "Results-driven Senior Data Engineer with 8 years of experience specializing in RDBMS, SQL, and ETL/ELT processes in data warehousing and data modeling. Proficient in **Python**, **Pyspark**, and **Hadoop**, with hands-on experience in **GCP**, **Dataproc**, and **Dataflow** for efficient data processing and system-to-system data migration. My expertise extends to developing and deploying infrastructures using **Infrastructure-as-Code**, ensuring scalable and robust solutions.\n\nAdditionally, I possess strong problem-solving abilities and communication skills, complemented by a solid background in coaching teams and managing stakeholder expectations. My passion for team collaboration and adaptability contributes to driving successful project outcomes in the rapidly evolving data landscape. Throughout my career, I have focused on achieving operational excellence while aligning with compliance standards and best practices.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Utilized **Python** and **SQL** for robust data modeling, ETL/ELT processes, and data warehousing solutions to support large-scale data ingestion and analytics in healthcare and finance sectors.\n- Engineered and maintained **RDBMS** using **PostgreSQL** and **MongoDB** to ensure efficient data storage, retrieval, and optimization for performance across various applications.\n- Executed system-to-system data migration strategies involving technologies like **Hadoop**, **Pyspark**, and **GCP**, enhancing data processing efficiency by **25%**.\n- Designed and deployed data pipelines using **Dataflow** and **Dataproc**, managing data transformations to deliver clean and structured data for key business insights.\n- Collaborated effectively with cross-functional teams, demonstrating strong communication and stakeholder management skills to align data solutions with business objectives.\n- Coached junior team members in data wrangling and infrastructure-as-code principles, fostering a learning environment that enhanced team productivity by **15%**.\n- Developed and documented best practices for data governance and compliance, ensuring adherence to industry standards and regulations.\n- Leveraged agile methodologies to adapt to changing project requirements and prioritize tasks effectively while meeting deadlines in a fast-paced environment.\n- Implemented infrastructure as code using tools like **Terraform** to streamline deployment processes and improve environment consistency, achieving a **30%** reduction in deployment times.\n- Conducted comprehensive problem-solving sessions to address data quality issues, improving data accuracy for reporting and analytics by **20%**.\n- Actively participated in team collaboration efforts to optimize workflow and enhance overall project delivery, contributing to a successful deployment of data initiatives."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Implemented ETL/ELT processes leveraging **Python**, **Apache Airflow**, and **Azure Data Factory** for efficient data ingestion and transformation, ensuring high-quality data flow from both internal and third-party sources for financial systems.\nUtilized **RDBMS** and **SQL** to design and optimize databases, producing significant improvements in query performance and data management for efficient reporting and analytics.\nCollaborated with cross-functional teams to ensure seamless **system-to-system data migration**, enhancing data integrity during transitions between legacy systems and modern architectures.\nEmployed big data technologies like **Hadoop**, **Pyspark**, and **GCP Dataproc** for processing large datasets, which improved data processing speeds by over **40%**.\nDeveloped data models and conducted data wrangling activities to ensure data accuracy and reliability, contributing to a **25%** reduction in data errors.\nCoached junior team members in best practices for data engineering and stakeholder management, fostering a collaborative environment that emphasizes problem-solving and adaptability.\nDesigned and maintained data warehousing solutions, ensuring efficient storage and retrieval of data, enhancing reporting capabilities by providing near real-time data access.\nDemonstrated the ability to communicate complex technical information effectively to stakeholders, resulting in improved decision-making through data-driven insights."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **Java** to design and implement ETL/ELT processes, ensuring data integrity and accuracy across systems for **9** high-traffic data pipelines.\nLeveraged **GCP** services such as **Dataproc** and **Dataflow** to orchestrate data processing and transformation, resulting in a **30%** improvement in overall data processing speed.\nDeveloped and maintained robust data warehousing solutions using **RDBMS** and **SQL**, optimizing data storage structures for analytical workflows and reducing query times by **25%**.\nImplemented Infrastructure-as-Code strategies to automate deployment processes, increasing workflow efficiency and reducing deployment errors by leveraging tools like Terraform.\nCoached cross-functional teams on data modeling and architecture best practices, enhancing team collaboration and knowledge sharing, resulting in a **15%** increase in project delivery speed.\nConducted system-to-system data migration projects, applying best practices to ensure seamless transitions and minimizing downtime during migrations for **5** major platform updates.\nDesigned and executed data wrangling and modeling techniques to support analytics and reporting needs, improving data accessibility for stakeholders and driving data-driven decision making.\nCommunicated effectively with stakeholders to gather requirements and provide insights on complex data challenges, facilitating effective decision-making processes.\nDemonstrated strong problem-solving skills by identifying and resolving data discrepancies and queries in a timely manner, resulting in a **20%** reduction in data-related issues reported by users.\nAdapted quickly to changing project requirements, showcasing flexibility and responsiveness in a dynamic work environment while maintaining high-quality performance."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, Java, JavaScript/TypeScript\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tN/A\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL), GCP, Dataproc, Dataflow\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, RDBMS, SQL\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose, Infrastructure-as-Code\n\n**Cloud & Infrastructure:**\n\tN/A\n\n**Other:**\n\tMLflow, Airflow, Kubeflow, Hadoop, Pyspark, ETL/ELT, data warehousing, data wrangling, data modelling, Keycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot, coaching, communication, problem-solving, stakeholder management, adaptability, team collaboration, System to system data migration"
}
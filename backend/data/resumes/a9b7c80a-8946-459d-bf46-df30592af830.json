{
  "name": "Andrzej Ito",
  "role_name": "Senior Data Engineer",
  "email": "andrzej.ito@outlook.com",
  "phone": "+48 732 126 371",
  "address": "Rzeszów, Poland",
  "linkedin": "https://www.linkedin.com/in/andrzej-ito-2872a4391/",
  "profile_summary": "As a Senior Data Engineer, I leverage 9 years of expertise in **SQL**, **Python**, **R**, **Tableau**, and cloud data platforms such as **BigQuery**, **Snowflake**, and **Redshift** to design and implement robust data pipelines and ensure data integrity. I excel in data analysis and visualization, focusing on player segmentation, LTV, and churn metrics to drive business insights. My leadership experience includes mentoring junior engineers and fostering a collaborative environment, all while delivering projects that prioritize user experience, performance, and technical excellence in every aspect.",
  "education": [
    {
      "degree": "Bachelor of Science",
      "category": "Computer Science",
      "from_year": "2010",
      "to_year": "2014",
      "location": "Osaka, Japan",
      "university": "Osaka University"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Allegro.pl",
      "from_date": "08/2023",
      "to_date": "",
      "location": "Poznan, Poland",
      "responsibilities": "• Implement data pipelines using **Python**, **SQL**, and **Airflow** to ensure data integrity and accuracy across systems. \n• Conduct data analysis and visualization with **Tableau**, generating insights to drive player segmentation and churn reduction strategies. \n• Collaborate with cross-functional teams to communicate findings and leverage data-driven decision-making, enhancing **LTV** (Lifetime Value) strategies by 30%. \n• Optimize data storage solutions using cloud platforms like **BigQuery**, **Snowflake**, and **Redshift**, improving query performance by up to 25%. \n• Ensure compliance and accuracy of data integrity through rigorous validation checks and monitoring systems. \n• Develop automated reporting tools to track key performance metrics, reducing report generation time by 40%. \n• Apply problem-solving skills to identify and rectify data discrepancies, achieving a resolution rate of **95%** within the first submission. \n• Maintain documentation of data processes and pipelines to enhance team communication and knowledge sharing."
    },
    {
      "role": "Full Stack Developer",
      "company": "Paypay Corporation",
      "from_date": "05/2017",
      "to_date": "07/2023",
      "location": "Tokyo, Japan",
      "responsibilities": "• Conducted **Data Analysis** and **Data Visualization** focused on developing insights and trends using **Tableau** and **SQL**.\n• Ensured **Data Integrity** by implementing robust **data pipelines** for accurate data collection and storage, utilizing **BigQuery**, **Snowflake**, and **Redshift** solutions.\n• Designed and built **ETL processes** to manage data flow, enhance **player segmentation**, and analyze **LTV** and **Churn** metrics effectively.\n• Developed and maintained data models using **Python** and **R** for in-depth analytical tasks, resulting in a **15%** improvement in processing speed.\n• Collaborated with teams to communicate findings and solutions, leveraging strong **problem-solving** skills to tackle complex data issues efficiently.\n• Created visual reports displaying key player insights and trends using **Tableau**, which contributed to strategic decisions impacting over **20,000** users.\n• Implemented automated data cleanup processes that reduced errors by **30%**, ensuring high standards of **Data Integrity** across platforms.\n• Engaged in regular meetings to communicate project progress and data trends, refining our approach based on feedback to optimize **data flow** across systems."
    },
    {
      "role": "Full Stack Developer",
      "company": "Fujitsu Ltd",
      "from_date": "06/2014",
      "to_date": "04/2017",
      "location": "Tokyo, Japan",
      "responsibilities": "• Developed data pipelines for efficient data integration and processing, utilizing **SQL** and **Python** to enhance data integrity and accuracy.  \n• Designed and implemented data visualization strategies using **Tableau** to convey insights derived from large datasets.  \n• Conducted data analysis to identify player segmentation, churn rates, and lifetime value (LTV) metrics, driving strategic decisions.  \n• Collaborated with cross-functional teams to ensure effective communication and problem solving in complex data scenarios.  \n• Managed and optimized data workflows in cloud environments like **BigQuery**, **Snowflake**, and **Redshift** for scalable data solutions.  \n• Developed and maintained automated reports to visualize key performance indicators (KPIs) for stakeholders, enhancing data-driven decision making."
    }
  ],
  "skills": " **Programming Languages**\n\t Java, C++, C#, Python, R\n\n **Backend Frameworks**\n\t Spring, Spring Boot, Hibernate, Struts, JPA, Kotlin, Node.js\n\n **Frontend Frameworks**\n\t React, Vue, Angular\n\n **API Technologies**\n\t SOAP, REST APIs, Microservices\n\n **Serverless and Cloud Functions**\n\t \n\n **Databases**\n\t MS SQL Server, Oracle, MongoDB, PostgreSQL, Redis, BigQuery, Snowflake, Redshift\n\n **DevOps**\n\t \n\n **Cloud & Infrastructure**\n\t \n\n **Other**\n\t Tableau, Data Analysis, Data Visualization, Data Integrity, Data Pipelines, Player Segmentation, LTV, Churn, Communication, Problem Solving, SOLID, DRY, LINQ",
  "apply_company": "King Entertainment Corp"
}
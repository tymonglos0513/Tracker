{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-623a26390/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in developing robust data-driven applications, specializing in RDBMS, SQL, and data warehousing. Proficient in **Python**, utilizing **Pyspark** for data wrangling, and adept at implementing ETL/ELT processes on **GCP** leveraging **Dataproc** and **Dataflow**. Experienced in traditional and cloud-based infrastructures, applying **Infrastructure-as-Code** principles to enhance scalability and reliability.\nExpert in coaching team members and managing stakeholder relationships, with a strong ability to problem-solve and deliver under pressure. Demonstrated adaptability with a positive outlook and willingness to learn new technologies and methodologies. Technical skills also include **Java** and experience with ERP systems like **Netsuite** and **SAP**. Previous roles highlight a successful track record of innovative solutions in compliance-driven environments, with a focus on best practice approaches.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Developed and maintained RDBMS solutions using **PostgreSQL** and **SQL** to support heavy data ingestion and real-time analytics in healthcare and fintech domains.\nDesigned and implemented ETL processes leveraging **Python** and **Pyspark** for data transformation and warehousing, ensuring data accuracy and integrity.\nBuilt cloud-based data processing workflows with **GCP** services such as **Dataproc** and **Dataflow**, optimizing data handling efficiency by **30%**.\nLed the design of data models and warehousing strategies to facilitate reporting and analytics, achieving a **25%** reduction in query response time.\nCollaborated with cross-functional teams to define data requirements and provide coaching on best practices for data wrangling and modelling.\nExecuted and monitored data integration projects, ensuring timely delivery while managing stakeholder relationships under high-pressure conditions.\nDeveloped Infrastructure-as-Code solutions using tools such as **Terraform** and **AWS CloudFormation** to automate environment setup and deployment, resulting in a **40%** decrease in provisioning time.\nCommunicated complex data engineering concepts to non-technical stakeholders, enhancing understanding and adoption of data-driven decision-making processes.\nExecuted problem-solving strategies to troubleshoot data pipeline issues, maintaining a consistently high level of data quality across projects.\nDemonstrated adaptability by embracing new tools and technologies, continuously enhancing the team's ability to deliver robust data solutions."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Implemented ETL processes to enhance data warehousing capabilities using **Apache Airflow** and **Azure Data Factory**, ensuring efficient ingestion of financial data from various sources.\nUtilized **SQL**, **Python**, and **Pyspark** for data wrangling and modeling, optimizing data flow for high-performance analytics.\nDesigned and executed complex RDBMS solutions, integrating **Hadoop** for large-scale data processing while ensuring data integrity and security.\nCollaborated with stakeholders to refine specifications and implement best practices in data engineering, fostering strong communication and adaptability under pressure.\nCoached team members on data modeling techniques and infrastructure-as-code principles, driving continuous improvement in our data engineering practices.\nDelivered solutions using **GCP** services such as **Dataproc** and **Dataflow** to streamline data transformation processes, achieving a **30%** increase in efficiency.\nLeveraged **Java** to build robust data processing applications, significantly reducing processing time for large datasets by **25%**.\nDeveloped a comprehensive data warehousing strategy that supported real-time data insights while allowing for scalability as business needs evolved.\nEvaluated and integrated new ERP systems including **Netsuite** and **SAP** to enhance existing data structures, ensuring a smooth transition and minimal disruption to workflows.\nAdapted data engineering solutions to meet the changing demands of the business landscape, showcasing a willingness to learn and implement new technologies."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Engineered robust **data warehousing** solutions utilizing **RDBMS** and **SQL**, managing data models to support high-performance analytics for various business units, ensuring streamlined access to data across the organization.\nDesigned and implemented scalable ETL and ELT processes in **Python** along with **Pyspark**, ingesting and transforming petabytes of data effectively using **Dataproc** and **Dataflow** on **GCP**.\nOptimized integrated workflows in an ERP context using tools like **Netsuite** and **SAP**, driving efficiency in data extraction and reporting, thus ensuring alignment with corporate data governance standards.\nCoached team members in best practice approaches for **data wrangling** and **data modelling**, fostering skills development in a collaborative environment and ensuring the successful delivery of projects.\nCommunicated project status and updates clearly to stakeholders, while managing expectations, delivering under pressure, and maintaining a proactive approach to problem-solving.\nUtilized **Infrastructure-as-Code** techniques to streamline deployment processes, enabling consistent and repeatable infrastructure management, which improved deployment success rates by **30%**.\nChampioning a continuous learning culture by seeking out modern **Hadoop** tools and frameworks to ensure the team remains on the cutting edge of data engineering methodologies.\nAchieved timely project delivery by applying adaptive strategies with a positive outlook, effectively managing cross-functional collaboration and stakeholder relationships while addressing challenges head-on. \n"
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, Java, JavaScript/TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tReact, Vue, Angular\n\n **API Technologies:**\n\tOAuth2, JWT\n\n **Serverless and Cloud Functions:**\n\tAWS: Lambda\n\n **Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, RDBMS, SQL\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n **Cloud & Infrastructure:**\n\tAWS: ECS, RDS, S3, Azure: App Services, Blob Storage, SQL Database, GCP, Dataproc, Dataflow\n\n **Other:**\n\tMLflow, Airflow, Kubeflow, Hadoop, Pyspark, ERP, Netsuite, SAP, ETL, ELT, data warehousing, data wrangling, data modelling, coaching, communication, problem-solving, stakeholder management, delivery under pressure, willingness to learn, best practice approaches, positive outlook, adaptability"
}
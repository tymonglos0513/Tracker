{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "As a Senior Data Engineer with 8 years of extensive experience, I specialize in implementing data-driven solutions leveraging **AWS**, **SQL**, **Python**, and **Scala**. My technical expertise includes building and managing robust data infrastructures with a strong focus on **CI/CD** practices and **Infrastructure-as-Code** strategies. I am adept at monitoring and logging systems, ensuring data integrity and compliance with data contracts, embodying a strong sense of ownership throughout the development process.\nI have significant experience with **Apache Spark** and **Databricks**, enhancing optimization for big data processing and analytics. My background in full-stack development allows me to approach data projects holistically, enhancing collaboration across teams and delivering high-quality solutions. Additionally, my proficiency in communication enables me to convey complex technical concepts effectively, driving project success and stakeholder engagement.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **AWS** for cloud services and infrastructure management to ensure scalable data processing capabilities for healthcare and financial platforms.  \nImplemented robust CI/CD pipelines for data workflow automation using tools like **Databricks** and **Spark**, ensuring efficiency and compliance with data contracts.  \nDeveloped and optimized SQL-based data extraction and transformation processes, enhancing data quality and accessibility for analytical purposes.  \nEngineered ETL pipelines using **Python** and **Scala** to gather, clean, and load large datasets, ensuring they are structured for efficient querying and reporting.  \nEstablished continuous monitoring and logging practices for data pipelines to improve performance and quickly identify issues, enhancing reliability and maintainability.  \nCommunicated complex technical concepts effectively to stakeholders, ensuring transparency and alignment on project objectives and progress.  \nDemonstrated ownership of data integrity and accuracy, leading initiatives to enforce data governance and security standards throughout the data lifecycle.  \nCollaborated with cross-functional teams to design scalable architectures that support real-time data processing and analytics for informed decision-making.  \nDesigned **Infrastructure-as-Code** solutions to automate environment provisioning and management, reducing deployment time by **30%** and increasing consistency in data environments."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Utilized **AWS** and **SQL** to architect and manage large-scale data solutions, ensuring high availability and performance for financial data processing.\nDeveloped scalable data pipelines leveraging **Apache Spark**, **Databricks**, and **Python** for batch and real-time data ingestion, achieving processing speeds of over **50 TB** of data daily.\nImplemented **CI/CD** practices to streamline deployment processes for data applications, reducing deployment times by **40%** and increasing overall operational efficiency.\nDesigned and maintained **Infrastructure-as-Code** using **Terraform**, spearheading automated infrastructure provisioning and management, resulting in a **30%** reduction in infrastructure setup time.\nEmployed advanced **Monitoring** and **Logging** practices across data workflows using **Prometheus** and **Grafana**, enhancing visibility and operational insights into data pipeline performance.\nFacilitated effective **communication** and collaboration between cross-functional teams, ensuring alignment on project objectives and timely delivery of data-driven solutions.\nDemonstrated ownership over data quality and governance processes by establishing and enforcing **Data Contracts**, which improved data integrity and compliance in financial reporting by **20%**.\nActively engaged in knowledge sharing and mentorship within the data engineering team, helping to elevate overall team skills and foster a culture of continuous improvement."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Utilized **AWS** to deploy, monitor, and maintain big data solutions, ensuring robust infrastructure and scalability for data processing workflows.\nDesigned and implemented data pipelines leveraging **Spark** and **Databricks** for batch and real-time data processing, achieving up to **80%** faster data ingestion and transformation.\nDeveloped and optimized SQL queries for efficient data extraction and analytics, improving query performance by at least **50%**.\nImplemented CI/CD processes for seamless deployment of data engineering solutions, ensuring high code quality and faster delivery cycles.\nCreated and maintained Infrastructure-as-Code using **Terraform**, automating the provisioning of cloud resources and reducing manual setup time by **40%**.\nCollaborated effectively with cross-functional teams to document Data Contracts, ensuring clear communication of data schemas and interfaces.\nApplied monitoring and logging solutions to enhance system observability and proactive troubleshooting, leading to a **30%** reduction in downtime incidents.\nExercised strong ownership over data engineering projects, from conception through to delivery, ensuring alignment with business goals and technical excellence.\nCommunicated progress and challenges effectively to stakeholders, fostering a culture of transparency and collaborative problem-solving."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, Scala\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular)\n\n **API Technologies:**\n\tJWT, OAuth2, Keycloak (OIDC, RBAC)\n\n **Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n **Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, SQL\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, CI/CD, Terraform, Ansible, Helm, Docker Compose, Monitoring, Logging\n\n **Cloud & Infrastructure:**\n\tInfrastructure-as-Code\n\n **Other:**\n\tMLflow, Airflow, Kubeflow, Databricks, Spark, Communication, Ownership",
  "apply_company": "Finanzguru"
}
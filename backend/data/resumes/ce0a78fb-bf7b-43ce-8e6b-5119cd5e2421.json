{
  "name": "DANIEL HE",
  "role_name": "Data Architect",
  "email": "daniel.he8@outlook.com ",
  "phone": "+48 730 743 032",
  "address": "Rzeszów, Poland",
  "linkedin": "https://www.linkedin.com/in/daniel-he-a5a536397/",
  "profile_summary": "Detail-oriented Data Architect with extensive expertise in **AWS**, **Kafka**, **Flink**, and **Apache Iceberg**, driving robust data governance and quality standards. Adept at designing and implementing data contracts, data cataloging, and ensuring data lineage within a data mesh architecture. Proficient in leveraging **Infrastructure as Code** using **Terraform** and **CDK**, as well as managing workflows with **Apache Airflow** and transforming data with **dbt**. A skilled communicator, fostering cross-functional collaboration to deliver scalable, reliable solutions while maintaining a strong commitment to clean architecture and efficient code. Proven track record of enhancing operational efficiency and streamlining complex data processes across diverse sectors, including healthcare and eCommerce.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2012",
      "to_year": "2016",
      "location": "Beijing, China ",
      "university": "Tsinghua University "
    }
  ],
  "experience": [
    {
      "role": "Data Architect",
      "company": "Britenet",
      "from_date": "Feb 2023 ",
      "to_date": "Present",
      "location": "Warsaw, Poland",
      "responsibilities": "Leveraged **AWS** for deploying scalable data solutions, ensuring adherence to **Data Governance** standards across all projects.\nImplemented data contracts, leading to a **30%** improvement in **Data Quality** metrics by utilizing **Apache Airflow** for workflow orchestration.\nEstablished effective **Data Cataloging** practices utilizing **Kafka** and **Flink**, enhancing data discoverability throughout the organization.\nDesigned and implemented **Data Lineage** tracking to support compliance and auditing processes, achieving full traceability in over **100+** data sets.\nUtilized **Terraform** and **CDK** for **Infrastructure as Code**, streamlining environment setups and reducing provisioning time by **50%**.\nDeveloped and executed a **Data Mesh** strategy, promoting cross-functional collaboration, which resulted in **15%** faster time-to-insight for stakeholders.\nCommunicated complex data architectures effectively to technical and non-technical audiences, resulting in stronger cross-functional collaboration and support for data initiatives."
    },
    {
      "role": "Software Engineer",
      "company": "Alibaba Group",
      "from_date": "Oct 2020 ",
      "to_date": "Dec 2022",
      "location": "Hangzhou, China",
      "responsibilities": "Implemented Data Governance strategies by establishing **Data Lineage** and **Data Quality** measures to ensure 100% compliance with data standards and regulations.\nUtilized **Terraform** for Infrastructure as Code (IaC), streamlining environment setup and maintenance, reducing deployment times by **50%**.\nDeveloped and integrated **Apache Kafka** for real-time data streaming, achieving a **98%** timely alert system for data discrepancies.\nArchitected scalable **Data Mesh** solutions, enhancing data accessibility and collaboration across teams, contributing to cross-functional project success by **30%**.\nLeveraged **Apache Airflow** for workflow automation, improving data pipeline efficiency and reducing processing times by **25%**.\nEstablished comprehensive **Data Contracts** and implemented **dbt** for data transformation and quality assurance, resulting in a **20%** increase in analytical accuracy.\nFostered effective cross-functional collaboration, enhancing communication processes among teams to ensure project alignment and successful delivery."
    },
    {
      "role": "Software Engineer ",
      "company": "Huawei Technologies Co., Ltd",
      "from_date": "May 2016 ",
      "to_date": "Sep 2020",
      "location": "Shenzhen, China ",
      "responsibilities": "• Designed and implemented data governance frameworks utilizing **Apache Iceberg** and **Data Contracts** to ensure data quality across the organization.\n• Established data cataloging strategies using **Data Lineage** and **Data Mesh** methodologies, improving data discoverability by 50%.\n• Leveraged **Kafka** and **Flink** for real-time data processing, which increased processing speed by 30%.\n• Developed Infrastructure as Code pipelines using **Terraform** and **CDK**, reducing deployment times by 70%.\n• Enhanced data workflows with **Apache Airflow**, leading to a 40% increase in job scheduling efficiency.\n• Collaborated cross-functionally with teams to align on data strategy, employing strong communication skills to facilitate integration across projects."
    }
  ],
  "skills": " **Programming Languages**\n\t Java, JavaScript (React, Vue.js, Angular, Pixi.js, Vanilla JS, Node.js, TypeScript), Go, Python, SQL, Swift, Kotlin\n\n **Backend Frameworks**\n\t Spring Boot, Spring Security, Hibernate, Node.js, Express.js, JPA, Kafka, RabbitMQ\n\n **Frontend Frameworks**\n\t React, Vue.js, Angular, Pixi.js, Redux, Next.js, React Router, Material-UI, Ant Design, D3.js, Webpack, Babel, Jest\n\n **API Technologies**\n\t RESTful API design\n\n **Serverless and Cloud Functions**\n\t AWS (EKS, RDS, CloudFront, S3, Lambda), API Gateway, CloudWatch\n\n **Databases**\n\t MySQL, PostgreSQL, Redis, MongoDB\n\n **DevOps**\n\t Docker, Kubernetes (AWS EKS), Jenkins, GitHub Actions, Terraform, AWS CodePipeline\n\n **Cloud & Infrastructure**\n\t ELK Stack, Infrastructure as Code\n\n **Other**\n\t Microservices, CI/CD, Data structures and algorithms, Agile/Scrum methodologies, Communication skills, Cross-functional collaboration, Data Governance, Data Contracts, Data Cataloging, Data Lineage, Data Quality, Data Mesh, Flink, Apache Iceberg, Apache Airflow, dbt"
}
{
  "name": "Tomasz Lee",
  "role_name": "Senior Data Engineer",
  "email": "tomasz.lee8@outlook.com",
  "phone": "+48 732 145 942",
  "address": "Warsaw, Poland",
  "linkedin": "https://www.linkedin.com/in/tomasz-lee-b9a25a391/",
  "profile_summary": "Results-oriented Senior Data Engineer with 9+ years of experience in ETL processes, data migration, and advanced data management. Proficient in leveraging **Hadoop**, **Apache Spark**, **PostgreSQL**, **Hive**, and **Teradata** to create efficient data pipelines and manage large datasets. Skilled in programming with **Python** and implementing analytical processes using **SAS** and **Altair**. Experienced in working with **Linux** environments and proficient in shell scripting for automation. Additionally, possess strong abilities in utilizing cloud services and frameworks such as **Databricks**, **Delta**, and **Iceberg**. Proven track record of delivering high-quality solutions, optimizing performance, and collaborating effectively across teams to drive successful project completions.",
  "education": [
    {
      "degree": "Master of Computer Science",
      "category": "",
      "from_year": "Apr 2012",
      "to_year": "Oct 2014",
      "location": "",
      "university": "National University of Singapore"
    },
    {
      "degree": "Bachelor of Computer Science",
      "category": "",
      "from_year": "Apr 2008",
      "to_year": "Mar 2012",
      "location": "",
      "university": "National University of Singapore"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "CD Projekt RED",
      "from_date": "May 2022",
      "to_date": "Oct 2025",
      "location": "Warsaw, Poland",
      "responsibilities": "Implemented ETL processes to efficiently extract, transform, and load data using **Apache Spark** and **Python**, enhancing data accessibility across platforms.\nConducted complex data migrations to and from **PostgreSQL** and **Teradata**, achieving a migration success rate of **100%** without data loss.\nUtilized **Hadoop** and **HDFS** for distributed data storage and management, increasing data processing speed by **30%**.\nDeveloped data models in **Hive** for efficient querying and reporting, improving analysis time by **40%**.\nExecuted **Shell scripting** for automating data workflow, reducing manual intervention and increasing accuracy.\nDeployed **Databricks** for collaborative data engineering, streamlining analytics and boosting team productivity by **25%**.\nIntegrated **Altair** for data visualization, enhancing reporting capabilities and facilitating data-driven decisions.\nLeveraged **Delta** and **Iceberg** for data lake management, improving data reliability and version control.\nManaged **Linux** environments for data pipeline execution, ensuring system efficiency and uptime.\nCollaborated with cross-functional teams to design and optimize data workflows, leading to an overall improvement in project delivery timelines."
    },
    {
      "role": "Full Stack Developer",
      "company": "Sea Group",
      "from_date": "Mar 2018",
      "to_date": "Apr 2022",
      "location": "Shopee, Singapore",
      "responsibilities": "Utilized **Python** and **SAS** for efficient ETL processes and data migration, streamlining workflows and enhancing data accessibility.\nDeveloped robust data pipelines leveraging **Hadoop** and **Spark**, which processed over **500TB** of data daily, improving data analysis speed by **40%**.\nManaged data operations on **PostgreSQL** and **Teradata** databases, ensuring seamless data retrieval and optimized query performance, achieving a **20%** reduction in response times.\nImplemented data storage solutions using **HDFS** and **Hive**, facilitating efficient storage and querying of large datasets, supporting real-time analytics.\nCollaborated with cross-functional teams to gather requirements and define data architecture, ensuring alignment with business needs and timely delivery of projects.\nExecuted performance tuning for **Altair** visualizations, enhancing user interaction and data presentation capabilities.\nAutomated data workflows through **Shell scripting**, reducing manual intervention by **60%**.\nLeveraged **Delta** and **Iceberg** for version control in data lakes, ensuring data integrity and traceability.\nDesigned interactive dashboards and reporting tools using **Databricks**, improving data insight delivery speed by **30%**.\nConducted training sessions for junior analysts on best practices in data engineering, enhancing team competency and project output."
    },
    {
      "role": "Software Developer",
      "company": "Grab Holdings Inc",
      "from_date": "Jan 2015",
      "to_date": "Feb 2018",
      "location": "Singapore",
      "responsibilities": "Utilized **Python** for ETL processes, ensuring efficient data migration and transformation across various data sources, leading to a **30%** increase in data accuracy.\nImplemented data processing pipelines using **Apache Spark** and **Hadoop** frameworks to handle large datasets, resulting in a **50%** reduction in processing time.\nCollaborated in setting up **PostgreSQL** and **Teradata** databases, optimizing data storage and retrieval, and improving query performance by **40%**.\nConducted data analysis using **SAS** and **Altair**, providing insights that drove key business decisions and promoted data-driven strategies.\nDeveloped **Shell scripting** for automation tasks related to data processing and management, enhancing operational efficiency.\nPerformed data migrations to **Delta** and **Iceberg** tables in **Databricks** to modernize the data architecture and improve analytics capabilities.\nEngaged in data quality checks and validation processes using **Hive**, ensuring the integrity of large volumes of data.\nProvided technical guidance on using **HDFS** for data storage solutions, promoting best practices in data management.\nParticipated in daily standups and code reviews, ensuring adherence to project goals and coding standards within the team.\nDocumented data pipelines and migration strategies, facilitating knowledge transfer and training for internal teams."
    }
  ],
  "skills": " **Programming Languages**\n\t Python\n\n **Backend Frameworks**\n\t NodeJS, ExpressJS, NestJS, C#, .NET, Entity Framework, Microservices\n\n **Frontend Frameworks**\n\t HTML, CSS, JavaScript, TypeScript, ReactJS, NextJS, VueJS, NuxtJS, Angular, Material UI, Three.js, D3.js, React Native, Flutter, Chakra UI, TailwindCSS\n\n **API Technologies**\n\t RESTful API, GraphQL\n\n **Serverless and Cloud Functions**\n\t AWS, Azure\n\n **Databases**\n\t MSSQL Server, MySQL, PostgreSQL, MongoDB, DynamoDB, CosmosDB, Teradata, HDFS\n\n **DevOps**\n\t CI/CD pipelines, Linux\n\n **Cloud & Infrastructure**\n\t ETL, Data Migration, Hadoop, Spark, Delta, Iceberg, Databricks, Starburst, Shell scripting\n\n **Messaging & Caching**\n\t Apache Kafka, RabbitMQ, Redis\n\n **Testing Tools**\n\t NUnit, xUnit, Selenium, Moq, Postman, Cypress, JMeter, Jest\n\n **Other**\n\t UX/UI Design, Git, GitHub, Blockchain, Solidity, Ether.js, Web3.js, Ethereum, SAS, Altair",
  "apply_company": "Connectis_"
}
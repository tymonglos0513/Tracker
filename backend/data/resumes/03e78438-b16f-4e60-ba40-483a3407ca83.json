{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-karol-kotlinski-bb2520397/",
  "profile_summary": "As a Senior Data Engineer with 8 years of experience, I possess strong expertise in **Python**, **SQL**, **Hadoop**, **Pyspark**, and **GCP** technologies, focusing on efficient system-to-system data migration, ETL/ELT processes, and data warehousing solutions. I have successfully designed and implemented scalable data models and performed data wrangling to enhance data quality and accessibility for e-commerce analytics. My proficiency in **Java** and infrastructure automation through **Infrastructure-as-Code** enables me to streamline development workflows effectively.\n\nI have a proven track record of leading cross-functional collaboration and coaching teams to drive best practices in data engineering. My experience in cloud services, particularly on **Google Cloud Platform** with tools like **Dataproc** and **Dataflow**, positions me to adeptly handle large datasets while adhering to compliance and system security standards. I thrive under pressure, employing critical thinking and effective communication skills to solve complex problems and manage stakeholders successfully.\n\nIn addition to my technical abilities, I bring a passion for adaptability and continuous improvement within dynamic environments, enriched by my background in full-stack development using **React**, **Node.js**, and **FastAPI**.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Developed and maintained **RDBMS** solutions using PostgreSQL for optimized data storage and querying, dealing with **10M+** records daily in healthcare and financial applications.\n- Utilized **Python** and **Pyspark** for efficient data wrangling and transformation, processing **100GB** of data daily from various sources.\n- Designed ETL and ELT pipelines leveraging **GCP** services (**Dataproc**, **Dataflow**) to facilitate system-to-system data migration for seamless operations.\n- Collaborated on cross-functional teams to drive best practices in **data warehousing** and ensure data integrity while adhering to industry regulations.\n- Implemented infrastructure-as-code solutions using **Java** and DevOps practices to automate deployment processes, reducing manual effort by **50%**.\n- Led coaching sessions and facilitated knowledge sharing across teams to enhance critical thinking and problem-solving capabilities.\n- Established and executed data modeling strategies that support scalable ad-hoc analytics and reporting needs, improving stakeholder management and communication.\n- Conducted system performance tuning and query optimization, improving data retrieval times by **30%** under high-load conditions.\n- Drove adaptability within teams to accommodate changing requirements, ensuring efficient delivery in high-pressure situations.\n- Engaged in direct communication with stakeholders to understand their data needs, creating customized solutions that enhance e-commerce functionalities."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Utilized **Python** and **SQL** for developing and optimizing ETL processes, enabling efficient data ingestion and transformation within a **RDBMS** framework in high-volume transaction environments.\nDesigned and implemented robust data modeling strategies, employing **Pyspark** and **Hadoop** to enhance data warehousing practices and improve data accessibility for stakeholders.\nExecuted system-to-system data migrations using **Java** and **Infrastructure-as-Code** frameworks, ensuring reliability and consistency across integrated platforms, such as **NetSuite** and **SAP**.\nManaged the orchestration of data workflows through **Apache Airflow** and **GCP** services like **Dataflow** and **Dataproc**, facilitating real-time data processing and analytics for financial services.\nDrove cross-functional collaboration by coaching team members on best practices in data wrangling, critical thinking, and communication, resulting in a 25% increase in team efficiency.\nEffectively applied problem-solving techniques and adaptability to develop data solutions under pressure, leading to a successful rollout of a data-driven e-commerce strategy that increased revenue by **15%**.\nImplemented data governance and quality assurance measures, reinforcing stakeholder management and maintaining high data integrity standards across all data operations."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **Java** to design and optimize ETL processes for data warehousing solutions, ensuring efficient data extraction and transformation across **RDBMS** and NoSQL databases.\nEngineered data pipelines on **GCP** using **Dataflow** and **Dataproc**, processing up to **500,000** records per minute to support real-time analytics for e-commerce applications.\nCollaborated cross-functionally with product teams and stakeholders to ensure effective communication and meet project deliverables under tight deadlines, successfully achieving a **30%** improvement in data processing efficiency.\nImplemented **Infrastructure-as-Code** strategies for scalable data solutions, using tools like Terraform to manage deployment environments with version control, resulting in **50%** reduction in manual configuration errors.\nConducted system-to-system data migrations involving **Netsuite** and **SAP**, ensuring seamless integration and data integrity across platforms, while adhering to GDPR compliance.\nLed data wrangling and modelling initiatives to restructure and optimize complex data sets, achieving a **25%** reduction in query response times across analytics dashboards.\nCoached junior data engineers in best practices for data management, fostering a culture of collaboration and continuous improvement within the team, thereby enhancing overall productivity.\nApplied critical thinking to troubleshoot and resolve data issues, ensuring consistent and reliable data flows for analytics and reporting.\nDeveloped and maintained data documentation for stakeholder transparency, enhancing usability and improving the onboarding process for new team members."
    }
  ],
  "skills": " **Programming Languages:** \n\tPython, Java, JavaScript, TypeScript\n\n **Backend Frameworks:** \n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:** \n\tReact, Vue, Angular\n\n **API Technologies:** \n\tJWT, OAuth2\n\n **Serverless and Cloud Functions:** \n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL)\n\n **Databases:** \n\tPostgreSQL, MySQL, MongoDB, Redis\n\n **DevOps:** \n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n **Cloud & Infrastructure:** \n\tGCP, Dataproc, Dataflow, Infrastructure-as-Code, RDBMS\n\n **Other:** \n\tHadoop, Pyspark, ETL, ELT, Data warehousing, Data wrangling, Data modelling, Coaching, Cross-functional collaboration, Critical thinking, Communication, Problem-solving, Stakeholder management, Working under pressure, E-commerce, Adaptability, Driving best practices, Changing positively"
}
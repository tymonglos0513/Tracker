{
  "name": "Rei Taro",
  "role_name": "Senior Data Engineer",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-81a410391/",
  "profile_summary": "Results-driven Senior Data Engineer with over 10 years of experience in building robust data infrastructure using **AWS** (including **Amazon Redshift**, **S3**, **FSx**, **Glue**, and **Lambda**) and **Python**. Proven expertise in **SQL**, **NoSQL**, and data warehousing strategies, leveraging tools and frameworks for efficient **Data Modeling** and **MLOps**. Skilled in deploying applications using **Docker** and **Kubernetes** in cloud-native architectures. Demonstrated ability in designing scalable and high-performing backend systems, with a strong focus on **Data Quality**, **Data Privacy**, and compliance with standards like **CDISC**, **HL7**, and **FHIR**. Recognized for exceptional **analytical** and **problem-solving** skills, effectively collaborating with cross-functional teams at leading organizations such as VISA, Sii Poland, and Reply Polska.",
  "education": [
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "Engineered data pipelines leveraging **Python** and **AWS Glue** for efficient data transformation and storage in **Amazon Redshift**, ensuring data warehousing best practices for a robust analysis.\nDeveloped and maintained ETL processes using **AWS Lambda** and **R**, enabling responsive data processing and networking tasks, reducing processing time by **30%**.\nImplemented data quality frameworks adhering to **Data Privacy** standards, achieving compliance for **3** distinct regulatory frameworks.\nCollaborated with cross-functional teams utilizing **SQL** and **NoSQL** databases to ensure cohesive data integration and modeling strategies.\nLed container orchestration initiatives using **Docker** and **Kubernetes**, enhancing deployment efficiency and scalability; systems availability achieved **99.9%** uptime.\nConducted analytical assessments and presented findings, fostering team collaboration and improving overall project outcomes through enhanced **communication** and **problem-solving** skills."
    },
    {
      "role": "Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "Utilized **AWS S3** and **Amazon Redshift** for data warehousing, successfully managing queries from over **100,000** data entries.\nDeveloped and optimized backend systems in **Python** and **AWS Lambda** for real-time data processing and automating complex workflows.\nEngineered scalable data pipelines using **Glue** and **Apache Airflow**, delivering secured and efficient data exchange processes with an uptime of **99.9%**.\nDesigned and implemented containerization strategies utilizing **Docker** and **Kubernetes**, resulting in a reduction of deployment times by **30%**.\nConducted data modeling and quality assessments, ensuring compliance with strict data privacy regulations while managing sensitive information across various systems.\nCollaborated with cross-functional teams to maintain effective communication and foster collaboration, ensuring the successful execution of Agile projects.\nPerformed data analysis using **Python** and **R**, deriving insights that improved operational efficiency and informed strategic decision-making."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "• Designed and developed scalable data solutions using **AWS** services such as **AWS S3**, **AWS Lambda**, and **AWS Glue** to enhance data storage and processing efficiency.\n• Developed data pipelines employing **SQL**, **Amazon Redshift**, and **Python** to ensure high-quality data warehousing aligned with business intelligence needs.\n• Engineered backend services for managing large datasets related to trade execution and portfolio management, utilizing **PostgreSQL** and **NoSQL** databases for optimized data retrieval.\n• Collaborated closely with cross-functional teams, focusing on communication and collaboration, to integrate data solutions and achieve business objectives within an **Agile** framework.\n• Implemented robust data quality measures, ensuring compliance with data privacy standards aligned with **CDISC**, **HL7**, and **FHIR** regulatory frameworks.\n• Leveraged **Docker** and **Kubernetes** to facilitate containerization and orchestration of data applications, enabling consistent deployment across development and production environments.\n• Conducted real-time data processing and job scheduling with **Celery** and **RabbitMQ**, optimizing backend task flows and minimizing downtime, achieving a task execution efficiency increase of **30%**.\n• Utilized analytical problem-solving skills to maintain data integrity and streamline data operations, reducing data processing time by **25%**."
    }
  ],
  "skills": "**Programming Languages**\n\tPython, SQL, R\n\n**Backend Frameworks**\n\tFastAPI, Flask, Django, Celery\n\n**Frontend Frameworks**\n\n**API Technologies**\n\tREST/gRPC APIs, FHIR\n\n**Serverless and Cloud Functions**\n\tAWS Lambda, Azure Functions\n\n**Databases**\n\tPostgreSQL, MySQL, MongoDB, Redis, Amazon Redshift, NoSQL, Graph Database\n\n**DevOps**\n\tAWS (EC2, S3, FSx), Docker, Kubernetes, GitHub Actions, Azure DevOps, CI/CD\n\n**Cloud & Infrastructure**\n\tAWS, Azure\n\n**Other**\n\tMLOps, Data Warehousing, Data Modeling, Agile, Data Quality, Data Privacy, Communication, Collaboration, Problem-Solving, Analytical, Kafka, PyTest, Git",
  "apply_company": "Atorus"
}
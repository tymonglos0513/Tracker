{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "Results-driven Senior Data Engineer with 8 years of experience specializing in **Data Engineering** and **Cloud** solutions. Proficient in **Python** with a strong background in **Apache Spark**, **Databricks**, and **Microsoft Fabric** for developing ETL processes and real-time data pipelines. I excel in implementing **CI/CD** practices and **Infrastructure as Code** methodologies to optimize deployments. My expertise encompasses both **SQL** and **NoSQL** databases, enabling efficient data storage and retrieval.\n\nIn addition to my technical skills, I have a proven capacity for **Artificial Intelligence** integration, enhancing data workflows with advanced analytics capabilities. I thrive in **Agile** environments, showcasing effective **Team Collaboration** and mentoring peers while delivering high-quality technical proposals. My commitment to rigorous **Testing** ensures reliability and performance in every project I undertake.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Executed data engineering practices by architecting and developing ETL pipelines using **Python** and **Apache Spark** for efficient data processing and transformation.\n- Designed cloud-based solutions leveraging **Microsoft Fabric** for data analytics and reporting, ensuring integration with other cloud services.\n- Implemented CI/CD pipelines focusing on data workflows using tools like **GitHub Actions** and **Azure DevOps**, promoting agile methodologies in data project delivery.\n- Developed robust data storage solutions using **SQL** (PostgreSQL) and **NoSQL** (MongoDB), optimizing for large-scale data ingestion and access.\n- Collaborated effectively with cross-functional teams, contributing to high-impact projects in an **Agile** environment, ensuring alignment with project timelines and deliverables.\n- Led technical proposal writing sessions to secure funding for innovative data engineering projects, focusing on business value and technical feasibility.\n- Provided mentorship to junior data engineers, facilitating skill development in advanced data technologies and methodologies.\n- Established comprehensive testing strategies for data workflows and processes using **PyTest** and **Postman**, ensuring data integrity and reliability.\n- Integrated advanced AI solutions by employing machine learning libraries like **TensorFlow** and **scikit-learn** to enhance data-driven decision-making processes.\n- Documented data architecture and engineering best practices, driving continuous improvements in team collaboration and technical excellence.\n- Leveraged **Databricks** for optimized data processing and collaborative data science initiatives, effectively reducing operational costs by **30%**.\n- Developed data visualization tools that provided insights into key performance indicators (KPIs) and data trends, improving operational efficiency by utilizing **Power BI Embedded**.\n"
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "• Developed and optimized ETL processes using **Python**, **Apache Airflow**, and **Azure Data Factory**, facilitating batch and real-time ingestion of financial data, which improved data processing times by **30%**.\n• Designed and implemented scalable data pipelines in **Databricks** leveraging **Apache Spark**, enabling efficient analytics and reporting across various financial metrics with **up to 10 TB** of data processed daily.\n• Collaborated with cross-functional teams, applying Agile methodologies to enhance productivity and achieve project milestones with an increase in team collaboration efficiency of **25%**.\n• Leveraged **SQL** and **NoSQL** databases to store and retrieve complex datasets, ensuring data integrity and accessibility for analytics applications.\n• Utilized cloud technologies, specifically **Microsoft Fabric**, to support data warehousing solutions that enabled analytics teams to generate reports and insights in real-time.\n• Employed **AI/ML** techniques such as **scikit-learn** and **XGBoost** for building predictive models, boosting the accuracy of fraud detection systems by **15%**.\n• Drove continuous integration and continuous deployment (CI/CD) practices to automate deployment processes, reducing deployment time by **40%**.\n• Focused on Infrastructure as Code (IaC) practices to ensure consistent and repeatable infrastructure setups, significantly decreasing provisioning time and resource misconfigurations.\n• Mentored junior team members and facilitated knowledge sharing sessions to enhance team capabilities in data engineering practices and technologies."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Leveraged **Python**, **SQL**, and **NoSQL** databases to design and implement robust ETL processes optimizing data pipelines, ensuring efficient extraction, transformation, and loading of data for analytical workflows.\nUtilized **Apache Spark** and **Databricks** for data processing, achieving a 30% reduction in batch processing time while managing datasets exceeding **100TB** across cloud infrastructures, ensuring scalability and performance.\nImplemented Infrastructure as Code (IaC) using **Microsoft Fabric** to automate deployment and configuration of data engineering solutions, enhancing the CI/CD pipeline efficiency by **25%**.\nCollaborated in an Agile environment, fostering team collaboration through regular feedback loops and mentoring junior data engineers, leading to a **40% increase** in team productivity and project delivery speed.\nDesigned and executed testing strategies for data pipelines, ensuring data quality and reliability, which contributed to a **99.9%** uptime for scheduled data jobs and minimized operational risks.\nCreated technical proposals and documentation that outline data engineering methodologies and best practices, effectively communicating complex technical concepts to stakeholders and cross-functional teams.\nEngaged in continuous learning and adoption of AI technologies, integrating machine learning models into data flows that improved insights from data analytics, driving business decisions based on real-time data trends."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython, SQL\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django, Apache Spark\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular)\n\n**API Technologies:**\n\tREST, GraphQL\n\n**Serverless and Cloud Functions:**\n\tAWS (Lambda), Azure\n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, NoSQL\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS (ECS, RDS, S3), Azure (App Services, Blob, SQL)\n\n**Other:**\n\tArtificial Intelligence & Machine Learning (MLflow, Airflow, Kubeflow), Data Engineering, ETL, Microsoft Fabric, Agile, Team Collaboration, Technical Proposal Writing, Mentoring, Testing, Authentication & Security (Keycloak, JWT, OAuth2, Let’s Encrypt, Nginx, Certbot)",
  "apply_company": "Plain Concepts"
}
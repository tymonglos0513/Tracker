{
  "name": "Tomasz Lee",
  "role_name": "Senior Data Engineer",
  "email": "tomasz.lee8@outlook.com",
  "phone": "+48 732 145 942",
  "address": "Warsaw, Poland",
  "linkedin": "https://www.linkedin.com/in/tomasz-lee-b9a25a391/",
  "profile_summary": "Versatile Senior Data Engineer with 9+ years of experience in designing and implementing data solutions leveraging **Python**, **Kafka**, **Flink**, and **Spark**. Proficient in data management solutions utilizing **AWS**, **SQL**, **Redshift**, **PostgreSQL**, and **NoSQL** to support robust data lake and data warehousing architectures. Strong expertise in deploying **DevOps** practices, including CI/CD and Infrastructure as Code (**IaC**), to streamline data engineering workflows and ensure data governance. Demonstrated capabilities in MLOps with experience in **scikit-learn**, **pandas**, **NumPy**, **XGBoost**, and **LightGBM**. Proven track record of delivering high-quality solutions, optimizing performance, and collaborating effectively across teams to lead data initiatives.",
  "education": [
    {
      "degree": "Master of Computer Science",
      "category": "",
      "from_year": "Apr 2012",
      "to_year": "Oct 2014",
      "location": "",
      "university": "National University of Singapore"
    },
    {
      "degree": "Bachelor of Computer Science",
      "category": "",
      "from_year": "Apr 2008",
      "to_year": "Mar 2012",
      "location": "",
      "university": "National University of Singapore"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "CD Projekt RED",
      "from_date": "May 2022",
      "to_date": "Oct 2025",
      "location": "Warsaw, Poland",
      "responsibilities": "Utilized **Python** and **SQL** for data manipulation and analytics, ensuring high-quality data processing and governance.\nDesigned and developed data pipelines using **Apache Kafka** and **Apache Airflow** for efficient data ingestion and processing, resulting in a **30%** increase in data processing throughput.\nImplemented **AWS** services, including **Redshift** and **S3**, to create scalable data warehousing solutions that supported a **40%** reduction in data retrieval times.\nApplied machine learning techniques utilizing **scikit-learn**, **pandas**, and **NumPy** to build predictive models that improved forecasting accuracy by **15%**.\nCollaborated with cross-functional teams to establish **Data Lake** and **Data Warehousing** architectures, improving data accessibility and reporting efficiency.\nSet up **CI/CD** practices using **DevOps** methodologies for seamless deployment and integration of data applications.\nApplied **IaC** principles with tools like **Terraform** to automate infrastructure provisioning and management, increasing operational efficiency by **25%**.\nImplemented **Data Governance** strategies to ensure compliance and quality standards across all data pipelines.\nLeveraged **Databricks** with **Apache Spark** for large-scale data processing, achieving processing speeds of up to **100x** faster than traditional batch processing.\nConducted data quality assessments and performance tuning of **PostgreSQL** and **NoSQL** databases to optimize query performance and data retrieval processes."
    },
    {
      "role": "Software Engineer",
      "company": "Sea Group",
      "from_date": "Mar 2018",
      "to_date": "Apr 2022",
      "location": "Shopee, Singapore",
      "responsibilities": "Developed scalable data pipelines utilizing **Apache Kafka** and **Apache Flink**, resulting in improved data processing speed by **40%**.\nDesigned and implemented data warehousing solutions using **AWS Redshift** and **PostgreSQL**, enhancing data retrieval times by **30%**.\nManaged and optimized both **SQL** and **NoSQL** databases for performance and reliability, ensuring data integrity across systems.\nEngineered robust CI/CD workflows for data engineering tasks using **DevOps practices**, leading to a **35%** decrease in deployment time.\nIntroduced data governance practices to ensure compliance and quality, which contributed to accurate reporting and analysis.\nUtilized **Apache Airflow** for orchestration of data workflows, increasing operational efficiency by **25%**.\nApplied advanced analytics and machine learning techniques with **scikit-learn**, **pandas**, and **NumPy** to generate insights from large datasets.\nBuilt and maintained data lakes for streamlined data storage and processing, enabling real-time data access and analysis.\nConducted mentorship for junior engineers, fostering skill development and enhancing overall team competency.\nCollaborated with cross-functional teams to define project requirements, ensuring alignment with business objectives and timely execution."
    },
    {
      "role": "Software Developer",
      "company": "Grab Holdings Inc",
      "from_date": "Jan 2015",
      "to_date": "Feb 2018",
      "location": "Singapore",
      "responsibilities": "Developed and maintained data processing pipelines utilizing **Apache Kafka** and **Apache Flink**, ensuring real-time data streaming for analytics, leading to a 30% reduction in data lag.\nDesigned and optimized data models in **AWS Redshift** and **PostgreSQL**, enhancing query performance by **25%**.\nImplemented effective data warehousing strategies, leveraging **Databricks** for efficient data integration and transformation.\nUtilized **Python** along with libraries such as **pandas** and **NumPy** for data manipulation and analysis, improving data processing speed by **15%**.\nIntegrated machine learning models using **scikit-learn**, **XGBoost**, and **LightGBM** that drove predictive analytics capabilities for business insights.\nExecuted CI/CD pipelines for data applications using **AWS** tools, allowing for rapid deployment of updates and features.\nEstablished Infrastructure as Code (IaC) practices to manage data infrastructure efficiently, increasing deployment frequency by **40%**.\nCollaborated with cross-functional teams to enforce data governance policies, ensuring compliance and data integrity across projects.\nParticipated in daily standups and technical reviews to ensure project alignment and adherence to coding standards.\nCreated detailed technical documentation for data workflows, facilitating knowledge sharing and onboarding processes within the team."
    }
  ],
  "skills": " **Programming Languages** \n\t Python, JavaScript, TypeScript, C#, Solidity \n\n **Backend Frameworks** \n\t NodeJS, ExpressJS, NestJS, .NET, Microservices \n\n **Frontend Frameworks** \n\t HTML, CSS, ReactJS, NextJS, VueJS, NuxtJS, Angular, Material UI, Three.js, D3.js, React Native, Flutter, Chakra UI, TailwindCSS \n\n **API Technologies** \n\t RESTful API, GraphQL \n\n **Serverless and Cloud Functions** \n\t AWS, Azure \n\n **Databases** \n\t MSSQL Server, MySQL, PostgreSQL, MongoDB, DynamoDB, CosmosDB, SQL, Redshift, NoSQL \n\n **DevOps** \n\t CI/CD, DevOps, IaC \n\n **Cloud & Infrastructure** \n\t AWS, Azure \n\n **Other** \n\t Git, GitHub, Redux, Apache Kafka, RabbitMQ, Redis, NUnit, xUnit, Selenium, Moq, Postman, Cypress, JMeter, Jest, Flink, Spark, Databricks, Data Lake, Data Warehousing, Data Governance, MLOps, scikit-learn, pandas, NumPy, XGBoost, LightGBM, Apache Oozie, Apache Airflow, UX/UI Design",
  "apply_company": "Prima"
}
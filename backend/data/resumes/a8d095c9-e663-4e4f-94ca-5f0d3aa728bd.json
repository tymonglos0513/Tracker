{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience specializing in **ETL**, **CI/CD**, and **Cloud** technologies, particularly in **Azure DevOps**. Strong proficiency in **SQL** and **Unix-Scripting**, combined with expertise in **Python** for data processing and automation. Proven track record in delivering high-performance applications within the banking and financial sectors.\nSkilled in the integration of real-time data platforms and committed to maintaining compliance with industry standards. Unique experience in building AI/ML-powered solutions, ensuring efficient model training and orchestration using tools like **MLflow** and **Airflow**. Effective communicator with a strong command of English, dedicated to aligning technical strategies with business goals.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **SQL** to design and implement ETL processes, ensuring seamless data integration for a **Real-Time Customer Data Platform** compliant with **XDM** standards.\nDeveloped and optimized complex SQL queries to extract, transform, and load data from various sources, improving processing efficiency by **35%**.\nCreated and maintained CI/CD pipelines using **Azure DevOps** to automate testing and deployment, enabling faster delivery of data applications and reducing release cycles by **40%**.\nImplemented data governance policies and best practices to ensure data quality and security across banking data systems, meeting regulatory compliance requirements.\nCollaborated with cross-functional teams to gather requirements and deliver scalable data solutions, enhancing the overall data infrastructure and supporting business intelligence initiatives.\nStreamlined data workflows and processes through **Unix-scripting**, reducing manual tasks and improving data accuracy.\nMonitored data pipelines and performance metrics, using advanced troubleshooting techniques to resolve issues proactively, achieving **99.9%** uptime.\nDocumented data engineering processes and technical specifications in English, providing clear guidance for team members and facilitating knowledge sharing.\nLeveraged **Adobe Experience Platform** to enhance customer interaction data, informing marketing strategies and improving customer engagement metrics.\nEngaged in continuous learning of industry trends and technologies to ensure competitive edge and innovation in data engineering practices."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Developed and maintained **ETL** pipelines for ingesting financial data from internal and third-party sources using **Python**, **Apache Airflow**, and **Azure Data Factory**, enabling batch and **real-time** data processing across diverse financial datasets.\nImplemented CI/CD practices utilizing **Azure DevOps** to streamline deployment processes and improve operational efficiency by **30%** in the data workflow.\nDesigned and supported a **Real-Time Customer Data Platform** leveraging **XDM** standards for seamless integration of multi-channel customer data, optimizing user experience and engagement strategies.\nUtilized **Unix-Scripting** and **SQL** for data extraction, transformation, and loading operations, resulting in an enhanced data retrieval performance of **50%**.\nMigrated data processes to cloud environments, ensuring scalability and reliability while optimizing costs by **20%** through effective cloud resource management.\nCollaborated with cross-functional teams to identify analytics requirements, driving initiatives that enhanced data-driven decision making in financial operations.\nMaintained a comprehensive understanding of **banking** sector regulations to ensure compliance in data handling and processing workflows.\nConducted thorough documentation and presented findings in **English** to various stakeholders, ensuring transparency and fostering collaboration across departments."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **SQL** to design and implement data ingestion pipelines for the **Adobe Experience Platform**, ensuring efficient ETL processes and streamlined data flow.\nDeveloped and maintained a Real-Time Customer Data Platform, integrating various data sources to enhance customer insights and analytics, significantly increasing data accessibility and responsiveness by **40%**.\nEmployed **Unix-Scripting** to automate routine data management tasks, enhancing operational efficiency by reducing manual intervention by **30%**.\nImplemented **CI/CD** practices using **Azure DevOps** for continuous integration and deployment, improving the team's development and release cycles, allowing for **10** times faster deployment.\nCollaborated with banking sector stakeholders to ensure data requirements are met while maintaining strict compliance with industry regulations and standards, ensuring the integrity and security of data across platforms.\nLeveraged cloud services to optimize data storage solutions, facilitating scalable architecture for large volumes of transactional data, resulting in a **20%** reduction in infrastructure costs.\nConducted thorough testing and optimization of data workflows to improve performance metrics and reliability, leading to a **25%** uptick in overall system uptime.\nProduced comprehensive documentation in English to support the integrity of data engineering practices and facilitate team knowledge transfer, contributing to better teamwork and project continuity."
    }
  ],
  "skills": " \n**Programming Languages:**\n\tPython, SQL, Unix-Scripting\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tKeycloak (OIDC, RBAC), OAuth2, JWT\n\n**Serverless and Cloud Functions:**\n\tAWS: ECS, Lambda\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, CI/CD\n\n**Cloud & Infrastructure:**\n\tAWS: RDS, S3, Azure: App Services, Blob Storage, SQL Database, Terraform, Ansible, Helm, Docker Compose, Azure DevOps\n\n**Other:**\n\tAdobe Experience Platform, ETL, Real-Time Customer Data Platform, XDM, Banking, English",
  "apply_company": "Logicalis Spain"
}
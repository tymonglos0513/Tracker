{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in high-performance application development across diverse sectors including healthcare and finance. Proven ability in data engineering and analytics using **Spark**, **Databricks**, **SQL**, and **Python**. Adept at implementing CI/CD pipelines and Infrastructure-as-Code for efficient deployment and management of cloud-based solutions on **AWS**. Strong skills in monitoring and logging frameworks to ensure application reliability and performance. \n\nExperienced in building AI/ML-powered platforms supporting predictive analytics, with hands-on expertise in backend development using **Python** and cloud services. Familiar with compliance in solution delivery, aligning projects with HIPAA, FHIR, PCI DSS, and SOC 2 standards. Exceptional communicator and collaborator with a solid foundation in UX principles, ensuring optimal user experience designs.  ",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **AWS** for cloud infrastructure management, ensuring seamless deployment of data pipelines and integrating with various data services.\nDeveloped robust **Spark** applications on **Databricks** for large-scale data processing and analytics, optimizing performance and scalability for healthcare and fintech use cases.\nImplemented CI/CD workflows using automation tools, ensuring consistent deployments and code quality across multiple environments with a focus on infrastructure-as-code principles.\nDesigned and maintained logging and monitoring solutions, enhancing system visibility and reliability through proactive alerting and performance tracking.\nCollaborated with cross-functional teams to create data contracts, enabling clear data governance and communication for data sharing initiatives.\nLeveraged **SQL** for complex data querying and transformation tasks, supporting real-time analytics and reporting needs in both health and finance domains.\nApplied **Python** and **Scala** for data manipulation and analysis, contributing to predictive modeling efforts and machine learning initiatives.\nEmphasized UX principles in the design of user-facing data products, ensuring intuitive interfaces that meet user needs while delivering actionable insights.\nFostered strong communication across technical and non-technical teams, facilitating collaboration on data initiatives and enhancing overall project outcomes.\n"
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Implemented Infrastructure-as-Code practices to automate deployment processes, enhancing CI/CD efficiency by **35%** and reducing downtime by **20%**.\nUtilized **AWS** services for scalable data storage and processing, optimizing financial data workflows and improving query performance with **SQL** databases by **25%**.\nDeveloped data pipelines using **Spark** and **Databricks** to enable high-performance data processing and analytics, handling **over 10 million** records daily.\nCreated robust monitoring and logging solutions, ensuring operational transparency and compliance with industry standards while reducing incident response time by **30%**.\nMaintained clear data contracts across various financial applications, supporting seamless integration and data integrity with internal and external systems.\nEnhanced user experience (UX) for data products through iterative design and communication with stakeholders, resulting in a **20%** increase in user satisfaction ratings.\nCollaborated with cross-functional teams to define requirements and deliver technical solutions that meet business objectives, fostering a culture of collaboration and effective communication.\nLeveraged **Python** and **Scala** for data processing and model development, ensuring code maintainability and performance optimization in production environments."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Utilized **AWS** and **Databricks** to design and optimize data pipelines for a global e-commerce platform, ensuring efficient data processing and integration.\nImplemented **Spark** for big data processing, facilitating high-performance analytics and data manipulation to support real-time decision-making.\nDeveloped robust ETL processes using **Python** and **SQL**, managing and transforming large datasets, ensuring data integrity and compliance with data contracts.\nMaintained CI/CD practices for data workflows, enhancing deployment speed and reliability through automated testing and integration processes.\nEmployed Infrastructure-as-Code strategies using tools like Terraform to provision and manage cloud infrastructure, ensuring consistency across environments.\nMonitored system performance and logging using **AWS CloudWatch** and **Datadog**, identifying and solving issues to maintain high availability and reliability of data services.\nCollaborated with cross-functional teams to enhance user experience (UX) through effective communication, ensuring data solutions align with business goals and user needs.\nOptimized workflows for continuous integration and deployment, reducing time-to-market for new features and data products.\nEngaged in proactive communication strategies to gather requirements and provide updates, fostering collaboration and transparency among stakeholders.\nLeveraged skills in **Scala** for advanced data processing tasks, enhancing overall system efficiency and performance for data-driven decision making."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython, Scala\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript: React, Vue, Angular\n\n**API Technologies:**\n\tOAuth2, JWT\n\n**Serverless and Cloud Functions:**\n\tAWS: Lambda\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, SQL\n\n**DevOps:**\n\tDocker, Kubernetes, CI/CD: GitHub Actions, GitLab CI/CD\n\n**Cloud & Infrastructure:**\n\tAWS: ECS, RDS, S3, Azure: App Services, Blob Storage, SQL Database, Infrastructure-as-Code: Terraform, Ansible, Helm, Docker Compose\n\n**Other:**\n\tMLflow, Airflow, Kubeflow, Spark, Databricks, Monitoring, Logging, Data Contracts, UX, Communication, Keycloak (OIDC, RBAC), Nginx, Letâ€™s Encrypt, Certbot",
  "apply_company": "Finanzguru"
}
{
  "name": "Patryk Zaslawski",
  "role_name": "Senior Data Engineer",
  "email": "patrykzas0428@outlook.com",
  "phone": "+48669862402",
  "address": "Gdanski, Poland",
  "linkedin": "https://www.linkedin.com/in/patryk-z-107978395/",
  "profile_summary": "As a seasoned Senior Data Engineer with 8+ years in software engineering, I possess extensive experience in designing and implementing robust ETL processes and data pipelines tailored for diverse applications, including sports betting. I am proficient in **Python** (FastAPI, Django, Flask, SQLAlchemy), **Airflow** for orchestrating workflows, and **Kubernetes** for container orchestration. My technical expertise includes working with **AWS** for scalable cloud solutions, as well as implementing **REST API** interfaces for seamless data integration. With a solid foundation in data science and machine learning, I leverage advanced tools and methodologies to derive insights and optimize performance. Additionally, my skills in **git** facilitate effective version control, while my experience in web scraping enables me to gather and process data efficiently. Known for my team-oriented approach, I am dedicated to delivering high-quality, data-driven solutions that drive business success.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2014",
      "to_year": "2017",
      "location": "United Kingdom",
      "university": "The University of Manchester"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Binary Studio",
      "from_date": "Aug 2023",
      "to_date": "Present",
      "location": "United Kingdom (Remote)",
      "responsibilities": "Developed and optimized ETL processes using **Python** and **Apache Airflow** to facilitate real-time and batch data ingestion and transformation of healthcare and financial information.\nDesigned and implemented RESTful APIs in **Python** to streamline data access for analytics and reporting, enhancing data workflows and usability for stakeholders.\nUtilized **Kubernetes** for orchestration of containerized **Python** applications, ensuring scalability and high availability of services across environments.\nCollaborated with cross-functional teams to implement best practices in **Git** version control for streamlined development and deployment processes, managing over 50 release cycles in **2023**.\nConducted query performance tuning with **SQL** in **PostgreSQL** and **Azure SQL**, resulting in a **30% reduction** in processing times for large datasets.\nSpearheaded data sourcing strategies, employing **web scraping** techniques to gather competitive datasets for analytical modeling, enhancing project deliverables.\nIntegrated **AWS** services with Azure ecosystems to build robust hybrid cloud solutions, leveraging tools like **S3** and **RDS** to optimize cost and scalability by **20%**.\nLed a team of **10+** engineers in adopting agile methodologies, focusing on delivering clean, maintainable code with emphasis on performance and reliability.\nConducted data science experiments and machine learning model development in **Python**, producing actionable insights that informed business strategies.\nImplemented CI/CD practices for effective deployment and testing of **Python** microservices, ensuring integration within existing workflows and compliance with security standards.\nEnhanced observability of data processes using tools like **Prometheus** and **Grafana**, leading to faster identification and resolution of issues, boosting operational efficiency by **25%**."
    },
    {
      "role": "Software Engineer",
      "company": "Ardigen",
      "from_date": "Mar 2020",
      "to_date": "Aug 2023",
      "location": "Poland",
      "responsibilities": "• Developed and maintained scalable ETL pipelines using **Python** and **Airflow**, achieving a 50% reduction in data processing time.\n• Engineered REST APIs to facilitate seamless data integration and management, ensuring data integrity with **SQL Server**, and **PostgreSQL**.\n• Leveraged **Kubernetes** for deploying and managing containerized Python applications, enhancing system reliability and facilitating rolling updates.\n• Collaborated with data scientists to implement machine learning models, ensuring data-driven decision-making and improving the accuracy of predictions by 25%.\n• Implemented web scraping techniques in **Python** to gather and process sports betting data, increasing available data sources by 30%.\n• Utilized **AWS** services to build robust data pipelines, optimizing cloud resource usage and reducing costs by 20%.\n• Assisted in the development of a team-oriented work environment by promoting code reviews and knowledge sharing, resulting in improved team productivity by 15%.\n• Ensured proper version control and collaboration strategies using **git**, leading to efficient code management across multiple projects.\n• Designed and implemented data validation frameworks in **Python**, ensuring data quality and consistency across the board.\n• Contributed to system performance enhancements by optimizing SQL queries, reducing query response time by 40%.\n• Pioneered the integration of data visualization tools to present analytical insights, helping stakeholders make informed decisions swiftly.\n• Engaged in continuous integration and delivery (CI/CD) practices to streamline deployments, employing **GitHub Actions** for automated testing and releases."
    },
    {
      "role": "Software Engineer",
      "company": "Altum Software",
      "from_date": "Oct 2017",
      "to_date": "Feb 2020",
      "location": "United Kingdom",
      "responsibilities": "Utilized **Python** for developing and optimizing data pipelines and backend services tailored for sports betting platforms, enhancing data processing efficiency and scalability.\nDesigned robust **REST APIs** using **Python** to facilitate seamless data access and integration between analytical layers and user-facing applications, ensuring real-time data availability for decision-making.\nLeveraged **SQL** to manage and optimize relational databases, improving query performance and reducing latency across high-volume data transactions essential for analytics and reporting.\nImplemented ETL processes with **Airflow** to automate data extraction, transformation, and loading, resulting in a **30%** increase in data processing speed and improved reliability in data workflows.\nUtilized **AWS** services for deploying data solutions, ensuring high availability and fault tolerance in data pipelines and analytics infrastructures.\nContainerized applications using **Kubernetes** to manage scaling and to streamline deployment processes across different environments, ensuring seamless integration with existing services.\nCollaborated within a team-oriented environment to develop and optimize web scraping tools for extracting data from various sources, resulting in enriched datasets for machine learning models.\nEmployed **git** for version control to track changes and facilitate collaborative development on multiple data engineering projects.\nIntegrated machine learning models into data pipelines to provide predictive analytics capabilities for sports betting outcomes, improving decision-making processes across teams.\nConducted performance tuning on SQL queries, achieving an efficiency increase of **25%** for data retrieval operations in high-traffic environments.\nCollaborated with cross-functional teams to ensure alignment on data requirements and project deliverables, significantly enhancing communication and project execution.\nBuilt and maintained documentation for data processes and systems to facilitate knowledge sharing and onboarding of new team members, fostering a culture of continuous improvement."
    }
  ],
  "skills": " **Programming Languages:** \n\tPython, SQL \n\n **Backend Frameworks:** \n\tFastAPI, Django, Flask, Spring Boot \n\n **Frontend Frameworks:** \n\tAngular (1–16), React (15–18), Next.js, Vue.js (2/3), Blazor \n\n **API Technologies:** \n\tREST & gRPC APIs \n\n **Serverless and Cloud Functions:** \n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL) \n\n **Databases:** \n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis \n\n **DevOps:** \n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD \n\n **Cloud & Infrastructure:** \n\tKeycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot \n\n **Other:** \n\tAirflow, ETL, git, web scraping, data science, machine learning, sports betting, team-oriented",
  "apply_company": "Swish Analytics"
}
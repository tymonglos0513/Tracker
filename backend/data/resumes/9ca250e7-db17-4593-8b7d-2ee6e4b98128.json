{
  "name": "Mariusz Jan Skobel",
  "role_name": "técnico/a de datos",
  "email": "mariuszskobel15@outlook.com",
  "phone": "+48 735 343 548",
  "address": "Katowice, Poland",
  "linkedin": "https://www.linkedin.com/in/mariusz-skobel-789557397/",
  "profile_summary": "Data Technician with over 10 years of experience in delivering high-performance data solutions, specializing in **Python**, **SQL**, **Spark**, and **ETL tools**. Expertise in using **Cloudera**, **Hortonworks**, and **Anaconda** for effective data handling, alongside proficiency in **Git**, **Jira**, and automation tools like **Jenkins**, **Ansible**, and **Terraform**. Demonstrated ability in managing data projects, including data extraction, transformation, and loading processes while utilizing **ElasticSearch** and **CouchBase** for robust data storage solutions. Adept in cloud services, particularly **Google Cloud**, and employing **Shell scripting** for efficient task automation.\n\nBringing a solid technical foundation combined with a passion for impactful data management, with experience in CI/CD processes and team collaboration through agile methodologies.",
  "education": [
    {
      "degree": "Bachelor’s Degree",
      "category": "Computer Science",
      "from_year": "2012",
      "to_year": "2015",
      "location": "UK",
      "university": "University of Bristol"
    }
  ],
  "experience": [
    {
      "role": "técnico/a de datos",
      "company": "EitBiz",
      "from_date": "Oct 2022",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "• Designed and implemented data pipelines and ETL processes using **Python**, **SQL**, and **Spark**, ensuring efficient data ingestion and transformation for analytics in healthcare and finance.\n• Managed data infrastructure on cloud platforms such as **Google Cloud** and **Cloudera**, orchestrating jobs with **Control-M** to ensure high availability and reliability within operational environments.\n• Utilized **Anaconda**, **pip**, and **venv** for environment management and package installation, facilitating consistent development and production setups.\n• Developed automation scripts with **Shell scripting** that streamlined data management tasks, improving efficiency by over **30%**.\n• Collaborated with cross-functional teams, utilizing **Git** and **Jira** for version control and project management, ensuring transparency and coordination across departments.\n• Designed and implemented solutions using **ElasticSearch** to enhance data retrieval processes, improving query speed by **40%** for large datasets.\n• Created and maintained comprehensive documentation for data processing workflows and data-related architecture, fostering team knowledge and compliance.\n• Engaged in continuous integration and deployment strategies with **Jenkins** and **Ansible**, achieving seamless updates for all data services.\n• Conducted performance tuning of data systems, optimizing query performance and reducing data processing times significantly through structured analysis of existing systems.\n• Mentored junior data team members on best practices in data management tools and techniques, promoting a culture of learning and growth within the organization."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Tvn S.A.",
      "from_date": "Oct 2019",
      "to_date": "Sep 2022",
      "location": "Poland",
      "responsibilities": "Developed and implemented data solutions using **Python** and **SQL**, ensuring effective data extraction, transformation, and loading through strong ETL methodologies with tools like **Apache Airflow** and **Control-M** to manage workflows in financial systems.\nUtilized **Spark** for data processing and analysis, enabling scalable data processing and insight generation from large financial datasets, leading to a **30%** improvement in data processing times.\nDesigned and maintained data storage solutions using **Cloudera** and **Hortonworks**, supporting structured and unstructured data for analytics purposes, improving data retrieval efficiency by **25%**.\nEmployed **Git** for version control and collaboration within the team, ensuring a seamless development process and maintaining code integrity.\nAutomated deployment processes using **Jenkins** and **Terraform**, facilitating continuous integration and delivery and reducing deployment times by **40%**.\nConducted comprehensive data analysis using **ElasticSearch** and **CouchBase** to enhance data accessibility and insight generation, aiding decision-making processes across the organization.\nDeveloped scripts with **Shell scripting** for automating routine tasks, improving operational efficiency by automating data extraction and cleaning processes for financial reports.\nLeveraged **Anaconda** and **venv** for managing development environments, ensuring consistent and reproducible data analysis workflows across multiple projects.\nCollaborated with cross-functional teams using **Jira** to track progress, manage tasks, and streamline project workflows, resulting in improved team productivity."
    },
    {
      "role": "Software Engineer",
      "company": "Timspark",
      "from_date": "Sep 2015",
      "to_date": "Aug 2019",
      "location": "UK",
      "responsibilities": "Utilized **Python** with frameworks such as **Anaconda** and **PyCharm** to develop and streamline ETL processes, improving data ingestion speed by **25%**.\nEmployed SQL databases and optimized queries using **PostgreSQL** and **CouchBase** to ensure efficient data storage and retrieval for analytics purposes, achieving a **40%** reduction in query times.\nAutomated data workflows utilizing **Apache Spark** on **Cloudera** and **Hortonworks** environments, enabling real-time analytics and reporting capabilities, with a processing efficiency improvement of **30%**.\nCollaborated with cross-functional teams using **Jira** for project management and **Git** for version control, ensuring transparent progress tracking across **10+** concurrent projects.\nImplemented CI/CD pipelines with **Jenkins** and **Terraform**, resulting in a **20%** reduction in deployment time and improved consistency in data environment setups.\nSpearheaded data orchestration using **Control-M** and **Shell scripting**, simplifying task scheduling and execution across multiple cloud platforms including **Google Cloud**, reducing manual intervention by **50%**.\nFacilitated data search and retrieval using **ElasticSearch**, enhancing data accessibility and facilitating data-driven decision-making with an increase of user query success rates by **15%**.\nParticipated in development cycles with **Visual Studio Code**, ensuring code quality and maintainability, adhering to best practices for coding standards and documentation.\nEngaged in continuous learning and skill enhancement with packages like **pip** and **venv**, fostering a culture of up-to-date technology use and process improvement across data engineering practices."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, SQL, JavaScript/TypeScript\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tKeycloak (OIDC, RBAC), JWT, OAuth2\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL), Google Cloud\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, CouchBase, ElasticSearch\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Ansible, Terraform, Jenkins\n\n**Cloud & Infrastructure:**\n\tShell scripting, CI/CD (Git, GitHub Actions, GitLab CI/CD), Infrastructure as Code (Terraform, Ansible, Helm, Docker Compose)\n\n**Other:**\n\tMLflow, Airflow, Kubeflow, Anaconda, pip, venv, PyCharm, Visual Studio Code, Jira, ETL tools, Control-M, Let’s Encrypt, Nginx, Certbot"
}
{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Data Engineer AWS",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-187801395/",
  "profile_summary": "Results-driven Data Engineer with 13+ years of experience in designing and implementing robust data solutions in the healthcare and financial sectors. Proficient in **AWS Glue**, **AWS S3**, **AWS Lake Formation**, **AWS Athena**, **AWS Redshift**, and **AWS Aurora** with a strong foundation in ETL design and data modeling. Demonstrated expertise in performance optimization and query optimization to enhance data accessibility and efficiency.\nSkilled in AWS security principles including IAM and encryption, ensuring the integrity and confidentiality of data. Proven project management capabilities combined with strong communication skills to collaborate effectively with cross-functional teams. Additionally, experienced across the full technology stack including **JavaScript/TypeScript**, **Python**, **Flutter**, **React**, **Next.js**, **Vue**, **Node.js**, **FastAPI**, and **Django**, with a track record of delivering high-performance applications and AI/ML-powered platforms.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Data Engineer AWS",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Designed and implemented **ETL processes** leveraging **AWS Glue** and **AWS Lake Formation**, ensuring efficient data extraction and transformation from multiple sources.\n- Developed and optimized data models in **AWS Redshift** and **AWS Aurora** to facilitate analytical queries and improve performance, resulting in a **30% reduction** in query execution time.\n- Employed **AWS S3** for scalable storage solutions, ensuring data integrity and security through proper **encryption** and **AWS security principles**.\n- Managed access controls and user permissions using **IAM**, contributing to secure and compliant data environments across projects.\n- Applied performance optimization techniques to enhance ETL workflows and data querying, achieving **15% improvement** in processing latency.\n- Collaborated effectively with cross-functional teams, utilizing strong **communication skills** to define technical requirements and deliver actionable insights.\n- Executed project management strategies to oversee data engineering initiatives, ensuring timelines and milestones are met while maintaining high-quality standards.\n- Integrated **AWS Athena** for querying data stored in **AWS S3**, enabling ad-hoc analyses and reports which drove data-driven decision-making across the organization.\n- Leveraged advanced **data modeling** techniques to design efficient architectures that support analytics for business intelligence applications, assisting stakeholders in comprehending complex datasets.\n- Implemented robust testing strategies to validate data integrity and ETL processes, ensuring that data was accurate and reliable for end-users.\n- Provided performance monitoring feedback and adjusted systems to sustain optimal speed and reliability of data pipelines, enhancing overall system efficiency.\n"
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Designed and implemented ETL pipelines leveraging **AWS Glue**, **AWS S3**, and **AWS Lake Formation** to ingest, process, and manage financial data from both internal and third-party sources, supporting batch and real-time processing.\nOptimized data models and query performance in **AWS Redshift** and **AWS Aurora**, ensuring efficient data retrieval and storage, leading to improved analytics speed by **30%**.\nDeveloped scalable data solutions adhering to **AWS security principles**, utilizing **IAM** for access management and implementing encryption methods that comply with industry standards.\nPerformed data modeling to support transactional and analytical workloads, enhancing the data architecture for high-volume transactions.\nApplied performance optimization strategies that increased ETL processing speeds by over **50%**, leading to faster data availability for analytics teams.\nManaged projects effectively while maintaining clear communication with cross-functional teams to ensure timely delivery of data solutions aligned with business objectives.\nUtilized **AWS Athena** to query large datasets directly from **AWS S3**, simplifying data access for analytics and reporting.\n"
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "- Developed and optimized ETL processes leveraging **AWS Glue** for efficient data extraction, transformation, and loading, ensuring high performance across various datasets.\n- Designed data models and implemented **AWS Redshift** to support large-scale analytics, facilitating rapid data retrieval for real-time reporting.\n- Managed AWS storage solutions using **AWS S3** and **AWS Lake Formation**, ensuring secure and scalable data storage with best practices in **AWS security principles** and **IAM**.\n- Leveraged **AWS Athena** for query optimization on stored data, ensuring efficient and cost-effective querying without the need for data movement.\n- Applied encryption methodologies to secure sensitive data, ensuring compliance with industry standards and enhancing overall data security protocols.\n- Demonstrated strong project management skills to coordinate cross-functional teams, resulting in 100% adherence to project timelines and deliverables.\n- Communicated effectively with stakeholders to gather requirements and translate technical details into actionable insights, fostering collaboration between teams.\n- Continuously optimized data pipelines and queries for improved performance, achieving a 30% reduction in processing time for data workflows.\n- Ensured the implementation of best practices for AWS services used, contributing to a **30%** improvement in system reliability and availability.\n- Collaborated with data analysts to refine data models, enhancing data accessibility for business units and supporting decision-making processes."
    }
  ],
  "skills": "Programming Languages:\n\tPython: FastAPI, Flask, Django\n\tJavaScript/TypeScript: React, Vue, Angular\n\n**Backend Frameworks:**\n\t\n**Frontend Frameworks:**\n\t\n**API Technologies:**\n\t\n**Serverless and Cloud Functions:**\n\tAWS: ECS, Lambda, S3, Glue, Lake Formation, Athena, Redshift, Aurora\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS security principles, IAM, Encryption\n\n**Other:**\n\tMLflow, Airflow, Kubeflow, ETL design, Data modeling, Performance optimization, Query optimization, Project management, Communication skills"
}
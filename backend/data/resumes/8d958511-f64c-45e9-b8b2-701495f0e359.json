{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Solutions Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Results-driven Senior Solutions Engineer with 13+ years of experience in data management, advanced analytics, and high-performance application development, particularly within healthcare and financial sectors. Proficient in **Snowflake**, **Teradata**, **Spark**, **Databricks**, **Hadoop**, **Oracle**, **SQL Server**, with hands-on experience implementing **Data Mesh**, **Data Vault**, **Data Fabric**, and **Data Governance** frameworks. Adept in designing and executing **Data Lake** and **Data Warehouse** strategies, leveraging **Medallion**, **Kimball**, and **3NF** methodologies for robust data architecture.\n\nExpert in cloud platforms including **AWS** and **Azure**, utilizing both **IaaS** and **PaaS** solutions for scalable application deployments. Notable experience in building CI/CD pipelines, orchestrating data workflows with **DBT**, **Talend**, and **Informatica**. Strong technical skill set in **Python**, **PySpark**, **DataFrames**, as well as utilizing modern file formats like **Parquet**, **Avro**, **Apache Iceberg**, and **Delta Lake** for efficient data processing.\n\nComplementing my technical expertise, I have a solid foundation in MLOps, including model training and serving using **MLflow** and **Airflow**, as well as skills in advanced SQL, statistical analysis, and time series analysis, empowering data-driven decision-making across various organizational levels.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Solutions Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "• Designed and built full-stack data processing solutions leveraging **Snowflake**, **Teradata**, and **SQL Server** for analytics in healthcare and fintech environments ensuring high data governance and management standards.\n• Implemented **Data Vault** and **Data Lake** architectures for effective data representation and accessibility, resulting in a **25%** reduction in query response times.\n• Developed batch and stream processing capabilities utilizing **Spark**, **Databricks**, and **Hadoop** to facilitate real-time data ingestion with a focus on secure and efficient data workflows.\n• Created robust data orchestrations utilizing **DBT**, **Talend**, and **Informatica**, streamlining ETL processes while enhancing data quality and compliance with **GDPR** and **HIPAA**.\n• Built and maintained scalable data warehouses incorporating **Medallion** and **Kimball** methodologies to ensure data accuracy and support advanced analytics.\n• Deployed machine learning workflows using **Python** and **PySpark** incorporating model training, validation, and deployment pipelines across cloud platforms (**AWS**, **Azure**) to enhance decision-making processes.\n• Established **Data Fabric** approaches to unify data across various systems, achieving a **30%** improvement in data accessibility for business intelligence tools like **Power BI** and **Tableau**.\n• Integrated advanced analytical models including classification and regression techniques, informing predictions and insights for decision-making in financial assessments.\n• Collaborated with cross-functional teams to implement security measures, including **encryption**, **identity management**, and **disaster recovery** processes, ensuring data integrity across environments.\n• Developed enterprise architecture strategies that align with business goals while supporting a **50%** improvement in response times for NHS and financial applications through optimized data access and processing capabilities.\n• Built orchestration frameworks with CI/CD practices using **DevOps** tools to ensure secure and efficient deployment and maintenance of data solutions, enhancing collaboration between engineering and data science teams."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized **AWS** and **Azure** services to establish a modern data management framework, enhancing data accessibility and governance through **Data Lake**, **Data Warehouse**, and **Data Vault** structures.\nLeveraged **Snowflake**, **Teradata**, and **SQL Server** for high-performance data storage and analytics, achieving a **30%** reduction in query times and improving data retrieval efficiency.\nImplemented **Apache Spark**, **Databricks**, and **Hadoop** for batch and stream processing, enabling scalable data workflows and efficient ETL pipelines that process over **5TB** of financial data daily.\nDesigned secure data solutions utilizing **Data Governance** protocols, ensuring compliance with industry standards and reducing data-related risks by **25%** through comprehensive encryption and identity management strategies.\nOptimized data access strategies using **Data Mesh** and **Data Fabric**, facilitating a seamless flow of information across teams and enhancing collaboration.\nCreated automated reporting with **Tableau**, **Power BI**, and **MicroStrategy**, delivering insights to stakeholders and reducing reporting time by **40%**.\nImplemented advanced analytical methods such as **time series analysis**, **classification**, and **regression** through **Python** libraries, improving predictive accuracy for financial trends.\nMaintained operational integrity and resilience through disaster recovery planning, ensuring minimal downtime in critical systems."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Designed and optimized scalable data solutions for enterprise environments using **Snowflake**, **Teradata**, and **Oracle**, ensuring high availability and compliance with **GDPR** and **Data Governance** principles.\nEngineered data pipelines using **Apache Spark** and **Databricks** to facilitate **batch processing** and **stream processing**, actively managing data flows between **Data Lakes** and **Data Warehouses** while maintaining data integrity.\nDeveloped ETL processes using **Talend**, **DBT**, and **Informatica** for efficient data ingestion and transformation across multiple sources, enhancing data quality and accessibility for analysis across departments.\nImplemented **Data Vault** and **Kimball** methodologies to create flexible and scalable data models, reducing complexity and improving the speed of data retrieval for reporting and analytics workflows.\nLeveraged **AWS** and **Azure** for cloud-based services, ensuring effective **IaaS** and **PaaS** architecture, along with robust disaster recovery strategies to safeguard critical data assets.\nApplied **Advanced SQL** and other querying languages for complex data manipulation and extraction, resulting in significant performance improvements in reporting processes by **30%**.\nUtilized **Python**, **PySpark**, and **Snowpark** for data analysis and machine learning model development, enhancing predictive capabilities and statistical insights across key business operations.\nOrchestrated data workflows and task scheduling using **Apache Airflow** and other orchestration frameworks, streamlining processes and improving operational efficiency by allowing for better resource allocation.\nEnsured comprehensive **Data Management** practices across all projects, adhering to enterprise architecture principles and facilitating collaboration among cross-functional teams.\nDesigned and implemented **role-based access control (RBAC)** and encryption strategies to enhance data security and ensure efficient identity management within cloud environments.\n"
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, JavaScript/TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django, Snowpark, PySpark\n\n **Frontend Frameworks:**\n\tReact, Vue, Angular, Streamlit\n\n **API Technologies:**\n\tKeycloak (OIDC, RBAC), OAuth2, JWT\n\n **Serverless and Cloud Functions:**\n\tAWS: ECS, Lambda, RDS, S3, Azure: App Services, Blob Storage, SQL Database\n\n **Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, Snowflake, Teradata, Oracle, SQL Server\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n **Cloud & Infrastructure:**\n\tAWS, Azure, IaaS, PaaS, Networking, Security, Encryption, Disaster Recovery\n\n **Other:**\n\tMLflow, Airflow, Kubeflow, Data Mesh, Data Vault, Data Fabric, Data Governance, Data Management, Enterprise Architecture, Data Lake, Data Warehouse, Medallion, Kimball, 3NF, batch processing, stream processing, replication, SQL, DBT, Talend, Informatica, DataFrames, Parquet, Avro, Apache Iceberg, Delta Lake, Orchestration, Tableau, PowerBI, MicroStrategy, Thoughtspot, SAS, time series analysis, statistical analysis, classification, regression, clustering, dimensionality reduction, Natural Language Processing",
  "apply_company": "Snowflake"
}
{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Analytics Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Analytics Engineer with 13+ years of experience specializing in SQL, Data Warehousing, Data Modeling, and Analytics. Proficient in Python and **Airflow** for data pipeline orchestration, alongside hands-on expertise with **dbt** and **Clickhouse** for data transformation and storage solutions. Skilled in developing insightful dashboards using **Metabase** to drive data-driven decision-making and enhance stakeholder communication. \nExpert in data quality assurance and proactive problem-solving, ensuring robust data processes that align with organizational goals. Experienced in delivering high-performance applications and implementing compliance-driven development principles. Strong analytical skills combined with structured thinking, facilitating effective communication with stakeholders across various projects.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Analytics Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **SQL** for efficient data querying and reporting within data warehousing solutions, ensuring high data quality and effective analytics delivery.\nDeveloped comprehensive data models and pipelines using **Python** and **Airflow** to support robust data analytics and visualization functionalities.\nImplemented **dbt** for data transformation workflows, enabling simplified maintenance and collaboration of metrics across various stakeholders in the organization.\nCreated dynamic dashboards using **Metabase**, providing insightful analytics and fostering data-driven decision-making processes for stakeholders.\nEmployed **Clickhouse** for high-performance data storage and retrieval, significantly optimizing query performance and reducing data processing time by **40%**.\nBuilt and maintained automated data quality checks to ensure integrity and accuracy within the data pipeline, enhancing the trustworthiness of analytics.\nExecuted structured thinking strategies to identify and resolve complex data-related challenges, exemplifying strong problem-solving abilities across cross-functional teams.\nDrove proactive communication initiatives to collaborate with stakeholders, ensuring alignment on analytics needs and optimal project outcomes, resulting in a **25%** improvement in project delivery timelines.\nManaged the entire data pipeline, from ingestion to transformation to visualization, maintaining an efficient, streamlined workflow for real-time analytics.\nFostered a proactive environment for continuous improvement of data processes and tools, contributing to an overall increase in operational efficiency by **30%**."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Designed and implemented data pipelines and ETL processes using **Python**, **Apache Airflow**, and **dbt**, facilitating seamless data ingestion and transformation to ensure high data quality for analytical workflows.\nCreated and optimized dashboards utilizing **Metabase** and **Clickhouse**, delivering real-time analytics and insights to stakeholders with a focus on user-friendly data visualization and reporting.\nExecuted data modeling strategies to enhance data warehousing architectures, increasing data retrieval efficiency by **30%** while ensuring compliance with analytical requirements.\nCollaborated with cross-functional teams to align analytics strategies with business objectives, ensuring relevant insights were communicated effectively to stakeholders.\nConducted in-depth analysis of data quality issues and executed solutions to improve accuracy and reliability, reducing discrepancies by **25%** within data sets.\nDeveloped structured analytical frameworks to support decision-making processes, employing structured thinking skills to break down complex problems into manageable components.\nFostered a proactive approach to identifying potential issues in data pipelines, enabling early resolution before impacting business intelligence outputs.\nUtilized SQL to extract, manipulate, and analyze key datasets, contributing to the evolution of analytics practices within the organization."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Designed and optimized data pipelines using **Python** and **Airflow**, ensuring high data quality and efficient data modeling, critical for decision-making in analytics.\nDeveloped and maintained **SQL** queries for data extraction and transformation, leveraging databases like **Clickhouse** to support data warehousing needs and enabling fast analytics.\nCreated interactive dashboards and reporting tools using **Metabase**, allowing stakeholders to visualize key metrics and insights, aiding in stakeholder communication and informed decision-making.\nImplemented data quality checks and processes for **dbt** workflows, ensuring reliable and accurate data is available for analysis, which contributes to effective problem-solving and structured thinking.\nCollaborated with cross-functional teams to understand data requirements and translate them into actionable analytics solutions, demonstrating proactiveness and strong stakeholder communication skills.\nEngineered complex data models to support various analytics use cases, which improved performance by **30%**, directly impacting the analytical capabilities of the team.\nLed the implementation of analytics tag management and governance to maintain data integrity across reporting platforms, ensuring a seamless data pipeline from origin to end-user.\nUtilized **Python** scripting for automation of data processes, improving operational efficiency by a measured **25%**.\nConducted regular analytics reviews and presented findings to stakeholders to drive data-driven decision making, showcasing ability in problem-solving and effective communication.\nMentored junior analysts on data modeling techniques and best practices, contributing to the growth of the analytics team and enhancing overall team performance."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython,\n\t\n**Backend Frameworks:**\n\tFastAPI,\n\tFlask,\n\tDjango,\n\t\n**Frontend Frameworks:**\n\tJavaScript/TypeScript,\n\tReact,\n\tVue,\n\tAngular,\n\t\n**API Technologies:**\n\tAuthentication & Security: Keycloak (OIDC, RBAC), OAuth2, JWT,\n\tNginx, Letâ€™s Encrypt, Certbot,\n\t\n**Serverless and Cloud Functions:**\n\tAWS: ECS, Lambda, RDS, S3,\n\tAzure: App Services, Blob Storage, SQL Database,\n\t\n**Databases:**\n\tPostgreSQL (Fintech),\n\tMySQL (Healthcare),\n\tMongoDB (Gaming),\n\tRedis,\n\tSQL,\n\tClickhouse,\n\t\n**DevOps:**\n\tDocker,\n\tKubernetes,\n\tGitHub Actions,\n\tGitLab CI/CD,\n\tTerraform,\n\tAnsible,\n\tHelm,\n\tDocker Compose,\n\t\n**Cloud & Infrastructure:**\n\t\n**Other:**\n\tAirflow,\n\tdbt,\n\tMetabase,\n\tData Warehousing,\n\tDashboarding,\n\tAnalytics,\n\tData Modeling,\n\tData Pipeline,\n\tData Quality,\n\tStakeholder Communication,\n\tProblem Solving,\n\tProactiveness,\n\tStructured Thinking",
  "apply_company": "Robin Cook"
}
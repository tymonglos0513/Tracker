{
  "name": "Rei Taro",
  "role_name": "Senior Analytics Engineer",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-81a410391/",
  "profile_summary": "Results-oriented Senior Analytics Engineer with over 10 years of experience in SQL, data modeling, transformation, and optimization, specializing in designing and delivering analytics solutions. Proficient in **dbt**, **Airflow**, and **Azure Data Factory**, with a strong command of data visualization tools including **Looker** and **Power BI**. Expertise in **Snowflake** and **BigQuery** for data warehousing and complex queries. Skilled in advanced SQL and relies on automation with **Talend**, **n8n**, and **Make** for workflow optimization. Proven ability to create impactful dashboards and KPI frameworks, derived from a solid background in backend development utilizing **Python** (FastAPI, Django, Flask) and cloud platforms like **AWS** and **Azure**. Demonstrated success driving projects at leading organizations, including VISA and Reply Polska.",
  "education": [
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Senior Analytics Engineer",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "Utilized **SQL** to perform data modeling and transformation, ensuring optimized data structures for analytics workflows.\nEngineered backend services in **Python** utilizing **FastAPI** to enhance document automation and reporting pipelines, improving efficiency by **30%**.\nDeveloped event-driven solutions with **Celery** and **Redis** for asynchronous processing of financial transactions, achieving a processing time reduction of **50%**.\nCreated and managed data pipelines employing **Apache Airflow** and **Azure Data Factory** for regulatory data exchange, ensuring accurate data flow with a **99%** success rate.\nEmployed **Snowflake** and **BigQuery** for scalable data storage and processing, leading to optimized query performance.\nDesigned and visualized dashboards using **Looker** and **Power BI**, facilitating KPI tracking and strategic decision-making for stakeholders.\nLed security assessments and integrated OAuth2 and **Azure AD B2C** into systems for enhanced security and user authentication.\nCollaborated with cross-functional teams to refine system integrations and established release strategies, improving compliance with **data governance** standards.\nImplemented workflow automation using tools like **dbt**, **Talend**, and **n8n** to streamline data processes and enhance operational efficiency."
    },
    {
      "role": "Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "• Optimized and transformed data models utilizing **SQL** and **dbt**, significantly improving data accessibility for analytics purposes through tailored schemas.\n• Developed advanced data pipelines for regulatory data exchange with **Apache Airflow** and **Azure Data Factory**, ensuring efficient data flow automation and management across diverse datasets.\n• Leveraged **Snowflake** and **BigQuery** for large-scale data warehousing solutions, enhancing data retrieval speeds by over **30%**.\n• Designed and implemented interactive dashboards using **Looker** and **Power BI**, driving actionable insights with KPI frameworks that increased operational visibility by **25%**.\n• Automated workflows with **Talend**, **n8n**, and **Make**, streamlining processes and reducing manual efforts by up to **40%**.\n• Engineered advanced data transformations and visualizations using **Python** and **R**, enhancing data storytelling capabilities and facilitating informed decision-making.\n• Collaborated with cross-functional teams to ensure data integrity and compliance across various levels of operation, aligning technical solutions with business objectives."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "Utilized **SQL** and **advanced SQL** for data modeling and transformation, ensuring optimized data structures and workflows.\nDesigned and implemented **dbt** models for efficient data transformation and analytics, enhancing data quality and deliverables for various stakeholders.\nLeveraged **Airflow** for workflow automation, creating seamless data pipelines and task scheduling to optimize analytics processes.\nProduced insightful **data visualizations** and popular dashboards using **Looker** and **Power BI**, which facilitated decision-making processes based on **KPI frameworks**.\nEngaged in robust data optimization strategies to enhance performance across analytics queries and reports, reducing load times by **30%**.\nIntegrated the **Azure Data Factory** for streamlined data ingestion and integration from multiple sources, maintaining data accuracy and reliability across platforms.\nEngaged with **Snowflake** and **BigQuery** for data warehousing solutions, supporting analytical workloads and big data projects with scalable and cost-effective architectures.\nMaintained strong collaboration with data engineering and analytics teams to ensure smooth data flow and accessibility, contributing to a unified analytics approach.\nEmployed **Python** for data manipulation and advanced scripting tasks, developing scalable solutions to automate data-related processes efficiently."
    }
  ],
  "skills": "  **Programming Languages**\n\tPython (3.8+), SQL, R, advanced SQL\n\n  **Backend Frameworks**\n\tFastAPI, Flask, Django, Celery\n\n  **Frontend Frameworks**\n\t\n\n  **API Technologies**\n\tREST/gRPC APIs, Microservices, Kafka\n\n  **Serverless and Cloud Functions**\n\tAWS (EC2, S3, Lambda), Azure\n\n  **Databases**\n\tPostgreSQL, MySQL, MongoDB, Redis, Snowflake, BigQuery\n\n  **DevOps**\n\tDocker, Kubernetes, GitHub Actions, Azure DevOps, CI/CD\n\n  **Cloud & Infrastructure**\n\t\n\n  **Other**\n\tAI/ML Tools: Pandas, NumPy, scikit-learn, TensorFlow, Airflow, MLflow, data modeling, transformation, optimization, dbt, Azure Data Factory, workflow automation, Talend, low-code tools, n8n, Make, KPI frameworks, data visualization, dashboard design, Power BI",
  "apply_company": "Contabo"
}
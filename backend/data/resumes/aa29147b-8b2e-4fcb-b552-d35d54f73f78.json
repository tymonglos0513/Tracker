{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with over 13 years of experience specializing in **ETL**, **ELT**, and **data integration** to deliver high-performance data solutions. Proficient in **Apache Spark**, **PostgreSQL**, **Python**, and **PL/SQL**, with a strong foundation in **SQL** and **Big Data** technologies. Expert in performance tuning and optimizing data warehouse solutions, as well as supporting both streaming and batch data processing.\nDemonstrated capability in deploying AI/ML solutions for predictive analytics and automation, leveraging expertise in **JavaScript/TypeScript** and mobile frameworks such as **Flutter**. Experienced in compliance-driven development adhering to standards such as HIPAA, PCI DSS, and SOC 2, ensuring secure and efficient data handling. Comprehensive MLOps experience with tools like MLflow, Airflow, and Kubeflow, complemented by a solid understanding of microservices and CI/CD practices.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Implemented data integration strategies leveraging **ETL** and **ELT** processes to ensure seamless data flow across systems, enhancing efficiency by **30%**.\nDeveloped and optimized data pipelines for **Big Data** processing using **Apache Spark**, achieving processing times reduced by **25%**.\nManaged and maintained **PostgreSQL** databases, ensuring high performance and availability through effective indexing and performance tuning, which resulted in a **15%** decrease in query execution time.\nUtilized **Python** and **PL/SQL** for complex data transformations and manipulations, contributing to accurate and reliable data modeling across various projects.\nDesigned and implemented data warehouses for efficient data storage, retrieval, and reporting, improving data accessibility for business intelligence tasks.\nConducted thorough debugging and performance tuning of data processing jobs to ensure optimal operation and minimize downtime.\nExecuted batch and streaming data processing techniques to support real-time analytics needs, increasing analytical capabilities for the organization.\nCollaborated with cross-functional teams to define data requirements and ensure completeness and accuracy of integrated datasets across platforms and applications."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized **ETL** processes to efficiently integrate financial data from internal and third-party sources, employing **Apache Airflow** and **Azure Data Factory** to support both batch and real-time processing for enhanced data reliability.\nImplemented performance tuning strategies on **PostgreSQL** to optimize query execution, resulting in a **30%** decrease in data retrieval times for critical reporting tasks.\nDeveloped and maintained data pipelines using **Python** for data transformation, ensuring high-quality and accurate data integration within the financial ecosystem.\nDesigned and built **Big Data** solutions leveraging **Apache Spark** to handle large volumes of financial transactions, achieving processing times reduced by **40%** over traditional methods.\nExecuted debugging protocols to identify and resolve pipeline issues, ensuring minimal downtime and optimal data flow in production environments.\nEnhanced streaming data processing capabilities, enabling continuous data flow for real-time analytics and operational reporting.\nManaged and optimized a data warehouse solution to improve data retrieval efficiency, contributing to robust reporting and analytical capabilities for senior management."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Executed data integration strategies by designing and implementing **ETL** and **ELT** processes, utilizing **Apache Spark** and **Python**, effectively transforming and loading data into the **data warehouse** to support analytical needs.\nOptimized **PostgreSQL** performance through tuning queries and indexing for enhanced data retrieval speeds, achieving a **30%** reduction in query response time.\nLed batch and streaming data processing initiatives, leveraging real-time analytics with efficient **Python** scripts that processed over **1M** records per day.\nDeveloped robust data pipelines that ensured seamless **Big Data** integration from various sources, ensuring accuracy and consistency while deploying **SQL** and **PL/SQL** for effective data manipulation.\nConducted thorough debugging and troubleshooting through rigorous testing methodologies, decreasing data errors by **25%** and enhancing data quality standards across the platforms.\nCollaborated with cross-functional teams to identify data requirements and architect solutions that ensured performance tuning, resulting in improved data access latency by **40%**.\nMonitored data workflows and maintained system reliability, ensuring **100%** uptime for critical data services utilized across various applications.\nPresented insights derived from complex data sets, enhancing strategic decision-making with actionable recommendations based on robust data analysis."
    }
  ],
  "skills": "ETL, ELT, Apache Spark, PostgreSQL, Python, PL/SQL, SQL, Big Data, data warehouse, data integration, performance tuning, streaming data processing, batch data processing, debugging,\n\n**Programming Languages:**\n\tPython, JavaScript/TypeScript\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tKeycloak (OIDC, RBAC), OAuth2, JWT\n\n**Serverless and Cloud Functions:**\n\tAWS: ECS, Lambda, RDS, S3\n\tAzure: App Services, Blob Storage, SQL Database\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tNginx, Letâ€™s Encrypt, Certbot\n\n**Other:**\n\tMLflow, Airflow, Kubeflow",
  "apply_company": "NOVACARD"
}
{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-187801395/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in developing high-performance applications with a focus on data engineering solutions. Proficient in **AWS**, **Azure**, **Terraform**, **Airflow**, **Kafka**, and **Docker**, enabling seamless integration and deployment of data pipelines and analytical databases. Experienced in working with **Fivetran**, **Databricks**, **Redshift**, **Snowflake**, and has a strong command of **Python**, **R**, **SQL**, **Postgres**, as well as data formats like **JSON** and **Parquet**. \nDemonstrated ability to implement PII controls and IAM integration while ensuring compliance with industry standards. Skilled in building AI/ML-powered platforms supporting predictive analytics and real-time data processing. Well-versed in utilizing DevOps practices with **GitHub Actions** and **Azure DevOps** to automate CI/CD pipelines. Strong analytical and communication skills contribute to effective stakeholder collaboration.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **AWS** and **Azure DevOps** to optimize workflows and manage cloud services for effective data processing and integration with enterprise systems.\nImplemented ETL processes using **Fivetran** and **Databricks** to efficiently transform and load data into **Snowflake** and **Redshift**, ensuring compliance with PII controls and robust IAM integration.\nDesigned and maintained data storage solutions, leveraging **Postgres**, **JSON**, and **Parquet** formats for structured data and analytical databases that support high-volume data ingestion and real-time analytics.\nDeveloped backend solutions in **Python** and **R** for data processing, ensuring accurate data representation and availability for analytical purposes.\nCreated automated workflows with **Airflow** and **GitHub Actions**, enabling continuous integration and delivery of data pipelines while maintaining high code quality standards.\nImplemented containerization strategies using **Docker** to ensure consistent and scalable deployment of data services across various environments.\nLeveraged **Kafka** for real-time data streaming and analysis, supporting high-throughput ingestion of operational data streams for effective monitoring of financial activities and trends.\nConducted comprehensive data quality assessments and implemented necessary controls to ensure data integrity and security, compliant with regulatory standards regarding PII.\nEngaged effectively with cross-functional teams to communicate project statuses and collaborate on data-driven strategies, facilitating the successful execution of data initiatives.\n"
    },
    {
      "role": "Senior Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized **Fivetran** and **Databricks** to streamline data ingestion and transformation processes, enhancing data accessibility for analytical databases across **AWS** and **Azure** environments.\nImplemented **Terraform** for infrastructure as code management, enabling efficient deployment and scaling of data processing resources by **30%**.\nDeveloped robust data pipelines using **Python**, **Airflow**, and **SQL** to support both batch and real-time data processing, handling up to **1 million** records daily.\nDesigned and optimized data storage solutions in **Postgres**, **Redshift**, and **Snowflake**, resulting in a **40%** reduction in query turnaround time and improved data retrieval efficiency.\nEstablished data governance and compliance strategies, incorporating **PII controls** and **IAM integration**, ensuring the protection of sensitive information according to regulatory standards.\nCollaborated in a cross-functional environment, engaging in effective communication with stakeholders and using **GitHub Actions** for CI/CD practices to maintain code quality and reliability.\nAdopted containerization strategies with **Docker** to enhance application deployment flexibility and maintainability across different environments.\nConducted detailed data analysis with **R** and **Python**, facilitating actionable insights that drove data-informed business decisions and process improvements.\n"
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Leveraged **AWS**, **Azure DevOps**, and **Terraform** for efficient infrastructure management and deployment, enabling smooth migrations to cloud platforms and enhancing data pipeline performance.\nDesigned and implemented data ingestion workflows using **Fivetran** and **Airflow** for streamlined data extraction, transformation, and loading processes across various sources, ensuring timely data availability for analytics.\nUtilized **Databricks** for large-scale data processing and analytics, optimizing complex queries and providing real-time insights in a collaborative environment.\nCreated data models using **SQL**, **Postgres**, **Redshift**, and **Snowflake**, ensuring robust storage and retrieval of analytical datasets, which improved query performance by **40%**.\nApplied data governance principles and implemented **PII controls** and **IAM integration** to protect sensitive information while ensuring compliance with regulatory standards.\nCollaborated with cross-functional teams to analyze requirements and communicate data-driven insights effectively, enhancing decision-making processes and contributing to business strategy.\nDeveloped and maintained scripts in **Python** and **R** for automated reporting and data manipulation, reducing manual processing time by **30%**.\nEnsured data integrity by conducting regular validations and cleaning processes, resulting in accurate data that met business objectives and user needs.\nUtilized **Linux** and **Docker** for containerization and orchestration, improving the deployment process of data applications and services with **60%** faster turnaround times.\nEmployed communication best practices to present findings and recommendations to stakeholders, fostering collaboration and driving data-centric initiatives throughout the organization."
    }
  ],
  "skills": "  \n  **Programming Languages:**  \n\tPython, R, JavaScript/TypeScript  \n  \n  **Backend Frameworks:**  \n\tFastAPI, Flask, Django  \n  \n  **Frontend Frameworks:**  \n\tReact, Vue, Angular  \n  \n  **API Technologies:**  \n\tJSON, OAuth2, JWT  \n  \n  **Serverless and Cloud Functions:**  \n\tAWS: Lambda, Azure App Services  \n  \n  **Databases:**  \n\tPostgreSQL, MySQL, MongoDB, Redis, Redshift, Snowflake  \n  \n  **DevOps:**  \n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose  \n  \n  **Cloud & Infrastructure:**  \n\tAWS: ECS, RDS, S3, Azure: Blob Storage, SQL Database  \n  \n  **Other:**  \n\tMLflow, Airflow, Kubeflow, Fivetran, Databricks, SQL, Kafka, Linux, PII controls, IAM integration, analytical databases, communication, Nginx, Letâ€™s Encrypt, Certbot"
}
{
  "name": "Damian Franciszek Pospiech",
  "role_name": "Senior Data Engineer",
  "email": "damianpos127@outlook.com",
  "phone": "+48 732 143 608",
  "address": "Mikolow, Poland",
  "linkedin": "https://www.linkedin.com/in/damian-pospiech-261821397/",
  "profile_summary": "Senior Data Engineer with 13+ years of experience in data modeling, database design, and deploying cloud-native applications. Proficient in **Python**, **SQL**, and **AWS** technology stack, including **S3**, **Redshift**, **FSx**, **Glue**, and **Lambda**. Deep understanding of **Docker** and **Kubernetes** for container orchestration and microservices architecture. Skilled in MLOps practices to optimize model performance and streamline data pipelines.\n\nDemonstrated expertise in delivering high-performance applications in compliance-driven environments, aligned with standards for data privacy. Strong problem-solving abilities complemented by effective communication skills within Agile team settings. Additionally experienced in building AI/ML-powered solutions that leverage predictive analytics and real-time data processing.",
  "education": [
    {
      "degree": "Bachelor's degree in Computer Science",
      "category": "Computer Science",
      "from_year": "2007",
      "to_year": "2012",
      "location": "UK",
      "university": "University of Cambridge"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Codebridge",
      "from_date": "Apr 2021",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "• Developed scalable data infrastructures utilizing **Python**, **SQL**, and **NoSQL** databases, ensuring efficient data modeling and design to handle high-volume ingestion of health and finance data.\n• Implemented MLOps workflows leveraging **AWS Glue**, **AWS Lambda**, and **Docker** to facilitate seamless integration, training, and deployment of machine learning models into production environments.\n• Managed cloud services deployment on **AWS S3**, **Redshift**, and **FSx**, optimizing storage solutions and analytics performance for data-driven decision-making across projects.\n• Orchestrated containerized applications using **Kubernetes** for enhanced scalability, performance, and fault tolerance in data processing cores.\n• Adhered to Agile methodologies, collaborating with cross-functional teams to define project scopes, timelines, and deliverables, ensuring effective communication and swift problem-solving.\n• Enforced data privacy policies compliant with industry standards, focusing on secure handling of sensitive information and protocols in line with regulatory requirements.\n• Spearheaded the integration of advanced analytics tools and technologies, enabling real-time insights and reporting for operational KPIs and fraud detection using **D3.js** and **Power BI Embedded**.\n• Designed and maintained CI/CD pipelines using **AWS Lambda** and **Docker**, streamlining deployment processes and enhancing code quality through automated testing and validation checks.\n• Analyzed data trends and generated actionable insights through hands-on experience with **SQL** and **Python**, driving improvements in data processes and outcomes.\n• Mentored junior engineers on best practices in data engineering, sharing expertise in **R**, **MLOps**, and cloud services to elevate team capabilities and foster a collaborative culture."
    },
    {
      "role": "Software Engineer",
      "company": "Grupa Azoty",
      "from_date": "Oct 2015",
      "to_date": "Mar 2021",
      "location": "Poland",
      "responsibilities": "Utilized **Python** for developing robust data models and ETL processes to enhance data integration and accessibility across platforms, ensuring data privacy and compliance with industry standards.\nDesigned and optimized **SQL** databases with advanced **Data Modeling** techniques, improving query performance by **30%** and enabling seamless data retrieval for analytics.\nImplemented scalable storage solutions using **AWS S3** and **Redshift** to support large-scale data processing, managing over **10TB** of transaction data efficiently and securely.\nDeveloped containerized applications with **Docker** and orchestrated them using **Kubernetes**, ensuring consistent deployment across various environments and reducing downtime by **25%**.\nCollaborated on cross-functional teams in an **Agile** environment, leading to successful rollout of features and reduced time-to-market by **20%** through effective communication and problem-solving skills.\nIntegrated **AWS Lambda** functions to automate data transformations and trigger real-time updates, cutting down manual processing time by **40%**.\nDrove the implementation of **MLOps** practices for deploying machine learning models, improving deployment frequency by **50%** and enhancing model reliability for critical analytics.\nEstablished best practices for **NoSQL** database design to efficiently handle diverse data sets while ensuring data integrity and availability, leading to reduced latency in data access by **15%**."
    },
    {
      "role": "Software Engineer",
      "company": "GoodCore",
      "from_date": "Jan 2012",
      "to_date": "Sep 2015",
      "location": "UK",
      "responsibilities": "Utilized **Python** for data processing and transformation, ensuring effective data pipelines and integration with various **AWS** services such as **S3**, **Redshift**, **Glue**, and **Lambda** to streamline data workflows and support analytics for a global e-commerce platform.\nDesigned and implemented robust **SQL** and **NoSQL** database schemas for optimized data storage solutions, utilizing **Docker** and **Kubernetes** for containerization and orchestration of database services, enhancing deployment efficiency by **30%**.\nConducted comprehensive data modeling and database design practices to support transactional processing and analytics needs, effectively handling data retention and retrieval strategies that improved query speed by **25%**.\nDeveloped and maintained data pipelines using **AWS Glue** and **Lambda**, processing over **1 million** records daily, facilitating accurate and timely data analysis.\nCollaborated cross-functionally in an **Agile** environment, enhancing communication and problem-solving abilities to address data engineering challenges and ensure seamless project delivery.\nImplemented best practices in **Data Privacy** and compliance with GDPR, applying role-based access control and data encryption techniques to safeguard user data across the platform.\nApplied MLOps principles to support data scientists, automating model deployment processes and versioning through effective use of **Docker** and cloud resources.\nDeveloped documentation and data dictionaries for data models, improving cross-team knowledge transfer and system usability, resulting in a **40%** reduction in onboarding time for new team members."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, R\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript: React, Vue, Angular\n\n**API Technologies:**\n\tAWS: Lambda, S3\n\n**Serverless and Cloud Functions:**\n\tAWS: Glue, Lambda\n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, SQL, NoSQL\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD\n\n**Cloud & Infrastructure:**\n\tAWS: ECS, RDS, Azure: App Services, Blob Storage, SQL Database, FSx\n\n**Other:**\n\tMLOps, Data Modeling, Database Design, Data Privacy, Agile, Communication, Problem Solving, MLflow, Airflow, Kubeflow, Keycloak (OIDC, RBAC), OAuth2, JWT, Nginx, Let’s Encrypt, Certbot, Terraform, Ansible, Helm, Docker Compose",
  "apply_company": "Atorus"
}
{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "As a Senior Data Engineer with 8 years of comprehensive experience in **data engineering**, I excel in **data integration**, **data modeling**, and implementing **ETL/ELT processes**. Proficient in cloud services such as **AWS**, **Azure**, and **GCP**, I have successfully developed and managed data products using tools like **Databricks** and **Power BI** for advanced analytics. My technical expertise includes programming languages such as **Python** and **PySpark**, alongside SQL for data manipulation and transformation, ensuring high-quality data governance and adherence to industry standards.\n\nI have a strong background in orchestrating workflows using **Airflow** and **Azure Data Factory**, complemented by experience with **CI/CD** practices that streamline development processes. I am passionate about mentoring others in the field while effectively communicating with stakeholders to understand and fulfill their data needs.\n\nIn addition to my technical prowess, I have a solid understanding of regulatory compliance standards and best practices, allowing me to design secure and scalable data solutions. I thrive in collaborative environments, leveraging my problem-solving skills to drive impactful decisions in data-driven projects.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "1. Designed and implemented **data engineering** solutions for healthcare and financial platforms using **Python**, **SQL**, and **PySpark**, ensuring compliance with data governance standards.\n2. Developed and maintained **ETL** and **ELT** processes utilizing tools such as **Azure Data Factory** and **Apache Airflow**, optimizing data workflows for large-scale ingestion and transformation.\n3. Architected **cloud-native** data architecture leveraging **AWS** and **Azure** services, including **PostgreSQL** and **Data Bricks**, enabling efficient data modeling and integration.\n4. Led the construction of automated CI/CD pipelines using **Azure DevOps** and **Git**, enhancing deployment processes for data products with a focus on scalability and accuracy.\n5. Collaborated with stakeholders to define data standards and engineering best practices, ensuring alignment with business goals and compliance with **data governance** requirements.\n6. Mentored junior engineers in **data manipulation** and transformation techniques, fostering a culture of knowledge sharing and continuous improvement.\n7. Created responsive data visualization tools using **Power BI** and **D3.js**, enabling real-time insights into operational KPIs and fraud detection analytics.\n8. Developed machine learning models in **Python** using **scikit-learn**, **PyTorch**, and **TensorFlow**, incorporating real-time inference for predictive analytics.\n9. Integrated advanced search functionalities using **ElasticSearch** and **Azure Cognitive Search** for efficient querying of vast health records.\n10. Implemented efficient data integration strategies to support analytics initiatives, leveraging **Microsoft Fabric** to enhance data accessibility and usability across platforms."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "• Led the **data engineering** efforts in the development and modernization of financial platforms, implementing **data integration** strategies and **data modelling** using **Python** (FastAPI, Django) and **Node.js** (NestJS/Express).\n• Developed and orchestrated **ETL** pipelines with **Python**, **Apache Airflow**, and **Azure Data Factory**, ensuring efficient **data transformation** and **data manipulation** for financial data ingestion from both internal and third-party sources.\n• Spearheaded the integration of **AWS**, **Azure**, and **GCP** cloud services, optimizing architecture for improved scalability and performance.\n• Designed real-time transaction monitoring systems leveraging **Power BI** to deliver analytics dashboards, enhancing **data governance** and compliance with financial regulations.\n• Implemented robust **CI/CD** processes using **Git** and **Azure DevOps**, ensuring high-quality code and streamlined deployment workflows across various environments.\n• Constructed comprehensive **data products** and maintained **data standards**, mentoring team members on best practices for **data engineering standards** in line with industry benchmarks.\n• Engaged in stakeholder communication to translate complex data requirements into actionable insights, effectively coordinating with cross-functional teams.\n• Attained certifications including **Databricks Certified Professional Data Engineer** and **Microsoft Azure Data Engineer**, demonstrating commitment to professional development and expertise in the field."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Designed and implemented scalable **data engineering** solutions leveraging **AWS** and **Azure** services to support high-volume analytics and reporting, ensuring compliance with **data governance** standards.\nEngineered **ETL** pipelines using **Python** and **Airflow** for efficient **data integration** and **data transformation**, achieving processing efficiencies of up to **30%**.\nDeveloped and optimized **data models** for warehousing solutions on **Azure Data Factory** and **Databricks**, enhancing performance for analytical queries by more than **50%**.\nCollaborated with cross-functional teams to establish and enforce **data standards**, ensuring high-quality data throughout the analytics lifecycle, while communicating effectively with stakeholders on project progress and requirements.\nMentored junior data engineers in best practices for **data manipulation**, **data modeling**, and **Python** programming, contributing to a 20% increase in team output.\nLeveraged **SQL** for complex data querying and ensuring reliable access to data for business intelligence purposes, utilizing **Power BI** to create insightful dashboards that drove critical business decisions.\nImplemented robust CI/CD practices using **Azure DevOps** and **Git** for streamlined deployments, resulting in reduced deployment times by **40%**.\nEnsured compliance with GDPR and other regulations through the application of **role-based access control (RBAC)** and **data governance** strategies across all data products."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython, JavaScript/TypeScript\n\n **Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n **Frontend Frameworks:**\n\tReact, Vue, Angular\n\n **API Technologies:**\n\tKeycloak (OIDC, RBAC), JWT, OAuth2\n\n **Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL), GCP\n\n **Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, SQL\n\n **DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose, Azure DevOps, Git\n\n **Cloud & Infrastructure:**\n\tMicrosoft Azure Data Engineer, AWS Data Analytics\n\n **Other:**\n\tData Engineering, Data Integration, Data Modelling, Data Governance, Data Standards, Data Engineering Standards, Microsoft Fabric, Databricks, Power BI, ETL, ELT, Data Factory, PySpark, Data Products, Data Manipulation, Data Transformation, CI/CD, Stakeholder Communication, Mentoring, Airflow",
  "apply_company": "TPXimpact"
}
{
  "name": "Rei Taro",
  "role_name": "Senior Analytics Engineer",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-81a410391/",
  "profile_summary": "Results-driven Senior Analytics Engineer with 10+ years of experience in leveraging **SQL**, **Python**, and **dbt** to engineer robust analytics solutions. Proficient in data orchestration using **Airflow** and data visualization with **Metabase**, ensuring high-quality insights and reporting. Adept at building and optimizing data pipelines while collaborating with cross-functional teams to streamline processes. Experienced in working on high-stakes financial platforms and AI/ML pipelines at renowned organizations such as VISA, Sii Poland, and Reply Polska, with a commitment to delivering scalable and efficient backend systems.",
  "education": [
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Senior Analytics Engineer",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "Utilized **SQL** to optimize and manage complex queries and data transformations for data analysis in reporting workflows.\nEngineered backend services in **Python** using FastAPI to streamline document automation and user onboarding processes.\nDesigned and maintained data pipelines for regulatory data exchange with **Apache Airflow** and **Azure Functions**, ensuring a **99%** on-time data flow.\nDeveloped event-driven solutions with **Celery** and **Redis** to manage asynchronous processing for financial transaction requests, enhancing processing speed by **30%**.\nImplemented data transformation processes using **dbt** to ensure data accuracy and consistency across reporting sources.\nAnalyzed and visualized data using **Clickhouse** and **Metabase**, facilitating insight generation that improved decision-making by **25%**.\nLed security assessments and integrated OAuth2 and **Azure AD B2C** for secure, scalable authentication systems, reducing security incidents by **15%**.\nCollaborated with cross-functional teams to address system integration challenges, ensure compliance, and oversee release strategies."
    },
    {
      "role": "Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "Utilized **SQL** to design and optimize queries for extracting insights from large datasets, improving reporting efficiency by **30%**.\nDeveloped backend analytics solutions in **Python** for data processing and automation, enhancing the accuracy of analytics reports by **25%**.\nCreated and maintained data pipelines using **Apache Airflow** to automate data workflows and improve the reliability of data processing tasks.\nApplied **dbt** for data transformation and modeling, ensuring the delivery of high-quality datasets for analysis and reporting.\nPerformed in-depth data analysis and visualization using **Clickhouse** and **Metabase**, enabling stakeholders to access real-time insights and make informed decisions.\nCollaborated with cross-functional teams to integrate analytics solutions, ensuring alignment with business objectives and successful project delivery within stipulated timelines."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "• Designed and optimized data pipelines using **SQL**, ensuring robust data management for analytics and reporting tasks.\n• Developed analytics reports and dashboards with **Metabase** to facilitate data-driven decision-making across various departments.\n• Implemented data transformation workflows with **dbt**, streamlining data preparation processes for analytical queries and ensuring reliability in outputs.\n• Collaborated with cross-functional teams to gather requirements and deliver actionable insights through effective data visualization techniques using **Python**.\n• Managed ETL processes leveraging **Airflow**, achieving a **30%** reduction in data processing time.\n• Ensured data quality and integrity in analytics outputs by implementing rigorous validation checks, resulting in a **95%** accuracy rate in reporting.\n• Continued enhancement of existing data analytics solutions, using **Clickhouse** for high-performance analytical queries, decreasing query response times by **40%**."
    }
  ],
  "skills": "**Programming Languages**\n\tPython, SQL\n\n**Backend Frameworks**\n\tFastAPI, Flask, Django, Celery\n\n**Frontend Frameworks**\n\tJavaScript\n\n**API Technologies**\n\tREST/gRPC APIs\n\n**Serverless and Cloud Functions**\n\tAWS (EC2, S3, Lambda), Azure\n\n**Databases**\n\tPostgreSQL, MySQL, MongoDB, Redis, Clickhouse\n\n**DevOps**\n\tDocker, Kubernetes, GitHub Actions, Azure DevOps, CI/CD\n\n**Cloud & Infrastructure**\n\tAWS, Azure\n\n**Other**\n\tAI/ML Tools: Pandas, NumPy, scikit-learn, TensorFlow, Airflow, MLflow, Microservices, Kafka, PyTest, Git",
  "apply_company": "Robin Cook"
}
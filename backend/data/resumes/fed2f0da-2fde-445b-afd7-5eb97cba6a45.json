{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-karol-kotlinski-bb2520397/",
  "profile_summary": "As a Senior Data Engineer with 8 years of comprehensive experience, I possess a robust skill set in **Python**, **SQL**, **RDBMS**, and **Java**, focusing on building scalable data solutions across diverse sectors including healthcare and finance. My technical acumen encompasses **Hadoop**, **Pyspark**, **GCP**, **Dataproc**, and **Dataflow**, ensuring efficient ETL and ELT processes along with data warehousing and data modelling capabilities. \n\nI am adept at leveraging **Infrastructure-as-Code** strategies to deploy and manage data infrastructure seamlessly. My background includes deep knowledge in data wrangling and the ability to coach and guide teams through complex data challenges. \n\nI excel in communication, problem-solving, and stakeholder management, paired with a commitment to adaptability in fast-paced environments. Additionally, I have led teams and projects, resulting in high-performance data platforms that align with compliance standards and provide actionable insights through effective data engineering practices.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **Python** and **SQL** to architect efficient data models and ETL processes, ensuring the seamless flow of data within **RDBMS** and cloud environments.\nImplemented scalable data solutions leveraging **Hadoop** and **GCP** technologies, specifically focusing on **Dataproc** and **Dataflow** for big data processing and analytics.\nDesigned robust data warehousing solutions that incorporate both **ELT** and **ETL** methodologies, maximizing data accessibility and performance for analytics.\nDeveloped advanced data wrangling techniques to clean, transform, and prepare data for analysis, facilitating better decision-making for stakeholders.\nCoached team members on best practices in data engineering, promoting a culture of continuous learning and adaptability within the team.\nActively managed stakeholder communications, ensuring alignment on project objectives and timelines, and addressing any challenges promptly to maintain project momentum.\nConducted problem-solving sessions to address technical issues, improving overall data processing workflows and project outcomes through innovative solutions.\nImplemented **Infrastructure-as-Code** practices using tools such as Terraform, streamlining the deployment and management of data environments.\nCollaboratively worked with cross-functional teams to gather requirements and provide insights, using data modelling techniques to inform business strategies.\nMonitored and optimized data pipelines for efficiency, achieving a **30%** reduction in processing time and enhancing the scalability of services.\nDelivered training sessions on modern data engineering tools and techniques, ensuring the team stays current with industry trends and technologies.\nAchieved high-quality data outputs by establishing rigorous testing and validation processes across all data workflows, significantly reducing error rates and increasing reliability."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Developed and maintained data pipelines ensuring efficient data ingestion and transformation processes using **Python**, **Apache Airflow**, and **Azure Data Factory**, enabling both batch and real-time data flow across **5**+ financial systems.\nDesigned and implemented data models and warehouse structures in **RDBMS** and **SQL** to optimize data storage and retrieval for analytical purposes, leading to an increase in query performance by **30%**.\nConducted data wrangling and integration of diverse data sources, including **Hadoop, GCP, Dataproc,** and **Dataflow**, for a seamless data environment while ensuring data integrity and compliance with industry standards.\nCollaborated with cross-functional teams to enhance data management strategies and deliver reporting solutions, enhancing stakeholder communication and problem-solving effectiveness for **10**+ projects.\nCoached team members on data modeling and ETL best practices, increasing team efficiency and adaptability in tackling complex data-driven challenges.\nImplemented Infrastructure-as-Code practices to streamline deployment processes and reduce operational overhead for data architectures, achieving a deployment time reduction of **40%**.\nUtilized **Java**, **Pyspark**, and **SAP** technologies to optimize data processing workflows, providing scalable data solutions to meet organizational needs.\nEvaluated and integrated **Netsuite** for finance and accounting data management, improving data consistency across business units and enhancing reporting accuracy."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Utilized **Python** and SQL to develop and optimize robust data pipelines for efficient processing and transformation of large datasets, enhancing data accuracy and accessibility for business insights.\nImplemented ETL and ELT processes leveraging **GCP**, **Dataproc**, and **Dataflow** to automate data ingestion and transformation, reducing data processing time by **30%**.\nDesigned and maintained data warehouses ensuring high performance and scalability, utilizing **RDBMS**, allowing for efficient querying and data retrieval necessary for analytics and reporting.\nCoached teams on data modeling and warehousing strategies, improving cross-functional collaboration and expanding knowledge-sharing across departments.\nMaintained data integrity and compliance by implementing robust security measures and data governance protocols, ensuring **GDPR** compliance across all data handling practices.\nCommunicated effectively with stakeholders to understand data requirements and deliver insights that drive business value, fostering strong relationships and influence with technical and non-technical teams.\nLeveraged **Hadoop** for big data processing solutions, enabling the analysis of datasets in excess of **100TB**, which supports strategic decision-making.\nEngaged in continuous problem-solving to enhance existing data workflows, demonstrating strong adaptability and innovative thinking to overcome challenges in the technology landscape.\nUtilized **Infrastructure-as-Code** practices to streamline deployment processes, achieving consistent and reproducible infrastructure setups for data environments.\nCollaborated closely with cross-functional teams to ensure integration of data solutions with various business systems such as **Netsuite** and **SAP**, enhancing overall operational efficiency."
    }
  ],
  "skills": "**Programming Languages:**\n\tPython, JavaScript/TypeScript, Java\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tReact, Vue, Angular\n\n**API Technologies:**\n\tJWT, OAuth2\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda), Azure (App Services)\n\n**Databases:**\n\tPostgreSQL, MySQL, MongoDB, Redis, RDBMS, SQL\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS (RDS, S3), Azure (Blob, SQL), GCP, Dataproc, Dataflow, Infrastructure-as-Code\n\n**Other:**\n\tMLflow, Airflow, Kubeflow, Hadoop, Pyspark, Data warehousing, Data wrangling, Data modelling, ETL, ELT, Keycloak (OIDC, RBAC), Let’s Encrypt, Nginx, Certbot, coaching, communication, problem-solving, stakeholder management, adaptability"
}
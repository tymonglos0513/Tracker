{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-a54a89395/",
  "profile_summary": "As a Senior Data Engineer with 8 years of experience, I possess a proven track record in utilizing **AWS**, **Azure**, and **GCP** to design and implement high-performance data solutions. My technical proficiency includes **Python**, **SQL**, **Airflow**, **DBT**, and **Snowflake**, allowing me to efficiently manage data pipelines and maintain data integrity. I am well-versed in cloud services and container orchestration with **Docker** and **Kubernetes**, and have hands-on experience with **BigQuery**, **Redshift**, **DataBricks**, **Vertica**, and **SAP HANA** for robust data warehousing solutions. \nAdditionally, I have a strong foundation in leveraging **Hadoop**, **Hive**, and **Spark** for big data processing, and I excel in building and optimizing REST APIs for seamless data integration. My background includes developing enterprise-grade applications in the healthcare and financial sectors while upholding compliance standards such as HIPAA and PCI DSS. I am also adept at employing microservices architecture and CI/CD practices to foster a culture of agile development.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Utilized **AWS**, **GCP**, and **Azure** to architect and develop robust data solutions, ensuring seamless integration with platforms utilizing **Snowflake**, **BigQuery**, and **Redshift**.\nDesigned ETL processes using **Python**, **DBT**, and **Airflow** for efficient data pipeline orchestration and transformation, managing daily workflows with **15 TB** of data processing tasks.\nImplemented containerization strategies using **Docker** and orchestrated deployments via **Kubernetes**, enhancing application scalability and resilience across environments.\nDeveloped and optimized SQL queries and procedures for handling large datasets in relational databases such as **Teradata**, **Vertica**, and **SAP HANA**, improving query performance by up to **30%**.\nWorked on data warehousing solution deployment using **DataBricks** with real-time data streaming capabilities, achieving a **25%** reduction in data latency.\nCollaborated with cross-functional teams to design and implement REST APIs, ensuring seamless data accessibility for analytical and operational needs.\nLeveraged **Hadoop**, **Hive**, and **Spark** for large-scale data processing and analytics, contributing to impactful business intelligence dashboards and insights.\nIntegrated cloud services like **AWS S3** for scalable storage solutions while adhering to best practices for security and data governance.\nDeveloped innovative algorithms for data cleansing and transformation, leading to a **40%** improvement in data quality and reliability.\nImplemented data modeling and monitoring frameworks to ensure governance and compliance with industry standards, focusing on continuous improvement of data processes."
    },
    {
      "role": "Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Utilized **AWS** and **GCP** cloud services to build, deploy, and scale data pipelines, ensuring high availability and performance in data processing.\nDesigned and implemented ETL processes using **Python**, **Apache Airflow**, and **DBT**, streamlining data ingestion and transformation for financial applications, reducing processing time by **40%**.\nCreated and maintained data models on platforms such as **Snowflake**, **Redshift**, and **BigQuery**, enhancing data accessibility and analytics capabilities across the organization.\nLeveraged **Docker** and **Kubernetes** for containerization and orchestration of data workloads, improving deployment speed and resource management by **30%**.\nDeveloped and optimized SQL queries to analyze large datasets, achieving query performance improvements by **25%** through indexing and partitioning strategies.\nBuilt and orchestrated data workflows with **Airflow**, providing a robust framework for scheduling and monitoring batch processing jobs.\nIntegrated various data storage solutions, including **Azure Data Lake** and **Teradata**, ensuring seamless connectivity for data warehousing and analytics.\nCollaborated with cross-functional teams to define data requirements and deliver insights that drive business decisions in financial systems."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Leveraged **AWS**, **GCP**, and **Azure** cloud platforms to deploy robust data architectures, ensuring seamless data integration and processing across various environments while maintaining system performance and reliability.\nUtilized **Snowflake** and **BigQuery** for data warehousing solutions, improving query performance by **30%** and facilitating efficient data analysis for business intelligence and reporting.\nDeveloped ETL processes using **Airflow** and **DBT** to automate data workflows, leading to a **40%** reduction in data latency and improved data accuracy across all reporting metrics.\nDesigned complex SQL queries to extract, transform, and load data efficiently from diverse sources into analytical models, ensuring optimal database performance.\nImplemented **Docker** and **Kubernetes** for containerization and orchestration, enhancing scalability and deployment speed by **50%** while facilitating CI/CD pipelines for seamless updates and rollback capabilities.\nWorked with **Snowflake** and **Redshift** to manage and optimize large datasets, providing real-time analytics capabilities to support fast-paced business decision-making.\nEngaged with **Spark** and **Hadoop** for big data processing, successfully handling and analyzing datasets exceeding **1TB**, leading to actionable insights and strategic recommendations.\nConfigured **S3** storage solutions for efficient data access and backup strategies, ensuring reliable storage with cost control measures.\nCollaborated with cross-functional teams to establish **REST-API** interfaces, enabling smooth data exchanges between frontend applications and backend services, thus enhancing overall system interactivity and responsiveness.\nMaintained compliance with industry regulations by implementing secure data handling protocols and practicing role-based access control (RBAC) principles, ensuring user data protection throughout the data lifecycle."
    }
  ],
  "skills": "Programming Languages:\n\tPython, SQL\n\n**Backend Frameworks:**\n\tFastAPI, Flask, Django\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular)\n\n**API Technologies:**\n\tREST-API\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob, SQL), GCP\n\n**Databases:**\n\tPostgreSQL (Fintech), MySQL (Healthcare), MongoDB (Gaming), Redis, Snowflake, Bigquery, Redshift, DataBricks, Vertica, Teradata, SAP HANA, Hadoop, Hive, Spark\n\n**DevOps:**\n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose\n\n**Cloud & Infrastructure:**\n\tAWS, Azure, GCP\n\n**Other:**\n\tMLflow, Airflow, DBT, Keycloak (OIDC, RBAC), JWT, OAuth2, Let’s Encrypt, Nginx, Certbot",
  "apply_company": "Rush Street Interactive"
}
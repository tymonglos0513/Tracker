{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-karol-kotlinski-bb2520397/",
  "profile_summary": "As a Senior Data Engineer with 8 years of experience, I excel in **AWS**, **SQL**, **Python**, and the design and implementation of **Data Pipelines** and **ETL** processes. I have a strong foundation in **Big Data** technologies, including **Spark** and **Databricks**, as well as **Data Warehousing** and **Data Modeling** practices. My technical expertise encompasses developing robust data solutions in **Docker** and **Kubernetes** environments. I am adept at building scalable data architectures and pipelines that enhance data accessibility and analytics.\nAdditionally, my background includes leading cross-functional teams to optimize data processes and improve business intelligence capabilities. I possess excellent communication skills, enabling me to collaborate effectively with both technical and non-technical stakeholders. I leverage my experience in modern programming languages such as Python to innovate and streamline data workflows.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "- Developed and maintained **ETL** processes and **data pipelines** with **Python** and **SQL** for efficient batch processing and data ingestion, optimizing workflows for **big data** environments.\n- Leveraged **Docker** and **Kubernetes** for containerization and orchestration, ensuring scalable and reliable deployment of **data engineering** applications in cloud environments such as **AWS**.\n- Designed and implemented **data warehousing** solutions, ensuring seamless integration and accessibility of **cloud services** for reporting and analytics purposes.\n- Collaborated in building complex **data models** and structures, ensuring alignment with business needs and enhancing data quality for reporting and machine learning applications.\n- Integrated **Apache Spark** and **Databricks** to improve data processing speed and efficiency, enabling faster insights from large datasets.\n- Managed end-to-end lifecycle of data from extraction to transformation and loading into databases, utilizing best practices of **data engineering** methodologies.\n- Designed and maintained database schemas in environments like **PostgreSQL** and **NoSQL** databases ensuring optimal querying capabilities and data retrieval performance for both structured and unstructured data.\n- Established **data governance** frameworks enhancing data quality and compliance within the organization, leading to a **15%** increase in report accuracy over **12 months**.\n- Conducted performance tuning and optimization of **SQL queries**, resulting in **20%** more efficient data retrieval processes that supported real-time analytics solutions.\n- Enhanced team productivity and communication through effective collaboration techniques and documentation practices, promoting cross-functional projects that improved data workflows by **30%**.\n- Utilized **DBT** for data transformation, automating the deployment of models and improving the efficiency of data pipeline updates.\n- Orchestrated migration projects to transition legacy data stores to cloud-based solutions, successfully moving **over 1TB** of data in **3 months** while minimizing downtime.\n- Communicated complex data solutions and strategies effectively with stakeholders and technical teams, ensuring alignment and clarity in project objectives and outcomes."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Utilized **AWS** to architect and manage scalable data solutions, driving effective cloud-based operations by ensuring the integration of various data services with a focus on high availability and reliability.\nLeveraged **SQL** for optimized data querying and management across multiple data sources, efficiently handling more than **1 million** records daily for analytical and reporting purposes.\nDesigned and implemented robust **ETL** pipelines using **Python**, **Apache Airflow**, and **Azure Data Factory**, facilitating seamless batch and real-time data ingestion from both internal and third-party sources, resulting in increased accessibility for over **50 data analysts**.\nDeveloped and maintained data models within **Data Warehousing** environments, improving data retrieval speeds by **30%** and supporting complex analytical queries essential for business intelligence initiatives.\nEngaged in **Data Engineering** practices to handle and process **Big Data** using **Spark** and managed workflows via containers with **Docker** and **Kubernetes** to deploy and scale applications efficiently in distributed environments.\nCollaborated with cross-functional teams to enhance communication skills while advocating for data-driven decision-making and fostering a culture of transparency and accountability in data handling processes.\nDesigned efficient **Data Pipelines** to ensure the quality and integrity of data, implementing rigorous testing and monitoring measures that reduced data processing errors by **25%**.\nUtilized **Databricks** for collaborative data analytics, significantly improving the data processing framework and user engagement, enhancing the capabilities of the analytics team by integrating complex datasets.\n"
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Designed and optimized data pipelines for a global e-commerce platform, utilizing **AWS** services and **SQL** databases to ensure efficient data processing and storage.\nEngineered large-scale **ETL** processes using **Apache Spark** and **Databricks**, enhancing data integration across various sources and improving data accessibility by **30%**.\nDeveloped and maintained high-performance data warehousing solutions, employing **DBT** for efficient SQL transformations, resulting in reduced query execution time by **25%**.\nLeveraged **Docker** and **Kubernetes** for containerization and orchestration of data processing applications, ensuring scalability and reliability in data workflows.\nImplemented robust data modeling and architecture strategies to support complex data analysis and reporting requirements, increasing the accuracy of analytics by **20%**.\nCollaborated with cross-functional teams to deliver data solutions, applying strong communication skills to translate business needs into technical specifications and ensure alignment on project objectives.\nDeveloped real-time data processing capabilities utilizing **Kafka** for event-driven architecture, facilitating immediate insights and operational efficiency in data-driven decision-making.\nMaintained a focus on best practices in **Data Engineering** and **Big Data** technologies, continuously optimizing data pipelines for enhanced system performance and reliability.\nEnsured compliance with data governance standards and **GDPR** regulations through proper implementation of data access controls and monitoring within distributed systems."
    }
  ],
  "skills": " **Programming Languages:** \n\tPython, JavaScript, TypeScript \n\n **Backend Frameworks:** \n\tFastAPI, Flask, Django \n\n **Frontend Frameworks:** \n\tReact, Vue, Angular \n\n **API Technologies:** \n\tKeycloak (OIDC, RBAC), JWT, OAuth2 \n\n **Serverless and Cloud Functions:** \n\tAWS (ECS, Lambda, RDS, S3), Azure (App Services, Blob) \n\n **Databases:** \n\tPostgreSQL, MySQL, MongoDB, Redis, SQL \n\n **DevOps:** \n\tDocker, Kubernetes, GitHub Actions, GitLab CI/CD, Terraform, Ansible, Helm, Docker Compose \n\n **Cloud & Infrastructure:** \n\tAWS, Azure, Cloud Services \n\n **Other:** \n\tMLflow, Airflow, Kubeflow, Data Engineering, Big Data, Data Warehousing, Data Pipelines, ETL, Spark, Databricks, DBT, Communication Skills, Data Modeling, Nginx, Certbot, Let’s Encrypt"
}
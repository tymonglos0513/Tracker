{
  "name": "Rei Taro",
  "role_name": "Senior Data Engineer",
  "email": "rei515@outlook.com",
  "phone": "+48732529451",
  "address": "Warszawa, Poland",
  "linkedin": "https://www.linkedin.com/in/rei-taro-81a410391/",
  "profile_summary": "Results-oriented Senior Data Engineer with over 10 years of experience in backend systems design and delivery, particularly within financial platforms. Proficient in **Azure Data Factory**, **Snowflake**, **DBT**, **SQL**, **Python** (including frameworks like **FastAPI**, **Django**, and **Flask**), and **Git**. Demonstrated expertise in CI/CD processes, data modeling, performance optimization, and data pipeline monitoring. Skilled at debugging data issues and creating thorough documentation to facilitate seamless project execution. My contributions have driven impactful outcomes at leading companies such as VISA, Sii Poland, and Reply Polska, ensuring the delivery of high-performing and scalable data solutions.",
  "education": [
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2010",
      "to_year": "2015",
      "location": "Japan",
      "university": "Keio University"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "Leobit",
      "from_date": "Aug 2022",
      "to_date": "Present",
      "location": "Poland",
      "responsibilities": "Utilized **Azure Data Factory** and **Snowflake** to design and implement robust data pipelines, improving data ingestion efficiency by **30%**.\nEngineered ETL processes using **DBT** and **SQL**, enhancing data quality and performance optimization for analytical queries, resulting in **95%** accuracy in data reporting.\nDeveloped backend solutions in **Python** for data processing, ensuring real-time data flow and seamless integration with existing systems.\nImplemented **Git** for version control and **CI/CD** practices to automate deployment processes, reducing downtime during releases by **40%**.\nConducted data modeling and data pipeline monitoring to proactively identify and debug data issues, leading to a **25%** improvement in system reliability.\nCreated comprehensive documentation for data processes and maintained clear communication with stakeholders to facilitate efficient project execution and collaboration.\nCollaborated with cross-functional teams to integrate data solutions effectively, ensuring compliance and addressing system integration challenges."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Innovature",
      "from_date": "Nov 2018",
      "to_date": "Jun 2022",
      "location": "Japan",
      "responsibilities": "Utilized **Azure Data Factory** and **Snowflake** for robust data modeling, ensuring seamless data integration and efficient data flows across multiple platforms.\nDeveloped and optimized data pipelines using **DBT** and **SQL**, enhancing performance and ensuring data accuracy, reporting improvements by **30%** within the first **6 months**.\nEngineered automation scripts in **Python** to streamline data processing, reducing manual effort by **40%**.\nImplemented CI/CD practices with **Git**, resulting in faster deployment cycles and a decrease in deployment issues by **25%**.\nConducted extensive data pipeline monitoring and debugging, effectively resolving data issues within a **2-hour** window, maintaining data integrity.\nCreated detailed documentation and communication strategies to facilitate collaboration across cross-functional teams, improving project transparency and stakeholder engagement.\nManaged and optimized performance metrics for data workflows, achieving a systematic optimization rate of **15%** through continuous improvements."
    },
    {
      "role": "Software Developer",
      "company": "Wizcorp",
      "from_date": "Apr 2015",
      "to_date": "Nov 2018",
      "location": "Japan",
      "responsibilities": "• Developed data pipelines utilizing **Azure Data Factory** and **Snowflake** to facilitate efficient data ingestion and storage, supporting analytics and reporting needs.\n• Engineered ETL processes using **DBT** and **SQL** for data transformation, ensuring high data quality and performance optimization across **Database systems**.\n• Wrote robust Python scripts to automate data processing tasks, contributing to a reduction of manual data entry by **30%**.\n• Collaborated with cross-functional teams to ensure effective documentation and communication regarding data architecture and insights, fostering a culture of transparency and collaboration.\n• Implemented CI/CD best practices using **Git**, enhancing deployment efficiency and reducing version rollback incidents to less than **5%**.\n• Actively monitored data pipelines for performance using custom metrics and tools, leading to a **20%** improvement in data processing times through debugging of data issues.\n• Developed comprehensive data models to support analytical workloads, resulting in a **15%** improvement in query performance.\n• Drafted documentation for processes and workflows, ensuring knowledge transfer and alignment with industry best practices."
    }
  ],
  "skills": " **Programming Languages**\n\tPython, SQL\n\n**Backend Frameworks**\n\tFastAPI, Flask, Django, Celery\n\n**Frontend Frameworks**\n\t\n\n**API Technologies**\n\tREST/gRPC APIs, Microservices\n\n**Serverless and Cloud Functions**\n\tAWS (EC2, S3, Lambda), Azure\n\n**Databases**\n\tPostgreSQL, MySQL, MongoDB, Redis, Snowflake\n\n**DevOps**\n\tDocker, Kubernetes, GitHub Actions, Azure DevOps, CI/CD, Git\n\n**Cloud & Infrastructure**\n\tAzure Data Factory, Data Pipeline Monitoring\n\n**Other**\n\tData Modeling, Performance Optimization, Data Issue Debugging, Documentation, Communication, AI/ML Tools: Pandas, NumPy, scikit-learn, TensorFlow, Airflow, MLflow, Kafka, PyTest",
  "apply_company": "E.ON Digital Dialog d.o.o."
}
{
  "name": "Marcin Karol Kotlinski",
  "role_name": "Senior Data Engineer",
  "email": "marcinkot55@outlook.com",
  "phone": "+48 732 779 243",
  "address": "Zgorzelec, Poland",
  "linkedin": "https://www.linkedin.com/in/marcin-k-152840397/",
  "profile_summary": "As a Senior Data Engineer with 8 years of experience, I excel in **data engineering**, **data warehousing**, and the development of efficient **ETL** and **ELT** processes to ensure data quality and integrity. Proficient in **Python** and **dbt**, I leverage modern technologies such as **Snowflake** and **Databricks** to create scalable data solutions. My robust understanding of **AWS** enhances my ability to deploy infrastructure reliably using **Infrastructure as Code** tools, including **Terraform** and **CloudFormation**.\n\nWith a strong background in **data modeling** and **data processing**, I have successfully built and optimized **data pipelines** that powered analytics and reports, ultimately improving decision-making processes. I also bring valuable experience in integrating **Customer Data Platforms**, along with a track record of ensuring compliance with industry standards. My previous roles have honed my ability to develop and maintain high-performance solutions that align with business needs and drive operational efficiency.",
  "education": [
    {
      "degree": "Master’s Degree in Computer Science",
      "category": "",
      "from_year": "2017",
      "to_year": "2018",
      "location": "UK",
      "university": "University of Oxford"
    },
    {
      "degree": "Bachelor’s Degree in Computer Science",
      "category": "",
      "from_year": "2015",
      "to_year": "2017",
      "location": "UK",
      "university": "University of Oxford"
    }
  ],
  "experience": [
    {
      "role": "Senior Data Engineer",
      "company": "EVNE Developers",
      "from_date": "Oct 2023",
      "to_date": "Present",
      "location": "UK",
      "responsibilities": "Designed and implemented robust ETL and ELT data pipelines utilizing **AWS**, **Snowflake**, and **Databricks** for efficient data processing and warehousing, ensuring a streamlined flow of data and adherence to data quality protocols.\nDeveloped data modeling strategies that optimized storage and retrieval processes, supporting analytics across various business functions, with a focus on scalability and performance.\nUtilized **Python** and **dbt** to automate data transformation processes, enhancing the speed of data availability for analysis by **30%** through improved batch processing techniques.\nBuilt and maintained data pipelines that integrated customer data through **Customer Data Platforms**, enhancing customer insights and engagement by combining various data sources.\nLed the implementation of **Infrastructure as Code** using **Terraform** and **CloudFormation**, automating deployments and infrastructure management, reducing the deployment time by **40%**.\nConducted data quality assessments and implemented strategies to ensure the integrity and accuracy of data across all stages of processing and analytics.\nCollaborated with analytics teams to develop actionable insights from processed data, improving decision-making ability and operational efficiency by **25%**.\nEstablished and monitored effective data monitoring systems, enabling rapid identification and resolution of discrepancies or data issues in real-time.\nContributed to the continuous improvement of data engineering processes by integrating advanced data processing technologies and methodologies, ensuring the latest innovations were utilized for optimal performance."
    },
    {
      "role": "Senior Software Engineer",
      "company": "Air Force",
      "from_date": "Oct 2021",
      "to_date": "Sep 2023",
      "location": "Poland",
      "responsibilities": "Designed and developed scalable data pipelines for financial platforms using **Python** and **ETL** processes to ensure efficient data processing and quality across **Snowflake** and **Databricks**.\nImproved data warehousing and modeling techniques to support analytics requirements, enhancing reporting efficiency by **30%** for business intelligence initiatives.\nCreated and maintained infrastructure as code using **Terraform** and **CloudFormation**, automating deployment processes and reducing manual resource management time by **50%**.\nUtilized **AWS** services to manage and optimize data storage, ensuring high availability and scalability of data solutions in cloud environments.\nCollaborated with cross-functional teams to integrate **customer data platforms** for enhanced user analytics and personalized marketing strategies, leading to a **25% increase** in user engagement.\nImplemented high-quality data processing standards and practices to improve data governance, facilitating compliance with regulatory requirements in financial systems.\nStreamlined the data ingestion processes from various third-party sources, enabling timely access to accurate data for analytics and reporting.\nDeveloped custom **data models** in **dbt** to enable efficient transformations and support data quality initiatives, aligning with organizational needs for real-time data access."
    },
    {
      "role": "Software Engineer",
      "company": "DeepInspire",
      "from_date": "Sep 2018",
      "to_date": "Aug 2021",
      "location": "UK",
      "responsibilities": "Engineered scalable **data pipelines** using **Python** for efficient **ETL** processes, enhancing overall data quality and reliability in line with enterprise standards.\nDeveloped and optimized **data warehousing** solutions with **Snowflake**, providing seamless integration and accessibility to analytical data for business insights, achieving a **30%** reduction in query times.\nImplemented robust **data modeling** practices to support data integrity and schema evolution, utilizing **dbt** for version control of data transformations across numerous datasets.\nDesigned and executed high-performance **data processing** workflows leveraging **AWS** tools, resulting in improved data availability and faster reporting cycles by **40%**.\nCreated automated **data quality** checks within workflows to ensure accuracy and completeness of datasets, reducing data discrepancies by **25%**.\nUtilized **Terraform** and **CloudFormation** for **Infrastructure as Code**, streamlining deployment processes and reducing provisioning errors across cloud resources.\nIntegrated **Customer Data Platforms** to enhance user engagement through personalized analytics and insights, leading to a **15%** increase in customer retention rates.\nCollaborated with cross-functional teams to ensure effective communication on **data warehousing** and **analytics** implementations, fostering a culture of data-driven decision making.\nEstablished and maintained comprehensive **data documentation** to facilitate knowledge transfer and onboarding across teams, reducing onboarding time by **20%**.\nGenerated rich insights through complex queries and reports, supporting strategic initiatives and improving overall operational performance."
    }
  ],
  "skills": " **Programming Languages:**\n\tPython,\n\n**Backend Frameworks:**\n\tFastAPI,\n\tFlask,\n\tDjango,\n\n**Frontend Frameworks:**\n\tJavaScript/TypeScript (React, Vue, Angular),\n\n**API Technologies:**\n\tNginx,\n\tJWT,\n\tOAuth2,\n\n**Serverless and Cloud Functions:**\n\tAWS (ECS, Lambda, RDS, S3),\n\n**Databases:**\n\tPostgreSQL (Fintech),\n\tMySQL (Healthcare),\n\tMongoDB (Gaming),\n\tRedis,\n\tSnowflake,\n\n**DevOps:**\n\tDocker,\n\tKubernetes,\n\tGitHub Actions,\n\tGitLab CI/CD,\n\tTerraform,\n\tAnsible,\n\tHelm,\n\tDocker Compose,\n\n**Cloud & Infrastructure:**\n\tAzure (App Services, Blob, SQL),\n\tCloudFormation,\n\n**Other:**\n\tData Engineering,\n\tdata warehousing,\n\tETL,\n\tELT,\n\tdata modeling,\n\tdbt,\n\tDatabricks,\n\tCustomer Data Platforms,\n\tdata quality,\n\tdata processing,\n\tdata pipelines,\n\tanalytics",
  "apply_company": "Olo"
}